[
  
    {
      "title"       : "大模型基于检索增强的微调-RAFT",
      "category"    : "",
      "tags"        : "NLP, 深度学习, LLM",
      "url"         : "./raft.html",
      "date"        : "2025-11-27 08:00:00 +0800",
      "description" : "记录学习大模型的RAFT",
      "content"     : "一. RAFT简单介绍 论文：《RAFT: Adapting Language Model to Domain Specific RAG》 1.1 RAFT的优势 RAFT其实是基于RAG（检索增强）的SFT（有监督微调），那么他和这两者的差异是什么呢？ RAG：检索增强是基于提出的问题在向量数据库中检索相应的片段，然后作为线索合并问题喂给大模型，然后输出。然后这里会出现一个问题：模型对于特定领域的知识没有得到提前学习的机会，说白了就是给了大模型很多材料，但是无法非常有效的使用这些材料。 SFT：有监督微调是基于提供特定的QA对的方式来喂给大模型，然后能够让大模型额外扩充知识面的方式。但是这也会存在一个问题：这些QA对更多的是让大模型增加了额外文档的记忆，而忽略了在回答实际问题时使用文档的机会，要么没有正确处理在寻找合适的文档来学习时出现的错误。 RAFT：基于检索增强的微调则是结合RAG+SFT提出的一种新的微调方案。他的优势： 过微调确保模型在特定领域的知识上得到良好的记忆和训练，从而达到对不准确的检索文档进行发现。 通过训练模型来将“理解问题”、“检索知识”和“正确答案”进行关联，从而提高达模型回答的准确性。 这也就是整片文章中一致强调的RAFT其实是一种“开卷考试” + “特定领域的提前学习”。1.2 RAFT如何训练区别于常规的SFT，RAFT在训练中加入了干扰的文档 + 正确的文档让模型来学习。这使得模型学习到的是基于记忆和文档的混合判断，参考论文中的“which is a mixture of both memorization and reading”。 训练的内容分为2块： 找到核心依据：QA对 + 标准答案的文档 + 多个误导的文档 + 思维链 正确的例子：QA对 + 标准答案的文档 + 思维链 训练的核心：通过思维链来解释找到的答案，从而基于上下文信息，思考其答案，并链接到相关文档。二. 基于unsloth和lamma_index来实现FAFT2.1 基于ollama和lamma_index来构建数据集RAFT的数据集的特点是： QA对 正确文档 错误文档 思维链所以我们需要一个能够提供思维链的大模型，帮助我们生成上述这些内容。 模型：Qwen3-32B，选择这个千问模型是因为中文很友好+支持思维链+阿里的预训练的数据很顶 调用框架：ollama，调用比如openai需要花钱，这个本地部署，调用不花钱！2.2 安装相关依赖这里需要使用以下几个深度学习相关的框架： lamma_index: 和langchain感觉很像，只是这个更加偏重于增强大型语言模型 (LLM) 处理广泛和异构数据集的能力。当然也能帮助我们构建raft数据集咯。 unsloth: 大模型训练的框架依赖：llama_indexllama-index-packs-raft-datasetllama-index-llms-ollamallama-index-embeddings-ollamallama-index-packs-rag-evaluatorunsloth2.3 构建raft数据集我这边选择了法律相关的文书作为文档，然后结合lamma_index来向大模型提问，进而生成训练所需的问题和答案以及思维链。import osimport datasetsimport pandas as pdfrom llama_index.packs.raft_dataset import RAFTDatasetPackfrom llama_index.llms.ollama import Ollamafrom llama_index.embeddings.ollama import OllamaEmbeddingdef gen_qa_raft_dataset(): llm = Ollama(model=\"qwen3:32b\", request_timeout=60.0) embed_model = OllamaEmbedding(model_name=\"modelscope.cn/yingda/gte-Qwen2-1.5B-instruct-GGUF:latest\") sub_dir = 'data/laws/' df_list = [] for file_path in os.listdir(sub_dir): print(file_path) file_path = os.path.join(sub_dir, file_path) raft_dataset = RAFTDatasetPack(file_path, llm=llm, embed_model=embed_model, num_questions_per_chunk=3, num_distract_docs=3, chunk_size=2048) dataset = raft_dataset.run() df = dataset.to_pandas() df['messages'] = df.apply(lambda row: [{'content': row['instruction'], 'role': 'user'}, {'content': row['cot_answer'], 'role': 'assistant'}], axis=1) df_list.append(df) df_all = pd.concat(df_list) dataset = datasets.Dataset.from_pandas(df_all) dataset.to_csv(\"data/法律文书_raft_dataset.csv\", encoding='utf-8')然后我们可以打开看下结果，会发现包含了一下列： question：大模型生成的问题 context：多个文档（这里就是raft中的正确文档+误导文档） cot_answer: 思维链答案 instruction: 多个文档 + 问题 message: 完整的基于rag的问答对2.4 基于lora的sft 加载上述存储的dataset df = pd.read_csv(\"data/法律文书_raft_dataset.csv\", index_col=False)dataset = datasets.Dataset.from_pandas(df) 构建dataset的templatetokenizer.chat_template = DEFAULT_CHAT_TEMPLATESYSTEM_PROMPT = \"\"\"你是一位很有能力的问题解答者，能够根据问题和相关背景提供答案。\"\"\"def apply_chat_template(example, tokenizer): messages = example[\"messages\"] messages = messages.replace('\\n',',') messages = eval(messages) # We add an empty system message if there is none if messages[0][\"role\"] != \"assistant\": messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT}) example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False) return examplecolumn_names = list(dataset.features)raw_datasets = dataset.map(apply_chat_template, num_proc=cpu_count(), fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=column_names, desc=\"Applying chat template\",)raw_datasets = raw_datasets.train_test_split(test_size=0.1)# create the splitstrain_dataset = raw_datasets[\"train\"]eval_dataset = raw_datasets[\"test\"] 加载模型# 加载模型 - 使用Unsloth的FastLanguageModel加载预训练的Qwen3-8B模型model, tokenizer = FastLanguageModel.from_pretrained( model_name='Qwen/Qwen3-0.6B', # 模型路径 dtype=torch.float16, # 使用float16数据类型以减少内存占用 max_seq_length=2048, # 设置最大序列长度为2048 load_in_4bit=False # 使用4位量化加载模型，进一步减少显存占用)model = FastLanguageModel.get_peft_model( model, r = 32, # Choose any number &gt; 0! Suggested 8, 16, 32, 64, 128 target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha = 32, # Best to choose alpha = rank or rank*2 lora_dropout = 0, # Supports any, but = 0 is optimized bias = \"none\", # Supports any, but = \"none\" is optimized # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes! use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context random_state = 3407, use_rslora = False, # We support rank stabilized LoRA loftq_config = None, # And LoftQ) 训练trainer = SFTTrainer( model = model, train_dataset = train_dataset, eval_dataset = eval_dataset, args = SFTConfig( dataset_text_field = \"text\", per_device_train_batch_size = 2, gradient_accumulation_steps = 4, # Use GA to mimic batch size! warmup_steps = 5, # num_train_epochs = 1, # Set this for 1 full training run. max_steps = 30, learning_rate = 2e-4, # Reduce to 2e-5 for long training runs logging_steps = 1, optim = \"adamw_8bit\", weight_decay = 0.01, lr_scheduler_type = \"linear\", seed = 3407, report_to = \"none\", # Use this for WandB etc ),)tokenizer.pad_token = tokenizer.eos_tokentrain_result = trainer.train()三. RAFT的总结RAFT和一般的SFT而言，其实本质上没有任何差别，只是说将RAG融合进了有监督微调中，从而达到一种让模型拥有更加准确筛选文档+使用特定领域类文档的能力。而在训练中增加误导的文档，更是能让模型达到一种思考和过滤无关信息的能力。"
    } ,
  
    {
      "title"       : "大模型的检索增强-NanoGraphRAG",
      "category"    : "",
      "tags"        : "NLP, 深度学习, LLM",
      "url"         : "./graphrag.html",
      "date"        : "2025-06-09 08:00:00 +0800",
      "description" : "记录学习大模型的RAG",
      "content"     : "一. RAG和GraphRAG1.1 什么是RAG RAG（Retrieval Augmented Generation，检索增强生成）是一种将信息检索技术与生成式语言模型（LLM）结合的AI框架。它通过从外部知识库中检索相关信息，然后使用这些信息增强LLM的提示词（prompt），从而生成更准确、更相关、更全面的答案。﻿大模型为什么需要RAG： 如果一个llm在pretrain之后没有做rag（外部检索）的话，其实往往可能存在幻觉。 基座模型在专业领域知识不足，比如一些医药、法律等。 外挂知识需要长期实时维护更新的。1.2 RAG在LLM中LangChain的例子如上图所示，整个RAG的过程分为三个部分： 文本嵌入阶段：将本地文本进行分段后，利用embedding模型对其每个段落进行词嵌入，并将其vector存入向量数据库（比如FAISS、Chroma）中。 请求阶段：对大模型进行请求之前，会将请求的文本同样利用embedding模型转成向量，然后会和向量数据库中比对相似度最高的本地文本段落，并将其作为关联的文本段放到prompt中。 输出阶段：将提示词 + 本地文本关联段落 + 请求一起输入大模型，得到输出。1.3 GraphRAG的定义及其优势 GraphRAG是一种基于知识图谱的检索增强技术，通过构建图模型的知识表达，将实体和关系之间的联系用图的形式进行展示，然后利用大语言模型LLM进行检索增强。论文：From Local to Global: A Graph RAG Approach to Query-Focused SummarizationGraphRAG比RAG优势在哪： RAG过于局限：RAG仅能检索到与query相速度最高的文本片段，无法从全局出发来看整个本地文本。 GraphRAG通过构建实体关系图谱实现了信息间的连接，能从全局出发，能更完整地理解和检索复杂的关联信息，从而生成更准确和全局性的结果。1.4 GraphRAG的流程如上图所示，就是GraphRAG的处理流程图，其实总结起来就两个部分：第一部分：构建知识图谱 将本地文本进行分段 对每个段落进行摘要提取：这里完全依赖于大模型的提取能力 实体和关系的提取：这里完全依赖于大模型的提取能力，实体和关系抽取可以参考本人之前写的《事件抽取实战》这篇文章 构建知识图谱，写入数据库第二部分：利用构建的知识图谱回答问题 社区检测：实际就是一种图聚类的方法，这里的“社区”，我们可以将其理解为一类具有相同特性的节点的集合。 社区总结：给每个社区创建类似报告的摘要。利于理解整体的结构和语义。 生成全局答案：给定用户查询，基于上一步生成的社区摘要可用于在多阶段过程中生成最终答案。二. GraphRAG的轻量版本——nano-GraphRAG对于GraphRAG而言，其封装的太死，代码量又很大，对于定制化的任务而言，不太好下手，因此推荐一个轻量化的版本——nano-GraphRAG。 代码：https://github.com/gusye1234/nano-graphrag相较于GraphRAG的优势： 代码量少：包含了tests + prompt也才只有1100行代码！ 更容易阅读 给的例子太好用了，直接上手没有难度。2.1 安装这里推荐不用官方推荐的pip install 来安装。因为这样会对于代码和prompt不好调整。直接clone项目即可，然后安装其依赖pip install -r requirements.txt2.2 基于ollama的nano-GraphRAG的三国例子由于nano-Graphrag中的例子已经包含ollama形式的调用大模型的方式，因此开箱即用即可： 找到examples/using_ollama_as_llm_and_embedding.py，这里需要你更改embedding大模型和chat大模型，我这里选择了魔塔社区的embedding模型，注意由于我们使用的是ollama来本地启动大模型，因此需要选择的是GGUF格式的模型。 MODEL = \"deepseek-r1:8b\"EMBEDDING_MODEL = \"modelscope.cn/yingda/gte-Qwen2-1.5B-instruct-GGUF:latest\"EMBEDDING_MODEL_DIM = 1536 配置工程目录： WORKING_DIR = \"/LLM/nano-graphrag/three_kingdom_ds\" 修改insert的文件地址：with open(\"/LLM/nano-graphrag/three_kingdom_ds/三国演义.txt\", encoding=\"utf-8-sig\") as f: 使用ollama开启本地模型 ollama run deepseek-r1:8bollama run modelscope.cn/yingda/gte-Qwen2-1.5B-instruct-GGUF:latest 然后直接python examples/using_ollama_as_llm_and_embedding.py就会自动构建知识图谱 + 查询了。2.3 爬坑之前本人已经实验过qwen3:8b,亲测会报错leiden.EmptyNetworkError: EmptyNetworkError然后解决方案有2种： 更换大模型，比如像上面使用deepseek-r1:8b或者mistral:7b都可以的 更改prompt然后推荐测试的话，可以不用上来就用《三国演义》，不然可能跑了好几个小时，发现出现问题，那真的搞心态啊！"
    } ,
  
    {
      "title"       : "大模型的微调和推理加速",
      "category"    : "",
      "tags"        : "NLP, 深度学习, LLM",
      "url"         : "./%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F.html",
      "date"        : "2025-05-27 08:00:00 +0800",
      "description" : "记录学习大模型的微调和推理加速",
      "content"     : "一. 大模型微调1.1 微调的原因在业务中直接使用大模型，往往会发现： 模型对prompt非常敏感，需要不断调整，迭代prompt的过程很痛苦 本身能力不满足独有的业务需求 模型的更新/更换 导致相同的prompt却拿到不同的结果1.2 微调的方式 zero-shot prompting（0样本）：撰写prompt，迭代prompt，获取答案。无需数据，无需额外开发、部署成本。 few-shot prompting（1-10样本）：收集样例，迭代样例，获取答案。需要提供少量的样本（正反例）。 adaptation（1k-1w样本）：收集数据，适配模型，部署模型。 适用百亿参数以下的模型 少量的适配特殊场景/任务的样本，这里其实可以利用更大参数、更优的大模型输出的结果来训练这类“小模型” 更深层次的微调： 领域预训练：用无监督领域数据继续训练基座模型 全参数精调：用有监督领域数据精调基座模型 检索增强：外挂知识库，这里比如一般的RAG，还有比较火热的GraphRAG。 多智能体：多模型/agent共同完成任务。 二. LLM微调工具-Unsloth git地址：https://github.com/unslothai/unsloth选择unsloth作为微调工具的理由： 训练更快 显存占用更少 迭代快，模型适配广，目前都能支持Qwen3了2.1 依赖的安装 这里爬了不少坑，因此记录下 直接pip安装：pip install unsloth, 这里会默认安装最新的torch版本，而非适配于自身系统cuda版本的torch（用nvcc -V可查看cuda版本） 重新安装torch，本人这里cuda是12.1的：pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121 重新安装对应cuda版本的xformers：pip install -U xformers --index-url https://download.pytorch.org/whl/cu121 如果需要微调Qwen3，那么对于transformers有版本要求：pip install transformers==4.51.0 --no-deps 改了transformers之后，那么也需要对应的trl版本更改：pip install trl==0.17.0 --no-deps 安装vllm:pip install vllm==0.7 --no-deps 这里会遇到unsloth和vllm版本不匹配的问题：No module named vllm.lora.peft_helper,简单的处理方案就是到site-packages/unsloth_zoo/vllm_lora_worker_manager.py文件中注释掉所有的和PEFTHelper相关的行即可。 如果出现No platform detected, vLLM is running on UnspecifiedPlatform，这里只需要安装pip install pynvml==12.0即可。2.2 微调的例子这里推荐使用官方的基于Qwen3的微调例子：https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb当然在上述微调的例子中，也存了坑，就在训练模型过程中： 对于高版本的trl而言，参数从tokenizer变成了processing_class 如果不指定data_collator的话，会出现padding相关的bug。下面的代码是修复之后的# 配置训练参数 - 使用SFTTrainer进行监督微调trainer = SFTTrainer( model=model, # 要训练的模型 processing_class=tokenizer, # 分词器 train_dataset=combined_dataset, # 训练数据集 data_collator=DataCollatorForLanguageModeling(tokenizer,mlm=False), # 定义data_collator，一定是这个不然会报错 # max_seq_length=1024, # 训练时使用的最大序列长度 args=SFTConfig( # SFT配置参数 dataset_num_proc = 12, dataset_text_field=\"text\", # 数据集中文本字段的名称 per_device_train_batch_size=2, # 每个设备的训练批次大小 gradient_accumulation_steps=4, # 梯度累积步数，用于模拟更大的批次大小 warmup_steps=5, # 学习率预热步数 # num_train_epochs = 2, # 训练轮数，这里注释掉了，使用max_steps代替 max_steps=50, # 最大训练步数 max_seq_length=1024, learning_rate=2e-4, # 学习率，对于长时间训练可以降低到2e-5 logging_steps=1, # 日志记录间隔步数 optim=\"adamw_8bit\", # 优化器，使用8位AdamW weight_decay=0.01, # 权重衰减率 lr_scheduler_type=\"linear\", # 学习率调度器类型 seed=3407, # 随机种子 report_to=\"none\", # 报告工具，可以设置为\"wandb\"等 ),)三. 推理加速对于训练完成后的模型，我们需要选择一个合适的加速推理的框架来加速大模型的推理。3.1 常用框架常用的推理加速框架： LLM-tensorRT：在TensorRT推理引擎基础上，针对大模型的推理优化框架 vllm：通过PagedAttention高效地管理attention中缓存的张量 LLMdeploy：LMDeploy 开发了 Persistent Batch(即 Continuous Batch)，Blocked K/V Cache，动态拆分和融合，张量并行，高效的计算 kernel等重要特性。推理性能是 vLLM 的 1.8 倍 llama.cpp：针对C/C++的加速推理框架3.2 vLLM的使用对于企业级的服务以及高并发场景，vLLM会更加适合。安装：pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121以下是两种构建服务的方式： 第一种：使用vllm.entrypoints.openai.api_server，适用curl调用python -m vllm.entrypoints.openai.api_server --model \"模型地址\" --served-model-name \"模型名字\" --port 8123 --dtype=half 第二种：使用vllm serve，适用于openai的方式调用apikeyvllm serve \"模型地址\" --tensor-parallel-size 1 --enforce-eager --port 8124 --dtype float16 --trust-remote-code --served-model-name 模型名字然后可以使用openai来调用vllm启动模型的接口from openai import OpenAI# Set OpenAI's API key and API base to use vLLM's API server.openai_api_key = \"EMPTY\"openai_api_base = \"http://localhost:8123/v1\"client = OpenAI( api_key=openai_api_key, base_url=openai_api_base,)chat_response = client.chat.completions.create( # model=\"Qwen/Qwen3-8B\", model=\"模型名字\", messages=[ {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}, ], # max_tokens=32768, temperature=0.6, top_p=0.95, extra_body={ \"top_k\": 20, },)print(\"Chat response:\", chat_response)"
    } ,
  
    {
      "title"       : "Taskflow 使用小结",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./taskflow.html",
      "date"        : "2025-04-07 08:00:00 +0800",
      "description" : "记录学习taskflow的心得",
      "content"     : "一. TaskFlow的优势和完整的工作流TaskFlow是OpenStack开源的Python库，他的优势： 可伸缩 简单创建任务级对象 任务可插拔 支持回滚容错机制api的文档参考：https://docs.openstack.org/taskflow/ocata/一个完整的taskflow包含了一下几个环节： 创建task 声明flow 构建engine1.1 构建task构建task的方式是继承task类，然后修改其excute方法，这里可以指定任务的返回结果为res，注意还可以改写revert函数来重定义回滚的操作。from taskflow import taskclass TaskA(task.Task): default_provides = 'res' def execute(self, a:int): res = a * a return res def revert(self, a): print(\"a执行失败。。。\")1.2 构建flow在构建完task之后，则是需要定义flow，一般的线性flow的定义如下：from taskflow.patterns import linear_flow as lfflow = lf.Flow('main').add( Task1(), )1.3 定义engine需要定义一个engine来运行整个工作流，用于task的执行、停止、继续和恢复。from taskflow import enginesengine = engines.load(flow)engine.run()# 拿到所有的结果context = engine.storage.fetch_all()二. 工作流形式taskflow的工作流组合形式分为3种： 串行工作流 并行工作流 图流2.1 串行工作流——linear_flow顾名思义，就是多个工作流利用串行的方式将多个flow拼接在一起，然后依次顺序执行，每个flow依赖于他的前一个的flow。graph LR A[taskA]:::highlight --&gt; B[TaskB] B[TaskB] --&gt; C[TaskC]注意：这里在构建串行的工作流后，我们可以选择engine时候，选择engine='serial'from taskflow.patterns import linear_flow as lffrom taskflow import enginesclass TaskA(task.Task): def execute(self): print('task A')class TaskB(task.Task): def execute(self): print('task B')flow = lf.Flow('merge_ab').add( TaskA(), TaskB())engine = engines.load(flow, engine='serial')2.2 并行工作流——unordered_flow多个工作流可以被指定为并行执行，python里面的多线程一样，谁先抢到资源谁就先执行，等到三个都执行完毕了，这个流就结束了。graph LR A[taskA]:::highlight B[TaskB] C[TaskB] 注意：这里在构建并行的工作流后，我们可以选择engine时候，选择engine='parallel'from taskflow.patterns import unordered_flow as uffrom taskflow import enginesclass TaskA(task.Task): def execute(self): print('task A')class TaskB(task.Task): def execute(self): print('task B')flow = uf.Flow('merge_ab').add( TaskA(), TaskB())engine = engines.load(flow, engine='parallel')2.3 图流——graph_flow对于两两有相关关联的工作流，比如图结构中的两个节点，两个节点之间有一条边（这里指代两节点有依赖关系），即taskA执行的时候需要taskB，而taskB执行的时候依赖了taskA。一般来说这种方式用的最少。graph TD A[taskA] &lt;--&gt; B[taskB]from taskflow import taskfrom taskflow.patterns import graph_flow as gffrom taskflow import enginesclass TaskA(task.Task): def execute(self): print(\"TaskA executed\")class TaskB(task.Task): def requires(self): return ['data_a'] def execute(self, data_a): print(\"TaskB executed\")def build_flow(): flow = gf.Flow('demo_flow') # 获取节点对象 a_node = flow.add(TaskA()) b_node = flow.add(TaskB()) # 节点间建立依赖 a_node.precede(b_node) # 正确调用层级 return flowengines.run(build_flow())三. 完整的项目的例子3.1 串行和并行工作流混合对于需要构建多个并行工作流得到多个各自的结果后，然后需要构建一个基于三者结果合并计算的工作流，那么这里可以构建串行和并行的混合工作流，如下所示graph LR A[taskA]:::highlight --&gt; D[taskD] B[TaskB]--&gt; D[taskD] C[TaskB] --&gt; D[taskD]那么对于上述串并行混合的工作流而言，可以参考下面代码，其中包含了项目中可能用到的： 多线程并行运行 多flow组合形式 每个task结合了输入和输出from taskflow import taskfrom taskflow.patterns import unordered_flow as uffrom taskflow.patterns import linear_flow as lffrom taskflow import enginesfrom concurrent.futures.thread import ThreadPoolExecutorpool_executor = ThreadPoolExecutor(max_workers=3)# 定义并行任务类，继承 task.Task 并声明 providesclass TaskA(task.Task): def execute(self, a:int): res_a = a * a print(f'task a result:{res_a}') return res_aclass TaskB(task.Task): def execute(self, b:int): res_b = b * b print(f'task b result:{res_b}') return res_bclass TaskC(task.Task): def execute(self, c: int): res_c = c + 2 print(f'task c result:{res_c}') return res_cclass FinalTask(task.Task): def execute(self, res_a, res_b, res_c): final_res = res_a + res_b + res_c print(f'task final result:{final_res}') return final_res# 创建主流程def build_flow(): parallel_flow = uf.Flow('parallel_tasks').add( TaskA(name='taska', provides='res_a'), TaskB(name='taskb', provides='res_b'), TaskC(name='taskc', provides='res_c'), ) main_flow = lf.Flow('main').add( parallel_flow, FinalTask(name='final', provides='final_res'), ) return main_flowif __name__ == \"__main__\": store = { 'a':1, 'b':2, 'c':3 } flow = build_flow() eng = engines.load(flow, store=store, engine='parallel', executor=pool_executor) eng.run() context = eng.storage.fetch_all() result = context.get('final_res') a_res = context.get('res_a') print(f'task A res: {a_res}, final_res: {result}')"
    } ,
  
    {
      "title"       : "知识图谱的小结",
      "category"    : "",
      "tags"        : "机器学习, NLP",
      "url"         : "./%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B0%8F%E7%BB%93.html",
      "date"        : "2024-12-25 08:00:00 +0800",
      "description" : "记录学习知识图谱的一些总结",
      "content"     : "一. 定义知识图谱是一种二元的图网络，用以描述客观世界中的实体信息及其相互关系规律的知识化体系。它的基本组成单元包括： 实体-关系-实体 实体-属性-属性值二. 知识图谱的构建2.1 知识建模对于构建知识图谱的时候，一般我们都是针对所在的特定领域进行建模，所以一般称为“本体建模”或者“领域建模”。这里构建本体模型一般分为3类技术： RDF：Resource Description Framework，即资源描述框架。采用“资源-属性-属性值”的“主谓宾”结构（或称三元组），提供一种框架容器，并通过XML定义了一套形式化的方法，为机器语义理解的结构基础。 RDFS：RDFS是RDF的一个扩展，它提供了一些基本的类和属性，以及它们之间的关系。 OWL：Web Ontology Language，Web本体语言，它是基于RDF和RDFS的一种更加丰富的知识表示语言。在OWL中，我们可以定义类、属性以及它们的约束和关系。此外，OWL还支持对知识的推理。2.2 知识获取构建好需要的本体模型后，那么就需要把数据构建成对应的模型的输入了。我们知识的来源基本分为以下几类： 私有数据 企业的文档 企业私有的用户数据 企业私有的非结构化数据 公有数据 新闻 百科 公有语料 那么对于一些上述这些数据拆分为两大类，来进行数据处理： 非结构化数据：一般指的是无任何标识的文本，我们可以采用相关的NLP算法，比如实体识别和关系抽取（或者直接使用事件抽取，这里事件抽取可以参考我之前写的一篇博客）来构建实体关系的三元组。 结构化数据：由于本身就是标准化的数据，我们只需要通过D2R服务器将数据库的内容转成RDF的形式。2.3 知识融合 知识融合是将不同来源的知识整合到一起的过程，用以解决知识的冲突和重复。以下是基于知识融合步骤： 实体识别和链接：首先需要识别出不同数据源的相同实体，并将其链接。比如这里的“Tesla”和“特斯拉”应该识别并链接为同一实体。 重复实体的合并：可以基于文本相似度来将不同数据源的重复实体进行合并 关系融合：识别并合并描述相同实体之间的关系。2.4 知识存储一般来说，会将知识利用图的形式进行存储，因此，我们需要将抽取的实体关系构建成图的形式： 图的节点：实体 图的边：关系图的存储一般使用的是Neo4j图数据库，据说其图的搜索的性能比mysql快1000倍！2.5 知识计算 知识计算是基于已构建的知识图谱进行能力输出的过程，它以图谱质量提升、潜在关系挖掘与补全、知识统计与知识推理作为主要研究内容。基于图论的相关挖掘算法： 图匹配 图查询 频繁子图 图聚类知识推理是指从知识库中已有的实体关系数据出发，进行计算机推理，建立实体间的新关联，从而拓展和丰富知识网络。以下是基于知识推理的计算方法： JENA：最常见的OWL推理工具是Jena，Jena2支持基于规则的简单推理，它的推理机制支持将推理器（inference reasoners）导入Jena，创建模型时将推理器与模型关联以实现推理。 JESS：使用规则引擎进行推理，同 大多数专家系统工具一样，Jess的核心也是由事实库、规则库、推理机三大部分组成，并采用产生式规则作为基本的知识表达模式。 ProLog：Prolog作为一种声明式逻辑编程语言，非常适合用于实现专家系统中的知识表示和推理。专家系统通过模拟人类专家的知识和决策过程来解决特定领域的复杂问题。在专家系统中，知识通常以事实（facts）和规则（rules）的形式表示，而Prolog的逻辑声明特性使得它成为实现专家系统的理想选择。 DataLog:是一种基于逻辑的编程语言。它是一阶谓词逻辑中Horn子句逻辑的一种受限形式，只允许变量或常量作为谓词的自变元，不允许函数作为谓词的自变元。Datalog的语句由事实和规则组成，同Prolog一样，它可以实现对知识库的演绎推理，即可以从已知事实中根据跟着推理得到新的事实。三. 知识图谱的应用3.1 安防领域对于传销组织而言，其实其特征信息是潜藏在通话网络中，我们可以利用构建通话网络的图谱，利用子图识别和匹配的挖掘手段找到十分特殊的传销子图。3.2 金融领域在金融领域中，根据获取用户的身份信息、授权信息以及长生命周期的信贷信息，构建的知识图谱，用于推断借款人的欺诈风险。3.3 智能客服在大模型出现之前，其实可以根据自身垂类信息来构建企业垂类的知识图谱，用于简单的人机对话及检索。但是随着AI大模型的广泛应用，现在在智能客服上，完全可以用垂类大模型对其进行替换。"
    } ,
  
    {
      "title"       : "写专利心得",
      "category"    : "",
      "tags"        : "机器学习",
      "url"         : "./%E5%86%99%E4%B8%93%E5%88%A9%E5%BF%83%E5%BE%97.html",
      "date"        : "2024-11-08 08:00:00 +0800",
      "description" : "记录写专利期间的心得体会",
      "content"     : "写专利心得 最近2个月一口气写了9篇专利，近些年加起来也写了近20篇AI算法类专利了，所以想写点自己对于写专利的一些心得体会。一. 专利相关知识1.1 专利是什么专利是专利权的简称，主要分为发明、实用新型及工业设计三种类型： 发明：产品、方法及其改进的技术方案 使用新型：产品的形状、结构或其结合的技术方案 外观设计：产品外部形状、图案、色彩及其组合，如外包装。 1.2 专利申请流程 想好idea，确保哪些可以写，哪些写不了 利用专利检索的工具（比如google学术），查看别人是否写过 写交底书（简写） 第三方机构代理人撰写 专利递交专利局 官方受理、初审、公开、实审 官方授权 专利维持 1.3 写专利好处 拓展和发散思维 锻炼文笔 了解当前行业已有的技术 其实说了这么多，还是因为写专利可以赚钱啦！二. 专利挖掘-算法类对于算法工程师而言，其实写发明类专利还是很简单的。个人感觉就和写小论文差不太多，甚至于比写小论文还简单，因为写专利其实可以不需要数据支持，只需要把自己的解决方案写出来就好了！2.1 创新点 专利审查时判断创造性的逻辑，是用你的方案和现有的技术比对，看有哪些区别点，然后判断在现有技术的基础上，做出这些区别点的改变得到你的方案是否是容易想到的，有没有一些难度。 如果都是一些常规的方案，并且应用到我们这里也没什么难度的话，创造性高度就比较低，如果并不是简单的能运用到我们的方案，需要做一些相应的技术难点的解决的话，创新性就比较好。 2.2 可写的几类算法专利 改进发明：这里最好详尽描述下现有技术，当前改进的技术 改进的数据处理 改进的数据挖掘 改进的数据标注 改进的模型结构 改进的算法公式 转用发明：现有算法应用在本发明的场景，说明克服了什么技术难点，增加了什么技术特征 组合发明：1+1 &gt; 2，意料不到的技术上效果。聚焦到实际解决什么问题，不能写过于宽泛 start=&gt;start: 老场景特征抽取及模型训练 info=&gt;operation: 新场景特征抽取 setCache=&gt;operation: 基于新场景特征及老场景模型合并得到压缩后的新特征 end=&gt;end: 基于压缩过的新特征进行训练，得到新场景模型 start-&gt;info-&gt;setCache-&gt;end2.3 算法关键点 用几句白话概括算法的关键点，相对于现有技术创新的地方 流程图绘制整体流程 模型架构图 模型layer的创新点 算法中构建的公式 2.4 交底书编写-算法类我们在写交底书的时候，需要注意把以下环节都写上： 描述现有场景下的问题 本发明用什么样的方案解决了什么样的问题 本发明在产品上属于什么环节 本发明具体在技术上的创新点 本发明所产生的有益效果 参考文献 三. 总结其实，写专利的最大的难点就是创新点，可能你感觉自己好不容想出来的点，一查专利网站，别人老早就写了，这就很蛋疼了。所以，一定要乘早写，尽量脑海中有一点灵光的时候，就开动吧！"
    } ,
  
    {
      "title"       : "pyspark使用总结-第二篇",
      "category"    : "",
      "tags"        : "机器学习, 代码",
      "url"         : "./pyspark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.html",
      "date"        : "2024-11-07 08:00:00 +0800",
      "description" : "记录pyspark的使用过程心得体会",
      "content"     : "一. 常见的OOM1.1 常用的解决方案我们在使用spark的时候，经常在save数据的时候，都会遇到内存溢出的问题，这通常是由于数据量过大导致的。以下是一些可能的解决方案： 增加分区数：如果数据集非常大，可以尝试增加分区数。可以使用repartition()或coalesce()函数来增加分区数。增加分区数可以将数据均匀地分布在更多的节点上，从而减少每个节点上的内存压力。 压缩数据：如果数据集包含大量重复的值，可以考虑使用压缩算法来减少内存使用。Pyspark提供了多种压缩算法，如Snappy、Gzip等。可以使用option(\"compression\", \"snappy\")来设置压缩算法。 增加集群资源：可以考虑增加集群资源。可以增加集群的节点数或增加每个节点的内存。可以通过调整spark.driver.memory和spark.executor.memory参数来增加内存分配，特别对于driver而言，最好把内存设置大一些。1.2 代码方面的优化如果以上常用的解决方案依旧无法解决OOM的问题，那么我们可能需要考虑是否需要优化pyspark的代码了 UDF过于复杂：尽可能将结果拆分不同的列，然后再用简单的udf来组合这些列进行计算。 多用filter算子：提前将大量数据剔除 多用select算子：只保留需要的列，减少内存的使用 尽量少用collect、count算子：像这些action算子基本都会把executor的数据全部加载回driver上，导致driver的内存吃紧。 当发现在某个udf环节只有一个节点在跑的时候，可以使用.cache()来分布式跑任务。1.3 数据倾斜导致的OOM和心跳时间超时通常我们会发现有些时候，数据本身并没有很大，但是要运行很长时间，而且最终还因为heart beat时间过长或则oom而失败。发生数据倾斜一般发生在： join两个df的时候 groupby某一列，然后这一列（包含了a，b，c等元素）中，很不巧，a有非常多（1000），而b和c等元素仅有2个验证这时候我们可以直接通过yarn日志中的shuffer resize/records看到很多excutors中会发现极个别失败的excutor的size非常大（10w个），而其他的excutor的size可能只有10个。那么这个时候无疑是发生数据倾斜了。解决这里推荐使用加盐处理，比什么加excutor的内存和核数更加有效df1 = df1.withColumn(\"salted_a\", F.concat(F.col(\"a\"), F.lit(\"_\"), F.floor(F.rand() * salt_num).cast('int')))df2 = df2.withColumn(\"salted_a\", F.concat(F.col(\"a\"), F.lit(\"_\"), F.floor(F.rand() * salt_num).cast('int')))df = df1.join(df2,on='salted_a')二. 在集群上使用自建的python环境2.1 构建python环境由于集群是centos，那么我们构建python环境的时候最好选择也是centos。 conda构建python3.8：conda create -n yy_env python=3.8 安装相关包：pip intall -r requirements.txt 进入miniconda目录下：cd /root/miniconda3/envs 压缩python环境：tar zcvf yy_env.tar.gz yy_env/ 2.2 从本地传到指定文件目录 如果需要推送到mdfs上，需要用mdfs和hdfs之间的映射关系 利用hadoop脚本上传至hdfs上 sh hadoop.sh fs -Dipc.client.fallback-to-simple-auth-allowed=true -put file:///yy_env.tar.gz hdfs://env/2.3 编写spark_confspark.yarn.dist.archives=mdfs:///env/yy_env.tar.gz;spark.yarn.appMasterEnv.PYSPARK_PYTHON=./yy_env.tar.gz/yy_env/bin/python3;spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=./yy_env.tar.gz/yy_env/bin/python3;spark.executorEnv.PYSPARK_PYTHON=./yy_env.tar.gz/yy_env/bin/python3;spark.sql.broadcastTimeout=800;spark.sql.broadcastMaxRetries=3;spark.executor.heartbeatInterval=100000三. pandas和pyspark的dataframe转换 两者之间的转换，代码很简单，但是实际中就会发现，当海量数据需要进行转换的时候，消耗时间成本是多么大！3.1 转换的代码# pandas ==&gt; pysparkpyspark_df = spark.createDataFrame(pandas_df)# pyspark ==&gt; pandaspandas_df = pyspark_df.toPandas() 3.2 爬坑在转化代码中有哪些坑呢？ pandas转pyspark的时候，如果你的pandas的版本过低，就会报错，这里你可以选择以下2个方案解决： 升级pandas 在代码中添加：pd.DataFrame.iteritems = pd.DataFrame.items 耗时过长，这里也有以下方案能缩减耗时： 减少df的列和行 ==&gt; 减少数据 利用pyArrow加速：pip install pyarrow spark = SparkSession.builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") # 加速转pandasdf的速度 四. 常用脚本4.1 加载数据# 加载mdfs文件，无表头schema = StructType([ StructField(\"link_id\", StringType(), True) ])df = spark.read.csv(file_path, header=True,schema=schema)# 加载mdfs文件，有表头df = spark.read.csv(file_path, header=True)# 加载hive表df = spark.sql(\"select a,b from save_tabel where \")df = spark.read.format(\"iceberg\").load(hive_tabel).where(\"a = a\").select(\"a\",\"b\")4.2 保存数据# 保存mdfsdf.repartition(partition_num).write.option(\"header\", \"true\").format(\"csv\").mode(\"overwrite\").save(save_path)# 保存hive表: append是添加，overwrite是覆盖df.write.format(\"iceberg\").option(\"mergeSchema\", \"true\").mode(\"append\").save(save_tabel)4.3 常用代码# 1. 根据某一或者几列去重df = df.dropDuplicates(subset=['a','b'])# 2. df上下拼接（保证两个df的列名和顺序一致）df = df_1.union(df_2)# 3. df横向拼接df = df_1.join(df_2, on='a', how= 'inner')# 4. 构建常数列df = df.withColumn('a', F.lit('0'))# 5. groupby多列，其他的列聚合成listdf = df.groupby('a','b').agg(F.collect_list('c'),F.collect_list('d'),F.collect_list('e'))df = df.withColumn('c', F.col('collect_list(c)').cast(StringType()))# 当有多个collect_list，然后需要保证数据同步的时候可以用F.structdf = df.groupBy(['a','b']).agg(F.collect_list(F.struct(\"aa\", \"bb\", \"cc\")))df = df.withColumn('values', F.col('collect_list(struct(aa,bb, cc))').cast(StringType()))# 6. filter过滤多个条件df = df.filter((F.col('a') == 1) &amp; (F.col('b') == 1)) # 且# 7. 两个df按照某一列进行计算差集diff_df = df_1.select(\"a\").subtract(df_2.select(\"a\")).distinct()# 8. explode with splitdf = df.withColumn(\"aa\", F.explode(F.split(F.col(\"a\"), ';')))# 9. substrdf = df.filter(col(\"a_str\").substr(1, 4) == \"1234\") # a = 123456784.4 udf使用可能在一个处理的过程中往往会使用多个自定义的udf函数，但是当项目非常大的时候，最好还是把归属于这个处理类的udf集成到类中：class A: @staticmethod @F.udf(returnType=IntegerType()) def is_a_equal0(a): if a == 0: return 1 else: return 0需要返回多列 def aaa(var_list): @F.udf(returnType=StringType()) def bbb(value): # 在这里可以对每个值进行自定义的处理操作 rs = '' value_js = json.loads(value) for v in var_list: if rs: rs += (';' + str(value_js[v])) else: rs += str(value_js[v]) return rs return bbb need_vars = ['a','b','c'] df = df.withColumn(\"need_data\", aaa(need_vars)(F.col(\"data\"))) df = df.withColumn(\"s\", F.split(df['data'], \";\")) for i, v in enumerate(need_vars): df = df.withColumn(v, df['s'].getItem(i))"
    } ,
  
    {
      "title"       : "LLM的应用开发框架——Langchain",
      "category"    : "",
      "tags"        : "深度学习, NLP, LLM",
      "url"         : "./langchain.html",
      "date"        : "2024-07-04 08:00:00 +0800",
      "description" : "记录Langchain的使用过程心得体会",
      "content"     : "一. Langchain是什么 Langchain 官网文档：https://python.langchain.com/v0.2/docs/introduction/LLM崛起出现了哪些需求？ 格式化输出：希望给的输出格式是json、csv、db格式 输出很长的提示词文本：如何总结一本书的内容？ 多次API调用：两次调用api，前后两次需要结合的 外部调用：比如需要进行web 搜索 标准化开发 快速切换模型：有多个大模型可用，支持代码不变，快速切换 二. Langchain支撑LLM的应用2.1 支持多种LLM无论是国外的GPT4、LLaMa，还是国内的ChatGLM、Baichuan，都支持调用api和huggingface模型的使用，下面主要介绍HF模型的下载使用。from huggingface_hub import snapshot_downloadfrom langchain.llms.base import LLM # 指定下载目录（在当前文件夹下）snapshot_download(repo_id=\"baichuan-inc/Baichuan2-7B-Chat-4bits\", local_dir=\"baichuan-inc/Baichuan2-7B-Chat-4bits\") class baichuan2_LLM(LLM): # 基于本地 Baichuan 自定义 LLM 类 tokenizer: AutoTokenizer = None model: AutoModelForCausalLM = None def __init__(self, model_path: str, dtype = torch.bfloat16): # model_path: Baichuan-7B-chat模型路径 # 从本地初始化模型 super().__init__() print(\"正在从本地加载模型...\") self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=dtype, device_map=\"auto\") self.model.generation_config = GenerationConfig.from_pretrained(model_path) self.model = self.model.eval() print(\"完成本地模型的加载\") def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any): # 重写调用函数 messages = [ {\"role\": \"user\", \"content\": prompt} ] # 重写调用函数 response = self.model.chat(self.tokenizer, messages) return response @property def _llm_type(self) -&gt; str: return \"baichuan2_LLM\"2.2 零样本少样本提示对于LLM来说，尽可能的使用prompt来尝试解决问题，而非直接对LLM进行训练。因此对于少数据或者没有数据来利用prompt来解决问题： zero-shot prompting：直接问模型，最低成本获取答案 few-shot prompting：给模型几个例子，引导它做的更好 在Langchain中已经集成了few-shot prompting，如下例子from langchain.prompts.few_shot import FewShotPromptTemplateexamples = [ { \"question\": \"你好吗？\", \"answer\": \"帅哥，我很好\" }, { \"question\": \"今天周几？\", \"answer\": \"帅哥，今天周日\" }, { \"question\": \"天气好吗？\", \"answer\": \"帅哥，是的，今天天气很不错\" } ] example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"提问: {question}\\n回答:{answer}\") prompt = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt, suffix=\"提问: {input}\\n\", input_variables=[\"input\"]) print(prompt.format(input=\"我怎么这么丑\")) # 这里相当于将前缀的这些少样本和当前问题全部放入llm，让其知道前后关系或者学习规则 print(llm.predict((prompt.format(input=\"我怎么这么丑\"))))2.3 文档问答方案：LangChain + ChatGLM，GitHub - chatchat-space/Langchain-Chatchat整体流程： 本地文档通过Loader读入 利用文档分割器对其文字进行分割，不然文字太长无法输入llm 利用Embedding Model（直接利用Huggingface下载相关词嵌入模型）对分割后的文档chunk进行向量化操作 利用VectorStore（可以选择Chroma或者FAISS作为DB）对向量进行存储 对于新的query而言，向量化后对Vecorstore进行搜索其相似度高的chunk 最后将chunk文档同样放入prompt中，输入模型，得到结果 Langchain支持的优势： 支持相关模块，比如TextSplitter、Loader、Embedding、Vector 在文档中直接找到相似度最关联的chunk，减少输入llm的文字，从而缩短推理时长 从 LangChain + LLM 的流程图可以看出，embedding 的召回率、LLM 的回答能力都会影响到最终回答的准确率。所以，要如果你遇到了 bad case ，你应该先确认这个 bad case 是召回错误，还是模型回答错误。下面使用庆余年这本书示范的例子，这里的LLM和Embedding模型均来自于Huggingface。from langchain.indexes.vectorstore import VectorStoreIndexWrapperfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import FAISSfrom langchain_community.document_loaders import TextLoaderdef load_text_save_index(file_path,index_name): loader = TextLoader(file_path) text_qyn = loader.load() splitter = RecursiveCharacterTextSplitter( chunk_size=100, # 分割出来的文本长度 chunk_overlap=10, # 块之间的重叠文字 length_function=len, # 计算每个块的长度 add_start_index=True # 决定是否在metadata中包含每个块在原始文档中的起始位置 ) texts = splitter.split_documents(text_qyn)[:100] faiss_db = FAISS.from_documents(texts, hf_embeddings) faiss_db.save_local(os.path.join(local_persist_path,index_name)) print('faiss db saved !')def load_index(index_name): index_path = os.path.join(local_persist_path, index_name) faiss_db = FAISS.load_local(index_path,embeddings=hf_embeddings,allow_dangerous_deserialization=True) index = VectorStoreIndexWrapper(vectorstore = faiss_db) return indexfile_path = '庆余年.txt'local_persist_path = './vector_store'# load_text_save_index(file_path,index_name='庆余年')index = load_index(index_name='庆余年')result = index.query(\"五竹是谁\", llm=llm)print(result)2.4 搜索助手对于Langchain而言，有专门的Agent模块，其支持对指令进行外部搜索。因此需要申请google搜索的APIkey，这里推荐SerpAPI的key。from langchain.agents import initialize_agentfrom langchain_community.agent_toolkits.load_tools import load_toolstools = load_tools(['serpapi','llm-math'],llm=llm)print(tools[1].name,tools[1].description)agent = initialize_agent(tools,llm,agent = 'zero-shot-react-description',verbose=True)agent.run('苹果的CEO，他10年后多少岁？')2.5 文章总结对于一个大型的文章而言，如果直接全部扔给LLM，无疑是会报显存错误的，因此需要将大的文本切分成小的docs，Langchain支持一下三种总结链方式将docs输入给LLM： stuff：将所有的docs汇总成一个总的提示直接塞给LLM，这里可能会字数太长而报错。 map_reduce：每个docs 依次输入LLM进行总结，并将每个总结的结果拼接后再次输入LLM作为汇总。 refine：通过循环遍历输入doc并逐步更新其答案来构建响应。对于每个doc，它将所有非文档输入、当前文档和最新的中间答案传递给LLM链以获得新的答案。 下面是对URL网页的文章做一个汇总的过程from langchain_community.document_loaders import UnstructuredURLLoaderfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.chains.summarize import load_summarize_chainfrom langchain_core.prompts import PromptTemplatedef load_news(url): text_splitter = RecursiveCharacterTextSplitter( # separators=['正文','撰稿'], chunk_size=300, chunk_overlap=10) loader = UnstructuredURLLoader([url]) data = loader.load_and_split(text_splitter) print(f'doc lenth is {len(data)}') return datadef summary_news(): map_prompt_temp = \"\"\"总结这段新闻的内容在50字以内：{text}, 总结：\"\"\" ch_prompt = PromptTemplate(template=map_prompt_temp, input_variables=['text']) chain = load_summarize_chain(llm, chain_type='map_reduce', map_prompt=ch_prompt, combine_prompt=ch_prompt) # summary = chain_ch.run(doc) summary = chain.invoke({\"input_documents\": doc})['output_text'] print(summary) # 这里展示的是中文的总结2.6 输出解析往往我们希望对于LLM生成的结果，能输出成我们预先定义好的格式。下面是对文章指定生成剧本的形式的过程。from langchain.output_parsers import PydanticOutputParserfrom pydantic import BaseModel,Fieldfrom typing import List# 注意：这里的BaseModel的类的description 不能用中文，因为到template的prompt的时候，会乱码，导致模型不懂描述的是什么class Line(BaseModel): # character:str = Field(description=u\"说这句台词的角色名字\",) character:str = Field(description=u\"The name of the character who said this line\",) # content:str = Field(description=u\"台词的具体内容，其中不再包含角色的名字\") content:str = Field(description=u\"The specific content of the line, which no longer contains the character's name\")class JuBen(BaseModel): # script: List[Line] = Field(description=u\"一段的台词剧本\") script: List[Line] = Field(description=u\"A talk script\")def parse_process(): temp = \"\"\"我将给你一段文章，请按照要求把这段文章改写成一个电视剧的剧本。 文章：\"{docs}\" 要求：\"{request}\" {output_instructions} \"\"\" parser = PydanticOutputParser(pydantic_object=JuBen) prompt = PromptTemplate(template=temp, input_variables=['docs', 'request'], partial_variables={\"output_instructions\": parser.get_format_instructions()}, # pattern = re.compile('\\n') ) jb_content = prompt.format_prompt(docs=docs, request=\"风格大胆悲情，剧本对话角色不少于三个人，以他们的自我介绍为开头\") # msg = [HumanMessage(content=jb_content)] # rs = llm.predict_messages(msg) rs = llm(jb_content.to_string()) jb = parser.parse(rs) # chain = jb_prompt | llm | parser # xiangsheng = chain.invoke({ # \"docs\" : docs, # \"request\" : \"风格大胆悲情，剧本对话角色不少于三个人，以他们的自我介绍为开头\" # }) # print(jb) return jb"
    } ,
  
    {
      "title"       : "pytorch_lightning使用体验",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./pytorch_lightning%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C.html",
      "date"        : "2024-01-18 08:00:00 +0800",
      "description" : "记录pytorch_lightning的使用过程心得体会",
      "content"     : "如果是一些小模型想要快速实验，不想怎么写代码的，可以通过pytorch_lightning快速搭建模型，但是如果涉及到大模型，以及分布式训练预测，咱还是老老实实用pytorch吧。一. 使用体验就像很多年前写过tensorflow之后看到keras后的欣喜，当我看到pytorch_lightning后瞬间就喜欢上了它！对于pytorch的重度使用者来说，每次都要写很多重复的训练预测代码，总感觉代码复用起来很麻烦，于是pytorch_lightning它来啦！pytorch_lightning的优势： 代码可读性、复用性高 自由度和pytorch一样高，并没有像使用keras一样感觉封装过死的感觉。 能像keras一样快速搭建模型，简化模型训练和预测的过程 支持分布式训练 二. 安装和使用官网地址是：https://lightning.ai/pip进行安装：pip show pytorch_lightning下面使用MNIST来展示如何使用pytorch_lightning来简化自己的代码2.1 数据模块LightningDataModule通常情况下，我们需要做一些预处理，以及在定义完自己的dataset后，需要定义dataloader，这里可以直接继承LightningDataModule模块，直接重写其中的方法即可。class MNISTDataModule(LightningDataModule): def __init__(self,root_dir,val_size,num_workers,batch_size): super(MNISTDataModule, self).__init__() self.save_hyperparameters() def prepare_data(self): \"\"\" download data once \"\"\" MNIST(self.hparams.root_dir, train=True, download=True) MNIST(self.hparams.root_dir, train=False, download=True) def setup(self, stage=None): \"\"\" setup dataset for each machine \"\"\" dataset = MNIST(self.hparams.root_dir, train=True, download=False, transform=T.ToTensor()) train_length = len(dataset) self.train_dataset, self.val_dataset = \\ random_split(dataset, [train_length - self.hparams.val_size, self.hparams.val_size]) def train_dataloader(self): return DataLoader(self.train_dataset, shuffle=True, num_workers=self.hparams.num_workers, batch_size=self.hparams.batch_size, pin_memory=True) def val_dataloader(self): return DataLoader(self.val_dataset, shuffle=False, num_workers=self.hparams.num_workers, batch_size=self.hparams.batch_size, pin_memory=True)2.2 训练和预测模块LightningModule之前每次训练和预测模型的时候，我都会写一个该过程的一个基类，来封装每个epoch模型训练、验证的过程，其实每次不同的项目、不同的模型继承了上述的基类，但是基本上也就是改变其中的每个batch训练、验证的方法，然后看到了一个别人封装的这么完美的训练预测基类，简直开心的不要不要的。。class MNISTModel(LightningModule): def __init__(self, hidden_dim, num_classes, lr, num_epochs): super().__init__() self.save_hyperparameters() self.net = LinearModel(self.hparams.hidden_dim) self.training_step_outputs = [] self.validation_step_outputs = [] self.loss_fn = nn.CrossEntropyLoss() self.accuracy = torchmetrics.Accuracy( task=\"multiclass\", num_classes=self.hparams.num_classes ) self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.hparams.num_classes) def forward(self, x): return self.net(x) def configure_optimizers(self): self.optimizer = Adam(self.net.parameters(), lr=self.hparams.lr) scheduler = CosineAnnealingLR(self.optimizer, T_max=self.hparams.num_epochs, eta_min=self.hparams.lr / 1e2) return [self.optimizer], [scheduler] def lr_scheduler_step(self, scheduler, *args, **kwargs): scheduler.step() def _common_step(self, batch, batch_idx): images, labels = batch logits_predicted = self(images) loss = self.loss_fn(logits_predicted, labels) acc = self.accuracy(logits_predicted, labels) # acc = torch.sum(torch.eq(torch.argmax(logits_predicted, -1), labels).to(torch.float32)) / len(labels) return loss, acc def training_step(self, batch, batch_idx): loss, acc = self._common_step(batch,batch_idx) self.log('lr', get_learning_rate(self.optimizer)) self.log('train_step_loss', loss) train_rs = {'train_loss': loss, 'train_acc': acc} self.training_step_outputs.append(train_rs) return loss def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0): rs = self(batch[0]) rs = torch.argmax(rs, -1).numpy().tolist() return rs def validation_step(self, batch, batch_idx): loss, acc = self._common_step(batch, batch_idx) log = {'val_loss': loss, 'val_acc': acc} self.validation_step_outputs.append(log) return log2.3 callbacks这里如果我们觉得上面这些无法满足我们的日常训练、预测的需求，那么完全可以再增加一些其他需要的第三方和自己定义的callbacks，当然pytorch_lightning其实已经封装了很多常用的callbacks了，比如下面的几个常用的： 模型定义怎么保存ckpt：ModelCheckpoint 如何定义训练及早停止：MINISTCallBack 定义进度条：TQDMProgressBar 当然了，我们想定义属于自己的callback怎么弄呢：class MINISTCallBack(Callback): def __init__(self): super(MINISTCallBack, self).__init__() def on_predict_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"): print(\"Predict is ending\") def on_train_epoch_end(self, trainer : \"pl.Trainer\", pl_module: \"pl.LightningModule\"): epoch_mean_loss = torch.stack([x['train_loss'] for x in pl_module.training_step_outputs]).mean() epoch_mean_acc = torch.stack([x['train_acc'] for x in pl_module.training_step_outputs]).mean() pl_module.log(\"train/loss\", epoch_mean_loss, prog_bar=True) pl_module.log(\"train/acc\", epoch_mean_acc, prog_bar=True) pl_module.training_step_outputs.clear() def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"): epoch_mean_loss = torch.stack([x['val_loss'] for x in pl_module.validation_step_outputs]).mean() epoch_mean_acc = torch.stack([x['val_acc'] for x in pl_module.validation_step_outputs]).mean() pl_module.log('val/loss', epoch_mean_loss, prog_bar=True) pl_module.log('val/acc', epoch_mean_acc, prog_bar=True) pl_module.validation_step_outputs.clear()2.4 调用当我们都写完了上述我们定义好的数据模块，训练预测模块，那么如何使用呢？pytorch_lightning这里用了一个专门的类Trainer来调用。训练调用：trainer = Trainer(max_epochs=config.num_epochs, # resume_from_checkpoint = 'ckpts/exp3/epoch=7.ckpt', # 断点续训 callbacks=callbacks, logger=logger, enable_model_summary=True, # 显示模型构造 accelerator='auto', devices=1, # 多少个设备 deterministic=True, num_sanity_val_steps=1, # 正式训练之前跑一次validation 测试程序是否出错 benchmark=True, # cudnn加速训练（要确保每个batch同一个大小） ) # mnist_model.load_from_checkpoint('ckpts/exp3/epoch=7.ckpt') trainer.fit(mnist_model,mnist_data)预测调用，可以定义一个dataloader，也可以定义测试的数据模块，同时也能直接对单一一个tensor作为输入，进行预测：rs = trainer.predict(mnist_model, dataloaders=test_loader)rs = trainer.predict(mnist_model, datamodule=test_datamodule)三. 分布式训练pytorch_lightning也支持分布式，但是它只支持pytorch原生的DDP，作为被HuggingFace的accelerate圈粉的我。。。只能退坑了，拜拜👋🏻"
    } ,
  
    {
      "title"       : "配置python远程环境",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./%E9%85%8D%E7%BD%AEpython%E8%BF%9C%E7%A8%8B%E7%8E%AF%E5%A2%83.html",
      "date"        : "2023-12-28 08:00:00 +0800",
      "description" : "记录配置远程linux的python环境",
      "content"     : "由于工作基本基于python，所以本文主要记录配置远程linux的python环境一. 定义Docker位置 为什么要修改docker存储的位置？ 因为往往docker的缓存过大，导致最后打的镜像多了，最后占满空间了步骤： 获取当前docker所在存储位置：docker info | grep \"Docker Root Dir\" 停止docker服务：systemctl stop docker 移动整个路径至新路径：mv /var/lib/docker /data/docker 创建软连接：ln -s /data/docker /var/lib/docker 重启docker服务：systemctl start docker 可以通过第一个命令查看现在的docker存储路径二. 安装python3或则conda 这里推荐安装conda，环境切换方便2.1 python3的安装2.1.1 前期准备 注意这里需要将openssl升级到1.0.2，因为不然pip3 安装包的时候会出现无法访问http请求。步骤：#安装openssl 1.0.2r版本cd /home/installwget http://www.openssl.org/source/openssl-1.0.2r.tar.gz #下载openssl包tar -zxvf openssl-1.0.2r.tar.gz #解压cd openssl-1.0.2r #进入文件夹./config shared zlib #配置make &amp;&amp; make install #解析和安装make clean //清除掉配置编译的一些文件rm -rf openssl* #删除 可以保留mv /usr/bin/openssl /usr/bin/openssl.bak #复制老的做备份mv /usr/include/openssl /usr/include/openssl.bak #复制老的做备份ln -sf /usr/local/ssl/bin/openssl /usr/bin/openssl #建立新的软链接 usr/local/ssl/为安装路径ln -sf /usr/local/ssl/include/openssl /usr/include/openss #建立新的软链接 usr/local/ssl/为安装路径echo \"/usr/local/ssl/lib\" &gt;&gt; /etc/ld.so.conf #写入openssl库文件的搜索路径ldconfig -v #使修改后的/etc/ld.so.conf生效openssl version #查看新版号2.1.2 安装python3.8 安装依赖包：yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel 安装wget：yum install wget 下载python3.8源码包：wget https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz 解压压缩包：tar -zxvf Python-3.8.1.tgz 进入文件夹：cd Python-3.8.1 配置安装目录并指定使用openssl： ./configure --prefix=/usr/local/python3 --with-openssl=/usr/local/ssl 编译安装：make &amp;&amp; make install 查看python3的路径：which python3 我们可以看到在/usr/local/bin/python3 添加python3的软链接：ln -s /usr/local/python3/bin/python3.8 /usr/local/bin/python3 添加 pip3 的软链接：ln -s /usr/local/python3/bin/pip3.8 /usr/local/bin/pip3 如果上述无法添加软连接，直接删除之前创建的软连接再次添加即可：rm -rf /usr/local/bin/python3和rm -rf /usr/local/bin/pip32.1.3 设定pip/pip3源 为什么要指定源，下载速度更快步骤： 进入个人目录：cd ~ 创建文件夹：mkdir .pip 进入pip文件夹：cd .pip 编辑文件pip.conf：[global]index-url=https://pypi.mirrors.ustc.edu.cn/simple2.2 安装conda2.2.1 下载minicoda并安装 官网地址：Miniconda — conda documentation 下载安装包之后，执行命令：sh Miniconda3-latest-Linux-x86_64.sh 安装完成后，查看conda版本是否安装成功，这里最好关掉当前窗口，新开一个：conda --version 2.2.2 创建虚拟环境 安装python3.8的python环境：conda create -n chatglm python=3.8 查看虚拟环境（当前是在base环境）：conda env list 进入创建的chatglm环境：conda activate chatglm 推出chatglm环境：conda deactivate 删除chatglm环境：conda remove -n chatglm --all 退出conda的所有环境（首先要先回到conda的base环境）：conda deactivate 直接进入conda的指定环境：conda activate chatglm 复制conda环境：conda create -n new_env --clone base 2.2.3 自动切换到指定的env环境由于每次连上linux后，发现conda的环境都是base环境，但是想要自己工作的env环境就需要来回切换，很不方便。所以这里推荐设置每次自动切换到指定的env环境 新增.zshrc一行：source activate my_env三. 配置vscode远程+pycharm远程3.1 vscode远程+免密登陆步骤： 在本地机器生成密钥对(公钥+私钥)：ssh-keygen 私钥放本机，公钥放远程(~/.ssh路径下) 在远程机器用公钥生成authorized_keys： 进入home目录下的.ssh文件夹：cd ~/.ssh cat id_rsa.pub &gt;&gt; authorized_keys vscode config文件加入本机私钥路径3.2 pycharm远程步骤： 新建一个工作目录，同时在远端也新建一个同名目录 然后选择远程conda的python解释器，同时将本地目录和远程目录进行映射 在deployment的configuration下配置登录远程的账号密码 四. 美化terminal 安装 zsh：yum install -y zsh 切换默认 Shell 为 zsh：chsh -s /bin/zsh 安装 Oh My Zsh：sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 显示绝对路径：vim ~/.oh-my-zsh/themes/robbyrussell.zsh-theme 修改这行，把c改成d：PROMPT+=' %{$fg[cyan]%}%d%{$reset_color%} $(git_prompt_info)'"
    } ,
  
    {
      "title"       : "pyspark使用心得",
      "category"    : "",
      "tags"        : "代码, 机器学习",
      "url"         : "./pyspark%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97.html",
      "date"        : "2023-12-27 08:00:00 +0800",
      "description" : "记录pyspark的使用体会",
      "content"     : "一. 安装 要使用PySpark，本地要有Java开发环境。 Java 8 : brew install --cask homebrew/cask-versions/adoptopenjdk8 pyspark安装：pip install pyspark 二. 和pandas之间的代码使用2.1 读取csv spark在读取csv上优势就很明显了，能直接快速读取几个G的大文件pandas读取大的csv，只能将其拆分为多个chunk进行读取，假如我们直接读取csv，可能会直接报内存不够导致进程被干掉。import pandas as pddf = pd.read_csv(path, index_col=False, iterator=True, chunksize=100000)for df_i in df: print(df_i)pyspark读取csv，快速高效from pyspark.sql import SparkSessionspark = SparkSession.builder.appName('learn').master(\"local\").getOrCreate()print(spark)df = spark.read.csv(path,header=True)2.2 写csvpandas写入csvdf.to_csv('test.csv',index=False)pyspark写入csv时，指定某个目录，这里推荐使用repartition(1)，让所有分区文件合并成一个，不然得话存储为多个分片文件spark_df.repartition(1).write.csv(\"data/\", encoding=\"utf-8\", header=True,mode='overwrite')2.3 构建Dataframepandas构建dataframedf = pd.DataFrame([['Sport', 1, 1], ['Flow', 2, 9], ['Sport', 2, 2],['Hear', 1, 6]], columns=['type', 'lenth', 'score'])pyspark构建dataframespark_df = spark.createDataFrame([['Sport', 1, 1], ['Flow', 2, 9], ['Sport', 2, 2],['Hear', 1, 6]],                                 ['type', 'lenth', 'score'])pandas的dataframe 转 pyspark的dataframespark_df = spark.createDataFrame(df)spark_df.show()2.4 自定义函数 在处理同一批数据（130w条样本测试）时，使用pyspark（local模式）处理需要0.05s，而pandas的apply函数则需要15s，快了300倍！pandas的自定义函数applydef is_sport(x): if x == 'sport': return 1 else: return 0df['is_sport'] = df['type'].apply(is_sport)pyspark的自定义函数udffrom pyspark.sql import functions as Ffrom pyspark.sql.types import IntegerTypetype_formater = F.udf(is_sport,IntegerType())new_type = type_formater(F.col('type')).alias('new_type')spark_df.select(['type','lenth',new_type]).show()2.5 查询函数pandas查询函数querydf = df.query('score == 1')pyspark查询函数filterspark_df.filter(\"score == 1\").show()2.6 分组聚合函数pandas分组函数groupbydf.groupby('type').sum()pyspark分组函数groupByspark_df.groupBy('type').sum().show()三. 机器学习3.1 构建特征VectorAssembler是一个Transformer，用来将数据集中多个属性按次序组合成一个类型为向量vector的属性。from pyspark.ml.feature import VectorAssemblerfeatureassembler=VectorAssembler(inputCols=[\"lenth\",\"score\"],outputCol=\"Features\")output=featureassembler.transform(spark_df)output.show()3.2 构建label使用StringIndexer来构建数据集的label，默认的index是从0开始indexer=StringIndexer(inputCol=\"type\",outputCol=\"label\")output=indexer.fit(output).transform(output)output.show()3.3 训练模型选择需要的特征后，将数据集拆分，进行训练，这里使用的随机森林模型finalized_data=output.select(\"Features\",\"label\")train_data,test_data=finalized_data.randomSplit([0.9,0.1])rf=RandomForestClassificationModel(labelCol='label',featuresCol='Features',numTrees=20,maxBins=122)rf=rf.fit(train_data)rf.save('./model')"
    } ,
  
    {
      "title"       : "分布式训练推理Accelerate",
      "category"    : "",
      "tags"        : "NLP, 深度学习",
      "url"         : "./%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86.html",
      "date"        : "2023-08-16 23:00:00 +0800",
      "description" : "记录accelerate在分布式训练和推理中的使用过程",
      "content"     : "分布式训练的加速策略详情可见本人上篇文章《 大语言模型的应用及训练》，一般有三种，分别是数据并行，流水线式并行和张量并行，本文主推使用hugging face的accelerate库来进行模型分布式的训练和推理。一. torch常用的分布式训练工具1.1 DataParallelDP(DataParallel)：实现数据并行方式的分布式训练，采用的是PS(worker-server)模式，不推荐。 单进程多线程 只能在单机上使用 训练速度慢，且由于是PS模式（存在负载不均衡的问题），随着worker的个数增多，训练速度越慢1.2 DistributedDataParallelDDP(DistributedDataParallel)：实现数据并行方式的分布式训练，采用的是ring-all-reduce模式。它将模型复制到每个 GPU 上 ，同时复制了每个dataloader，并且当 loss.backward() 被调用进行反向传播的时候，所有这些模型副本的梯度将被同步地平均/下降 (reduce)。这确保每个设备在执行优化器步骤后具有相同的权重。 多进程 支持多机多卡 训练速度较DP快，ring-all-reduce模式下，所有worker只和自己相邻的两个worker进行通信 1.3 ampapex由英伟达开发了一个支持半精度自动训练的pytorch拓展插件。Apex 对混合精度训练的过程进行了封装，从而大幅度降低显存占用，节约运算时间。torch 原生支持的amp，pytorch的版本一定&gt;1.6。不用额外再装apex，所以推荐使用这种方式使用amp。from torch.cuda.amp import autocast as autocast, GradScaler# 在训练最开始之前实例化一个GradScaler对象scaler = GradScaler()for input, target in tqdm(loader): # 前向过程(model + loss)开启 autocast with autocast(): output = model(input) loss = loss_fn(output, target) # Scales loss，这是因为半精度的数值范围有限，因此需要用它放大 scaler.scale(loss).backward() scaler.step(optimizer) scaler.update()二. Accelerate分布式训练和推理Accelerate：在无需大幅修改代码的情况下完成并行化。同时还支持DeepSpeed的多种ZeRO策略，简直不要太爽。 代码效率高，支持无论是单机单卡还是多机多卡适配同一套代码。 允许在单个实例上训练更大的数据集：Accelerate 还可以使 DataLoaders 更高效。这是通过自定义采样器实现的，它可以在训练期间自动将部分批次发送到不同的设备，从而允许每个设备只需要储存数据的一部分，而不是一次将数据复制四份存入内存。 支持DeepSpeed：无需更改代码，只用配置文件即可对DeepSpeed开箱即用。 2.1 分布式推理当模型参数大到单张卡放不下时候（哪怕batchsize为1也会报显存不足的情况），这里就需要将大模型中的不同layer放到不同的GPU上，而每个GPU只负责其中一部分训练，当然数据在不同卡上流转的时候，都需要自动将数据放到对应的卡上。from accelerate import dispatch_model# device_map设置为自动的最方便了，不用自己设计把模型的layer分配到哪个GPUmodel = dispatch_model(model, device_map=\"auto\")# 打印device_mapprint(model.hf_device_map)print(f'memory_allocated {torch.cuda.memory_allocated()}')2.2 分布式训练下面是官方的例子，只需要更新几行代码即可开启分布式训练之旅啦！ accelerator = Accelerator()实例化 accelerator.prepare把我们的模型、数据、优化器等等都放进accelerate里面，让他帮我们操作 accelerator.backward(loss)替换掉常用的loss.backword() 如果需要梯度裁剪，这里必须使用accelerator.clip_grad_norm_() 涉及到模型存储的时候，需要unwrap 下模型,因为在通过 prepare() 方法时，模型可能被 wrap 从而用于分布式训练 import torch import torch.nn.functional as F from datasets import load_dataset+ from accelerate import Accelerator+ accelerator = Accelerator()- device = 'cpu'+ device = accelerator.device model = torch.nn.Transformer().to(device) optimizer = torch.optim.Adam(model.parameters()) dataset = load_dataset('my_dataset') data = torch.utils.data.DataLoader(dataset, shuffle=True) + model, optimizer, data = accelerator.prepare(model, optimizer, data) model.train() for epoch in range(10): for source, targets in data: source = source.to(device) targets = targets.to(device) optimizer.zero_grad() output = model(source) loss = F.cross_entropy(output, targets)- loss.backward()+ accelerator.backward(loss) optimizer.step() # 等待所有进程达到一定程度后再执行指令 accelerator.wait_for_everyone() # 只在主进程中保存模型 if self._accelerator.is_main_process: + unwrapped_model = accelerator.unwrap_model(model) + accelerator.save(unwrapped_model,model_path) - torch.save(unwrapped_model.state_dict, \"./model/accelerate.pt\")之后，只需要配置下accelerate的config文件，使用accelerate launch --config_file default_config.yaml train.py启动脚本开始训练啦！三. accelearte使用例子3.1 单机多卡下面是以单机多卡（1机器共2卡）的config.yaml例子，这里是我根据accelerate config 生成后的config文件：compute_environment: LOCAL_MACHINEdistributed_type: MULTI_GPUdowncast_bf16: 'no'gpu_ids: allmixed_precision: 'no'num_machines: 1num_processes: 2rdzv_backend: staticsame_network: falsetpu_env: []tpu_use_cluster: falsetpu_use_sudo: falseuse_cpu: false3.2 多机多卡下面是以多机多卡（2机器共4卡）的config.yaml例子，这里是我根据accelerate config 生成后的config文件：compute_environment: LOCAL_MACHINEdistributed_type: MULTI_GPUdowncast_bf16: 'no'gpu_ids: allmachine_rank: 0main_process_ip: 主机器的ipmain_process_port: 端口号main_training_function: mainmixed_precision: 'no'num_machines: 2num_processes: 4rdzv_backend: staticsame_network: falsetpu_env: []tpu_use_cluster: falsetpu_use_sudo: falseuse_cpu: false3.3 DeepSpeed集成在accelerate上的使用Deepspeed集成了ZeRO的三种方式，在accelerate中，我们可以直接根据配置文件来选择使用： Stage 1：将 optimizer states 分片到数据并行 workers/GPUs 上。 Stage 2：将 optimizer states + gradients 分片到数据并行 workers/GPUs 上。 Stage 3 ：将 optimizer states + gradients + model parameters 分片到数据并行 workers/GPUs 上。 Optimizer Offload：将 optimizer states + gradients 卸载到 CPU/Disk ，建立在 ZERO Stage 2 之上。 Param Offload：将 model parameters 卸载到 CPU/Disk ，建立在 ZERO Stage 3 之上。下面是使用stage3的accelerate配置的例子：compute_environment: LOCAL_MACHINEdeepspeed_config: deepspeed_multinode_launcher: standard gradient_accumulation_steps: 4 offload_optimizer_device: none offload_param_device: none zero3_init_flag: false zero3_save_16bit_model: false zero_stage: 3distributed_type: DEEPSPEEDdowncast_bf16: 'no'machine_rank: 0main_process_ip: 主机器的ipmain_process_port: 端口号main_training_function: mainmixed_precision: 'no'num_machines: 2num_processes: 4rdzv_backend: staticsame_network: falsetpu_env: []tpu_use_cluster: falsetpu_use_sudo: falseuse_cpu: false在从机器上我们只需要复制这个config文件和所有的代码数据，并把config.yaml中的machine_rank改成1即可。两台机器同时启动脚本：accelerate launch --config_file default_config.yaml train.py"
    } ,
  
    {
      "title"       : "大语言模型的应用及训练",
      "category"    : "",
      "tags"        : "NLP, 深度学习, LLM",
      "url"         : "./%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8.html",
      "date"        : "2023-07-18 23:00:00 +0800",
      "description" : "记录LLM的应用及训练过程",
      "content"     : "一. 大语言模型LLM1.1 LLM的发展史LLM(Large Language Model大语言模型)的发展起源应该是从Transformer开始，在chatGPT出现后热度达到顶峰。整个发展的过程可以概括为三个阶段： 第一阶段：真对某些领域数据单独微调，出现大量的预训练模型 第二阶段：扩大模型参数及训练语料的规模，模型架构偏向于生成式模型，Prompt在微调阶段开始展现。 第三阶段：模型参数和数据的规模急剧扩大，注重模型和人之间的交互。AI的安全性、可靠性更加收到关注。 1.2 开源可用的中文LLM目前公认较好的中文开源大语言模型如下： Meta AI发布的Llama系列模型，目前已经到Llama2了，推荐一个中文的Llama2仓库 清华大学的chatGLM系列模型，目前已经到chatGLM2了，推荐官方的chatGLM2仓库 1.3 LLM应用的NLP任务现在LLM基本都是生成式模型，因此一般可应用的NLP任务： 翻译 文本摘要 阅读理解 知识问答 二. LLM的训练大语言模型训练面临最大的挑战主要有以下2方面： 模型太大，一个GPU放不下 数据量太大，一个GPU训练时长太长 2.1 主流的训练加速方式目前主流的LLM训练的加速方式有以下三种： 数据并行DataParallel：N个GPU上放置同一个模型（模型复制N份），将数据切分成N份。每台GPU都独立地执行前向计算过程(获得损失loss)和后向传播过程（计算梯度），之后对所有GPU上的梯度同步后进行参数更新（或直接同步更新后的参数）。 流水线并行PipelineParallal：N个GPU上放置一个模型的不同layer，同一份数据（数据复制N份）。每个GPU只负责其中一部分layer的训练，数据在不同卡上流转的时候，都需要自动将数据放到对应的卡上。 张量并行TensorParallel：可以理解为数据并行+流水线并行（强力推荐）。N个GPU上放置一个模型的不同layer，同时将数据切分成N份，即每个GPU上对应模型的一份layer和一份数据。 那么为什么要推荐使用张量并行的方式来进行加速呢？无论是数据并行还是流水线并行，当模型足够大时，在前向和后向传播时，GPU之间的所需要传递的信息（包括模型的参数/梯度/优化器状态）就会很大，那么GPU之间通信所需的时间就会很长，而这段时间中其他的GPU其实是处于空置状态，很浪费资源。2.2 DeepSpeed的ZeRO DeepSpeed是微软的分布式框架，而它的核心就是ZeRO(Zero Redundancy Optimizer)，《ZeRO: Memory Optimizations Toward Training Trillion Parameter Models》的论文地址DeepSpeed的优势如下： 支持更大参数量级的模型 训练速度更快 开销更少 代码使用更方便，更改更少 ZeRO的三种主要优化方式： 优化器状态拆分：4倍显存使用的减少，不会带来额外的通信时间。 优化器状态+梯度拆分：8倍显存使用的减少，这种需要额外的通信时间来同步梯度信息。 优化器状态+梯度+模型参数拆分：显存减少的量级随着GPU的个数成正相关（比如有64张卡，那么就会有64倍的显存减少），但这种也会存在最大的通信开销。 ZeRO第三种方式整体过程： 前向传播（蓝色部分） 将数据切分为4份 $D_0,D_1,D_2,D_3$，模型切分成4份 前向传播时候，对于$D_0$的数据经过$GPU_0$后得到其模型参数$M_0$，然后将模型参数也复制到其他三张GPU上。每张GPU上针对自己的数据都会跑$M_0$的模型参数，一旦$M_0$结束，其他三张GPU删除$M_0$的参数。 同上，每份数据和模型都经过上面的流程。前向过程全部结束后，针对每张GPU所代表的每份数据都要计算其loss。 后向传播（橙色部分） 对于$GPU_0,GPU_1,GPU_2$ 而言，在经过各自对应的数据时(这里注意所有的GPU都会存放$GPU_3$的模型参数)，会短暂的保持住$M_3$的梯度信息。 $GPU_0,GPU_1,GPU_2$ 都会把计算得到的梯度进行累积传递到$GPU_3$上，之后会把模型参数和梯度信息都删除。 同上，依次经过$GPU_2,GPU_1,GPU_0$。在梯度累加的过程中，使用优化器的参数进行更新 2.3 LLM微调对于一般的模型进行预训练时，基本都会选择基于预训练模型对下游任务进行微调。但是面对大模型和海量数据，全参数微调明显不现实，所以就需要引入高效的微调方法PEFT(Parameter-Efficient Fine-Tuning)。然后推荐以下几种高效微调技术，都是集成在accelerate中，方便调用。 Adapter：Parameter-Efficient Transfer Learning for NLP，是最早发布的参数高效微调技术之一，在 Transformer 架构中添加更多层，并且只对它们进行微调，而不是对整个模型进行微调。 LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS，其工作原理是修改神经网络中可更新参数的训练和更新方式。预训练神经网络的权重矩阵是满秩的，这意味着每个权重都是唯一的，不能通过组合其他权重来制作。当将预训练的语言模型调整为新任务时，权重具有较低的“内在维度”。这意味着权重可以用较小的矩阵表示，或者它具有较低的秩。这又意味着在反向传播期间，权重更新矩阵具有较低的秩，因为预训练过程已经捕获了大部分必要的信息，并且在微调期间仅进行特定于任务的调整。 Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning，这是第一篇基于仅使用软提示进行微调的想法的论文之一。P-Tuning和Prefix Tuning的思想来自于这篇论文。 P-Tuning: GPT Understands, Too，方法旨在优化传递给模型的提示的表示。根据prompt来创建一个小型编码器网络，因此，在使用时候，应该创建一个prompt的提示模版。 Prefix Tuning: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks，P-Tuning的第二版，将可学习参数添加到所有层当中，确保了模型本身能够更多地了解正在对其进行微调的任务。与 P-Tuning 的区别在于，没有完全修改prompt嵌入，而是在Transformer的每一层提示的开头添加很少的可学习参数。它效果很好的一个重要原因是可训练参数的数量不仅限于输入序列。每一层都添加了可学习的参数，使模型更加灵活。"
    } ,
  
    {
      "title"       : "事件抽取实战",
      "category"    : "",
      "tags"        : "NLP, 深度学习",
      "url"         : "./%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E5%AE%9E%E6%88%98.html",
      "date"        : "2023-02-14 10:00:00 +0800",
      "description" : "记录实战中NLP的事件抽取",
      "content"     : "一. 事件抽取1.1 事件抽取定义在NLP中，我们经常听到NER实体抽取，但是对于事件抽取却相对陌生。这里简单介绍下什么是事件抽取。 定义：对非结构化的文本抽取感兴趣的事件信息，并用结构化（论元）进行展示，即事件要素提取。 应用：抽取的事件可以广泛应用在舆情发现领域；同时事件抽取其实也是知识图谱的构建中的重要一环。 下面用百度的LIC2021事件抽取任务来举例，事件抽取到底做了什么事？如上图所示，给定了一个句子，通过事件抽取我们可以得到： 2个目标事件：竞赛行为-胜负，竞赛行为-夺冠 各自事件的论元：比如事件的发生时间/地点，以及胜者、败者、冠军分别是谁。1.2 事件抽取拆分通过上面的例子，我们可以看出其实所谓的事件其实就是将论元（实体）、触发词（实体）在文本中抽取出来；同时，也需要将论元和触发词进行关系匹配。因此，我们可以将事件抽取任务进行拆分，即： 实体抽取（NER）：抽取触发词（动作实体）、抽取论元（名次实体） 关系抽取（RE）：判定俩俩实体之间是否存在关系（什么类型关系）二. 相关模型实战目前，事件抽取模型一般分为pipeline模型（流水线）和联合抽取模型。 pipeline流水线式模型：先训练一个实体抽取模型，然后再训练一个关系抽取模型，两个模型相互不影响。 joint联合抽取式模型：将实体抽取和关系抽取放在同一个模型上，这里的loss = 实体抽取loss + 关系抽取loss。 由于BERT的预训练模型在样本较少的情况下，文本特征抽取上有很大优势，因此下面所有实战的模型基本都是基于BERT的。 本人更喜欢torch，因此下面所有实战的代码都是基于pytorch框架的。2.1 Pipeline模型我这边使用的方案是： NER模型：BERT + BiLSTM + CRF RE模型：Enriching Pre-trained Language Model with Entity Information for Relation Classification2.1.1 NER模型 推荐的git仓库：https://github.com/cooscao/Bert-BiLSTM-CRF-pytorchNER模型这边选用的文本标注模式为BIOES这种模式，其原因是对比BIO这种标注模式而言，对于实体最尾端的“E”标识能更好的帮助我们在预测时找到实体的尾端，对于错误实体的修正更加有帮助。比如预测的结果为”I-I-I-O-I-I”（BIO）可能识别为2个实体，然是如果是BIOES预测结果为“I-I-I-O-I-E”则其实就是一个实体。这边主要介绍下选用的层的各自作用： BiLSTM层：能解决文本实体的顺序问题，比如抽取的时间中要区分“开始时间”和“结束时间”。 CRF层：给最终预测的标签添加一些约束来保证预测的标签合法性（其实效果不是很显著）2.1.2 RE模型 推荐的git仓库：https://github.com/monologg/R-BERT选用该模型的原因：它的模型结构和我起初看的snow ball(雪球算法)很像，在抽取的关系规则上很近似，都是开始词向量 + 实体1 + 中间词向量 + 实体2 + 尾词向量。这里再推荐一篇基于上面稍加改造的文章（很秀，就加上了“开始词向量” + “中间词向量” + “尾词向量”就能发文章了。。。）：基于信息增强BERT的关系分类模型结构： BERT层：文本特征抽取 表征关系：将[cls]表征的全句特征 + 实体1的特征 + 实体2的特征 分类层：输出关系分类 这里需要说下关于这个关系抽取模型的弊端：当一条文本中包含大量的实体对需要预测其关系时，哪怕是用batch预测，消耗的时间也是很长的。 因此如果真的是用这个模型的话，还是推荐自己改下dataset格式和模型结构，具体可以参考我下面使用的Spert模型结构中的关系抽取部分。2.2 Joint联合抽取模型我这边选用的模型是：Span-based Entity and Relation Transformer原因其实是上面的NER模型用的是BIOES这种数据模式，联合模型我这边就想用span这种标注方式。span最大的优势就是能够解决实体嵌套的问题。比如”武汉长江大桥”这个实体，其实本身是一个“桥”类实体，但是“武汉”两个字也是“市”类的实体，即实体嵌套。 官方的git仓库：https://github.com/lavis-nlp/spert在官方的代码中，模型的训练阶段和推理阶段其实模型结构是不同的： 训练阶段： 数据加载：创建实体的负样本和关系的负样本。实体的负样本可以理解为图像中的预选框，只不过这里是随机创建并抽样比指定最长实体短些的实体；关系的负样本则是基于俩俩实体是真正实体却没有关联。 模型部分：对输入的不同长度的span实体利用嵌入层表示，通过实体的分类器输出实体的logits；在关系分类中，这里是定义了max_pairs作为步长，预选的关系对作为最大长度，滑块的形式来预测关系对，最终将其拼接。 推理阶段： 数据加载：创建所有的实体候选 模型部分：同训练部分 + 实体过滤器（过滤为None的实体） 看了这么多的代码，还是觉得spert作者的这篇repo的代码最棒！三. 训练的小tricks 选用的BERT预训练模型（中文）我这边选用了一大一小两个模型来测试：chinese-roberta-wwm-ext和albert-chinese-tiny。 在训练大模型的时候，这里推荐freeze住bert层；而训练小模型的时候，不冻结bert层。 在训练的时候发现loss变为nan的时候，在模型和数据都没啥问题的情况下，推荐使用模型剪枝（clip_grad）来处理。 设置不同的学习率：在模型的训练阶段对于不同的层我们可能会需要设置不同的学习率，尤其是当我们在预训练模型的基础上对下游任务进行微调，对于预训练模型需要设置较小的学习率，而其他一些我们自定义的层则需要较大的学习率。 学习率预热：当设置较大的学习率的时候，会导致模型loss震荡；warm up则可以使得在在最初的几个epoch中lr较小，之后模型趋于稳定后，lr变大到和自己设置的lr一致。"
    } ,
  
    {
      "title"       : "docker使用总结",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./docker%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.html",
      "date"        : "2023-01-10 08:00:00 +0800",
      "description" : "记录docker的使用过程",
      "content"     : "一. 定义Docker的特点： 是一个工具，能快速方便的创建、运行、部署软件。 能将一个软件及其依赖打包成一个单独的库，方便移植。 能解决不同application之间版本不兼容的问题（比如一个是python2环境，一个是python3环境） 相较于虚拟机更为轻量，占用资源少，资源利用率高，运行速度快。Docker的应用： 提供一次性的环境 可组建微服务架构，大程序更方便扩容、稳定。二. docker使用流程2.1 docker的流程整个docker的使用流程基本是：编写docker file ==&gt; 创建image(镜像) ==&gt; 实例化为container(容器) ==&gt; 跑container。2.2 拉取使用的例子 这里用的是doccano这个NLP文本标注工具作为例子，记录下docker在使用该镜像时遇到的问题和总结。使用过程常用的命令： 拉取官方的一个doccano镜像：docker pull doccano/doccano 查看本地镜像列表：docker image list 将镜像实体化为容器：docker container create --name doccano -e \"ADMIN_USERNAME=admin\" -e \"ADMIN_EMAIL=admin@example.com\" -e \"ADMIN_PASSWORD=password\" -v doccano-db:/data -p 8000:8000 doccano/doccano 容器跑起来：docker container start doccano 查看docker的容器（在跑）列表：docker container ls 查看docker的容器（包含在跑/退出）列表：docker ps -as 查看运行日志：docker logs 上面查询到的容器的id 关闭指定的容器：docker container kill 容器的id 删除指定的容器：docker container rm 容器的id 进入到指定的容器中：docker exec -it 容器的id /bin/bash三. 创建自己的docker3.1 创建docker的镜像下面用简单的python脚本构建镜像（必须包含dockerfile文件和python脚本文件）。下面是整个项目的目录：├── Dockerfile # Dockerfile文件└── hello.py # python脚本：功能打印N行helloworld，N是命令行参数其中Dockerfile文件的内容如下：FROM python:3.9 COPY ./hello.py /hello.pyENTRYPOINT [\"python3\", \"hello.py\"]CMD [\"3\"] 如果是正常启动python脚本应该是：python3 hello.py 3 ，这里的3是打印多少行的参数。这里的CMD[“3”] 指的是其后面的参数设定，默认是3。 创建docker镜像：docker build . --tag helloworld 跑镜像：docker run helloworld 10 (这里跑10次hello world) 注意这里如果跑实例的时候，下面删除镜像的时候会冲突，需要删除这里跑起来的实例3.2 搭建并推送到本地的docker registry我们可以将自己创建好的docker放到docker仓库中，也就是dockerhub中。当然也可以从dockerhub拉取别人的image。 拉取docker官方的私有仓库镜像：docker pull registry 创建一个docker仓库：docker run -d -p 5050:5000 registry 将自己的镜像重新tag标记下镜像：docker tag helloworld localhost:5050/my-helloworld 镜像推送到本地仓库中：docker push localhost:5050/my-helloworld 可以将本地的helloworld和localhost:5050/my-helloworld镜像删除：docker rmi helloworld localhost:5050/my-helloworld 然后再从本地的私有仓库中拉取镜像：docker pull localhost:5050/my-helloworld3.3 推送镜像到docker hubdocker hub我们可以理解为一个云仓库，可以和github上类似，用以在远程仓库上存储自己的镜像，方便我们随时拉取。这里我们需要在dockerhub的官网上进行注册，然后创建属于自己的私有远程仓库。如下所示之后我们就可以将自己的镜像上传到dockerhub远程仓库中了。 在docker的destop上登陆自己的id，这样才能推到远程仓库。 tag标记下自己本地仓库和远程仓库的映射：docker tag localhost:5050/my-helloworld siwangtt/test-repo 推送到dockerhub远程仓库中：docker push siwangtt/test-repo至此，我们就可以在dockerhub上看到我们刚刚上传的镜像咯。"
    } ,
  
    {
      "title"       : "Selenium和webscraper爬虫",
      "category"    : "",
      "tags"        : "代码, 爬虫",
      "url"         : "./Selenium%E5%92%8Cwebscraper%E7%88%AC%E8%99%AB.html",
      "date"        : "2022-12-24 08:00:00 +0800",
      "description" : "记录自动化爬虫的使用过程和技巧",
      "content"     : "一. Selenium爬虫1. 安装 Selenium 本身是一个自动化测试的工具，模拟人为在浏览器上进行操作，比如点击和下拉等等。安装的环境： python环境 pip安装Selenium包：pip install selenium chrome浏览器驱动：去官网下载自己电脑已安装的浏览器所在的版本驱动我这边的chrome驱动装的是mac版本的，这里需要将其路径放到mac的环境变量中，下载后放到了/usr/local/chromedriver，vim ~/.profileexport PATH=\"$PATH:/usr/local/chromedriver\"source ~/.profile可以在控制台打chromedriver测试是否ok。2. 使用 这里的例子是的主要任务是：将给的头条url所在的公众号遍历其30条的文章标题和链接。所以我们需要拆解为2步： 打开给定的url，并点击头条的公众号。 另开一个页面，找到公众号下前30条文章的标题和链接（这里需要下拉才能看到后面的文章）。第一步：实例一个浏览器驱动，并找到公众号的XPath（这里推荐一个测试XPath的浏览器插件XPath Helper）driver = webdriver.Chrome()driver.get(url)time.sleep(2)try: name = driver.find_element(By.XPATH,\".//div[@class='article-meta']/span[@class='name']/a\").get_attribute('href')except: driver.quit()driver.find_element(By.XPATH,\".//div[@class='article-meta']/span[@class='name']/a\").click()由于点击了公众号后会出现一个新的页面，这里最好指定下现在的driver是归属于哪个页面的，不然写的xpath就会找不到元素。list_windows = driver.window_handlesprint(list_windows)driver.close()driver.switch_to.window(list_windows[1]) #list_windows 存储了上一步中获取的窗口time.sleep(2) # 防止页面还未加载完全第二步：找到所有的页面的文章（这里需要下拉才可以获取）data = []titles = set()last_position = driver.execute_script(\"return window.pageYOffset;\") # 执行下拉操作scrolling = Truecount = 0while scrolling: conti_type = True page_articles = driver.find_elements(By.XPATH, \".//div[@class='profile-article-card-wrapper']\") for art in page_articles[-15:]: art_url = art.find_element(By.XPATH, \".//div[@class='feed-card-article-l']/a\").get_attribute('href') art_title = art.find_element(By.XPATH, \".//div[@class='feed-card-article-l']/a\").text count += 1 if art_title: if art_title not in titles: titles.add(art_title) data.append((art_url,art_title)) if count &gt; 30: conti_type = False scrolling = False scroll_attempt = 0 while conti_type: # check scroll position driver.execute_script('window.scrollTo(0, document.body.scrollHeight);') time.sleep(0.5) curr_position = driver.execute_script(\"return window.pageYOffset;\") if last_position == curr_position: scroll_attempt += 1 # end of scroll region if scroll_attempt &gt;= 50: scrolling = False break time.sleep(1) else: time.sleep(0.5) # attempt another scroll else: last_position = curr_position break二. Web Scraper浏览器插件爬取 用过selenium之后，觉得如果能不写代码就能够实现上面的代码功能多好啊，确实有这样一个chrome插件可以进行替代——Web Scraper2.1 安装直接打开chrome应用商店进行搜索Web Scraper,下载安装插件即可。在chrome中打开一个页面，点击右键，选择检查，即可看到web scraper了。2.2 使用 还是利用上面的例子，这里用Web Scraper来不写代码进行爬取。 新建一个sitemap，选择create new sitemap, 填写名称和url，如下图 选择初始的选择器，这里选择的是公众号名字。注意这里是需要点击“公众号”，因此选择link。 在上面选择的选择器中点进去后，新建一个选择器，这里是需要下拉动作，因此选择element scroll down。 点击scroller，新建一个选择器，用以保存文章标题。 同时在scroller下新建一个选择器，用以保存文章链接。 我们可以通过查看graph的形式来查看整个workflow（点击sitemap下的Selector graph）。 至此，配置部分就全部完成了，现在只需要跑起来就行，点击sitemap下的Scrape即可。 三. AutoScraper推荐 这里推荐一个不需要xpath来选择元素的自动爬取器，很有意思。git地址：https://github.com/alirezamika/autoscraper这里其实类似于构建一个爬取器的模型，先选择一个链接中的几个独有的元素，然后放到autoscraper里面，让其知道需要爬取的位置在哪。from autoscraper import AutoScraperurl = 'https://stackoverflow.com/questions/2081586/web-scraping-with-python'# We can add one or multiple candidates here.# You can also put urls here to retrieve urls.wanted_list = [\"What are metaclasses in Python?\"]scraper = AutoScraper()result = scraper.build(url, wanted_list)print(result)然后再给予一个相同的链接，让其自动找到我们需要找的元素信息。scraper.get_result_similar('https://stackoverflow.com/questions/606191/convert-bytes-to-a-string')"
    } ,
  
    {
      "title"       : "模型压缩",
      "category"    : "",
      "tags"        : "深度学习, NLP",
      "url"         : "./%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9.html",
      "date"        : "2022-09-28 08:00:00 +0800",
      "description" : "记录模型压缩的技巧和方法",
      "content"     : "一.压缩的技巧1.1 模型剪枝(Network Pruning) 方法：将网络中的权重或者神经元进行删除，再次重新训练。 原因：大的网络中包含很多冗余参数，而小的网络很难训练，那就直接用大的网络删减成小网络就好了。 应用：属于神经网络就可以。1.2 结构设计(Architecture Design) 方法：通过设计更少参数的layer来替代现有layer（效果不变情况下） 原因：模型中有些layer可能参数很冗余，例如DNN。 应用：利用套用新的模型结构或者新的layer。1.3 参数量化(Parameter Quantization) 方法：将网络中常用的计算单位(float32/float64)压缩成更小的单位(比如int8) 原因：计算的单位变得更小，运算更快。 应用：对所有已经训练好的model使用；或者边训练边诱导model去量化。1.4 知识蒸馏(Knowledge Distillation) 方法：利用一个已经学好的大model，来教小model如何做好任务。 原因：让学生直接做对题目太难，可以偷看老师是怎么做题的。 应用：通常用于classification（也可以用于generative model生成式模型比如GAN），而且学生只能从头学起。1.5 混合使用的例子 这里其实在我面试的时候考过一次，当时只考虑了模型选型用mobilenet和参数量化，没有考虑到知识蒸馏和模型剪枝比如我现在需要一个极小的model来学习图像分类任务。这里完全可以使用以上技巧来混合压缩模型，如下图所示 可以先选择小的cv模型架构(这里选择mobilenet)来对大的图像分类模型(比如resnet101这种大且深的model)进行知识蒸馏。 然后将蒸馏后的mobilenet进行模型剪枝，进一步减少模型中冗余参数。 其次，利用参数量化将模型的计算单位压缩成int8，从而得到最终压缩后的模型。二. 知识蒸馏知识蒸馏最主要的问题其实是：蒸馏哪里？ 输出(Logits)： 直接匹配logits 让student学习teacher的logits distribution（在一个batch里面） 中间值(Feature)： 直接匹配中间的feature student学习teacher的Feature是如何转换的 2.1 输出蒸馏Distilling the Knowledge in a Neural Network 论文地址：https://arxiv.org/pdf/1503.02531.pdf 核心方法：让大的model的logits除掉T（原因是由于大model其实已经学很好，导致logits都是1，除掉T之后变得不要那么完美）之后得到soft target，然后让小model去学习hard target和soft target结合起来的loss。Deep Mutual Learning 论文地址：https://arxiv.org/pdf/1706.00384.pdf 核心方法：让两个network同时学习，互相学习对方的logits，同时让其看到真实的label的样子。Improved Knowledge Distillation via Teacher Assistant 论文地址：https://arxiv.org/pdf/1902.03393.pdf 核心方法：当小的model无法理解/学习大型model的时候，这里引出“Teacher Assistant”（助教）来作为中间model，起到学习大model，来教小model。2.2 中间值蒸馏FITNETS: HINTS FOR THIN DEEP NETS 论文地址：https://arxiv.org/pdf/1412.6550.pdf 核心方法：当小的model无法理解/学习大型model的时候，这里引出“Teacher Assistant”（助教）来作为中间model，起到学习大model，来教小model。2.3 知识蒸馏开源代码推荐 github 地址：https://github.com/airaria/TextBrewer这个repo是哈工大开源的知识蒸馏的工具包（基于pytorch、bert的工具包，偏向于文本）。下面是具体的使用的流程"
    } ,
  
    {
      "title"       : "文本标注工具Brat",
      "category"    : "",
      "tags"        : "深度学习, NLP",
      "url"         : "./%E6%96%87%E6%9C%AC%E6%A0%87%E6%B3%A8%E5%B7%A5%E5%85%B7brat.html",
      "date"        : "2022-08-19 08:00:00 +0800",
      "description" : "记录文本领域好用的标注工具",
      "content"     : "一. brat的介绍在文本领域，特别是需要对实体、关系进行标注的情况下，我们是十分需要一个开源便捷的打标工具帮助我们生成属于自己的数据集的。所以之前一直在想，文本里有没有像图像里标注labelme一样的开源打标工具呢？还真有，而且还不少。这里推荐一个本人用的很顺手的文本标注工具——brat。brat工具的特点： 使用方便，可直接应用于webserver端。 开源 支持实体、关系、事件的标注。 python写的，因此方便改写脚本(支持python2和python3) 支持中文展示 支持Linux、Mac系统二. brat的安装及使用1.2 安装 github仓库地址：https://github.com/nlplab/brat安装步骤： 直接clone仓库到本地 进入主目录下，并本地web服务：cd brat &amp; python standalone.py 在看到Serving brat at http://0.0.0.0:8001后，在网页中输入http://0.0.0.0:8001即可看到本地brat服务了。 注意这里需要注册和登录账号以上全部流程走完后，你就可以真正开始使用brat了。2.2 使用brat的使用也是特别简单，大家可以跟着我的流程一步一步走： 在brat的主目录下有一个data目录，其中是存放的我们需要打标的数据。 在data目录下cd data新建一个属于我们自己的数据集目录：mkdir mydata 在mydata目录下新建一个需要打标的txt文件并随便写点啥：vim 1.txt 新建专属于自己的标注类别conf文件：vim annotation.conf[entities]personact like hate[relations]Act-Person Arg1:&lt;ENTITY&gt;, Arg2:&lt;ENTITY&gt;[events][attributes]这里我只需要对实体和关系进行标注，比如我需要标注实体：人物，动作（包含喜欢/讨厌）；关系：人物和动作的关系当我们在网页server端打开/mydata/1.txt后，可以如下图开始标注了 定制化实体/关系颜色及中文展示，在mydata目录下新建一个conf文件:vim visual.conf[labels]person | 人名like | 喜欢hate | 讨厌[drawing]SPAN_DEFAULT fgColor:black, bgColor:lightgreen, borderColor:darkenARC_DEFAULT color:black, arrowHead:triangle-5person bgColor:yellow, borderColor:redlike bgColor:green, borderColor:redhate bgColor:gray, borderColor:red接下来，就会在标注的页面看到如下所示了。三. brat结果输出及扩展（干货）3.1 实体BIO输出当我们标注完成之后，需要将实体的标注结果转成BIO进行输出，这该怎么输出呢？ 其实可以直接使用brat的python脚本直接将标注的.ann文件输出成BIO形式的.conll文件 在brat的主目录下进入tools目录，利用python输出结果：python anntoconll.py ../data/mydata/1.txt 这里就可以在mydata目录下看到1.conll文件了，如下所示B-person 0 1 姚I-person 1 2 明B-like 2 3 喜I-like 3 4 欢O 4 5 打O 5 6 篮O 6 7 球B-person 8 9 小I-person 9 10 明B-like 10 11 喜I-like 11 12 欢B-person 12 13 姚I-person 13 14 明3.2 实体BIO输出脚本错误检查当我们利用brat自带的python脚本anntoconll.py脚本生成来conll文件后，其实会有bug存在（亲测），啥bug呢？比如“小明小李”这两个实体连在了一起，你的ann命名显示着“小明”和“小李”分别是一个PERSON，但是经过anntoconll.py脚本生成后的conll文件后，你会发现只有一个实体“小明小李”，因此我们还需要一个检查脚本来修改这个bug。import osimport sysdef read_file(file_path): with open(file_path, 'r',encoding='utf8') as file: lines = file.readlines() return linesdef write_file(file_path, lines): with open(file_path, 'w',encoding='utf8') as file: for line in lines: file.write(line)def check_one_file(conll_path, ann_path): # check process: 针对conll文件进行检查B开头的字符是否通过brat转化后是正确的 ann_lines = read_file(ann_path) conll_lines = read_file(conll_path) begin_words, begin_index = [], [] for ann_i in ann_lines: class_i, start_i, end_i = ann_i.split('\\t')[1].split(' ') begin_words.append('B-' + class_i) begin_index.append(start_i) ann_zip = dict(zip(begin_index, begin_words)) # check new_conll = [] for conll_i in conll_lines: if conll_i == '\\n': new_conll.append(conll_i) else: tag_i, start_i, end_i, word_i = conll_i.split('\\t') if start_i in begin_index: if tag_i != ann_zip[start_i]: new_conll.append(ann_zip[start_i] + '\\t' + start_i + '\\t' + end_i + '\\t' + word_i) else: new_conll.append(conll_i) else: new_conll.append(conll_i) return new_conllif __name__ == '__main__': sub_dir = sys.argv[1] # sub_dir = '../data/fd' # 生成一个保存新conll文件的文件夹 new_dir = os.path.join(sub_dir,'conll') if not os.path.exists(new_dir): os.mkdir(new_dir) path_pre = [] for path_i in os.listdir(sub_dir): if path_i.endswith('.ann'): path_pre.append(path_i.split('.ann')[0]) for path_pre_i in path_pre: ann_path_i = os.path.join(sub_dir,f'{path_pre_i}.ann') conll_path_i = os.path.join(sub_dir,f'{path_pre_i}.conll') new_conll_i = check_one_file(conll_path_i,ann_path_i) new_path_i = os.path.join(new_dir,f'{path_pre_i}.conll') write_file(new_path_i,new_conll_i) print(f'Write {conll_path_i} completed!')3.3 实体BIOE输出当我们标注完成之后，需要将实体的标注结果转成BIOE格式进行输出，这该怎么输出呢？def bioe_transfer(tags:List[List[str]]): \"\"\" 将BIO格式转成BIOE格式 \"\"\" new_tags_all = [] for tags_i in tags: new_tags = [] for i in range(len(tags_i) - 1): tag_i = tags_i[i] tag_after = tags_i[i + 1] if tag_i == 'O': new_tags.append(tag_i) else: if tag_i.split('-')[0] == 'B': new_tags.append(tag_i) else: if tag_i == tag_after: new_tags.append(tag_i) else: # 最后一个 new_tags.append('E-' + tag_i.split('-')[1]) if tags_i[-1] == tags_i[-2]: if tags_i[-1] == 'O': new_tags.append('O') else: new_tags.append('E-' + tags_i[-1].split('-')[1]) else: new_tags.append(tags_i[-1]) new_tags_all.append(new_tags) return new_tags_all3.4 关系结果输出（干货）关系结果输出我实在是找不到啥脚本了，因此干脆自己写吧，下面是我自己写的脚本（随便写的，勿喷！）import pandas as pdimport numpy as npimport sysif __name__ == '__main__': path_txt = sys.argv[1] line_start = [] line_end = [] line_len = [] with open(path_txt,'r',encoding='utf8') as txt_f: txt_lines = txt_f.readlines() num = 0 for txt_i in txt_lines: txt_i = txt_i.split('\\n')[0] line_len.append(len(txt_i)) if num == 0: start_i = num else: start_i = num + 1 line_start.append(start_i) num = start_i + len(txt_i) line_end.append(num) rel_label = [] rel_index_1 = [] rel_index_2 = [] ent_index = [] ent_label = [] ent_start = [] ent_end = [] ent_words = [] with open(path_txt.split('.txt')[0] + '.ann','r',encoding='utf8') as ann_f: ann_lines = ann_f.readlines() for ann_i in ann_lines: e_r_i = ann_i.split('\\t')[0] if 'R' in e_r_i: rel_i = ann_i.split('\\t')[1].split(' ') rel_label.append(rel_i[0]) rel_index_1.append(rel_i[1].split(':')[1]) rel_index_2.append(rel_i[2].split(':')[1]) else: rs_ent = ann_i.split('\\t') ent_index.append(rs_ent[0]) ent_words.append(rs_ent[2].strip()) rs_ent_in = rs_ent[1].split(' ') ent_label.append(rs_ent_in[0]) ent_start.append(int(rs_ent_in[1])) ent_end.append(int(rs_ent_in[2]) - 1) df_ent = pd.DataFrame({'index':ent_index,'words':ent_words,'label':ent_label,'start':ent_start,'end':ent_end}) df_rel = pd.DataFrame({'label':rel_label,'ent_1':rel_index_1,'ent_2':rel_index_2}) df_line = pd.DataFrame({'txt':txt_lines,'start':line_start,'end':line_end}) ent_1_label = [] ent_1_words = [] ent_1_start = [] ent_1_end = [] ent_2_label = [] ent_2_words = [] ent_2_start = [] ent_2_end = [] txt_rep = [] txt_rep_start = [] for i in range(len(df_rel)): ent_1_i = df_rel.loc[i,'ent_1'] ent_2_i = df_rel.loc[i,'ent_2'] ent_1_label.append(np.array(df_ent.loc[df_ent['index'] == ent_1_i,'label'])[0]) ent_1_words.append(np.array(df_ent.loc[df_ent['index'] == ent_1_i,'words'])[0]) ent_1_start.append(np.array(df_ent.loc[df_ent['index'] == ent_1_i,'start'])[0]) ent_1_end.append(np.array(df_ent.loc[df_ent['index'] == ent_1_i,'end'])[0]) ent_2_label.append(np.array(df_ent.loc[df_ent['index'] == ent_2_i, 'label'])[0]) ent_2_words.append(np.array(df_ent.loc[df_ent['index'] == ent_2_i, 'words'])[0]) ent_2_start.append(np.array(df_ent.loc[df_ent['index'] == ent_2_i, 'start'])[0]) ent_2_end.append(np.array(df_ent.loc[df_ent['index'] == ent_2_i, 'end'])[0]) # 匹配文档 ent_1_index = np.array(df_ent.loc[df_ent['index'] == ent_1_i, 'start'])[0] df_line_i = df_line[(df_line['start'] &lt;= ent_1_index) &amp; (df_line['end'] &gt;= ent_1_index)] txt_rep_i = np.array(df_line_i['txt'])[0] txt_rep_start_i = np.array(df_line_i['start'])[0] txt_rep.append(txt_rep_i) txt_rep_start.append(txt_rep_start_i) df_rel['ent_1_label'] = ent_1_label df_rel['ent_1_words'] = ent_1_words df_rel['ent_1_start'] = ent_1_start df_rel['ent_1_end'] = ent_1_end df_rel['ent_2_label'] = ent_2_label df_rel['ent_2_words'] = ent_2_words df_rel['ent_2_start'] = ent_2_start df_rel['ent_2_end'] = ent_2_end df_rel['txt'] = txt_rep df_rel['txt_start'] = txt_rep_start df_rel.drop(['ent_1','ent_2'],axis=1,inplace=True) csv_path = path_txt.split('.txt')[0] + '.csv' df_rel.to_csv(csv_path, index=False) print(f'Saved in {csv_path}!')3.5 模型预测结果展示为brat（干货）由于自己觉着brat蛮好看的，用来给模型的结果展示也不错，决定写个代码转成brat的.ann形式，这里不太方便代码展示了，这里就把整体思路说下： 将NER模型输出的words，tags转成结果的词语（比如上面的“姚明”） 并将这些词的开始的字和结尾的字所在位置索引记录下来 如果涉及换行，需要将索引 += 改行句子的长度这里用mydata/1.ann来举例，如下图所示T1 person 0 2 姚明T2 person 8 10 小明T3 person 12 14 姚明T4 like 10 12 喜欢T5 like 2 4 喜欢R1 Act-Person Arg1:T1 Arg2:T5 R2 Act-Person Arg1:T2 Arg2:T4 规律： 想要转成.ann格式，我们可以看到每一行都是固定的格式，即：T总序列号\\t类别名 该词开始所在索引 该词结束所在索引\\t该词 如果换行了，比如第二行的“小明”，这里的开始索引 = 第一句长度 + “小”字所在第二句的位置"
    } ,
  
    {
      "title"       : "NLP数据增强",
      "category"    : "",
      "tags"        : "深度学习, NLP",
      "url"         : "./NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html",
      "date"        : "2022-08-17 08:00:00 +0800",
      "description" : "记录文本领域数据增强的方法",
      "content"     : "一. 数据增强的意义无论是文本还是图像领域，想要的到一个泛化性强，鲁棒性高的深度学习模型，往往数据才是至关重要的，毕竟好的数据决定了模型的上线。我们建模的时候往往会遇到很多数据问题，比如： 数据太少了，训练的模型在其他数据集表现很差。 数据分布上呈现拖尾现象（部分类别的数据量很少，样本不均衡问题），训练的模型在部份类别（数据量少）的识别上效果很差。 数据标签混乱（人工标记存在很大的问题），训练的模型在本身的验证集/测试集上就很差。针对第三点，我们还可以人工矫正数据。但是对于第1，2点情况，往往需要更多的数据。因此这里才需要进行数据增强。二. NLP分类任务的数据增强2.1 EDA文本数据增强文本分类任务其实是NLP中最常见也是最基础的任务。这里推荐最基础也是醉简单的数据增强——EDA（Easy Data Augmentation）。其常见的几种数据增强的方式为： 同义词替换：在同义词词典中随机替换文本中的N个词（不包含停用词） 随机插入：随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置，重复N次（不包含停用词） 随机交换：随机在文本中选择2个词，进行位置交换，重复N次。 随机删除：随机删除文本中的N个词汇。2.2 EDA的使用（干货） 这里推荐一个开源的github仓库，很方便我们进行简单的EDA文本数据增强（中文）：https://github.com/zhanlaoban/EDA_NLP_for_Chinese将该仓库clone到本地后，可以直接使用以下命令进行数据增强：python3 code/augment.py --input=content.txt --output=content_aug.txt --num_aug=3 --alpha=0.05EDA输入我们需要使用以上命令的时候，这里需要将自己的数据变换成EDA输入(即上面命令的content.txt)的形式，我这边写了一个df转成输入的txt的python函数import pandas as pddef transfer_input_data(df_small,content = 'content',label = 'result',output = 'tag.txt'): \"\"\"将输入数据转成eda输入的形式label+\\t+sentence\"\"\" df_small.reset_index(inplace = True) with open(output,'w',encoding='utf8') as file: for i in range(len(df_small)): label_i = df_small.loc[i,label] doc_i = df_small.loc[i,content] line = str(label_i) + '\\t' + doc_i + '\\n' file.write(line) print('write completed!')EDA输出我这边将EDA输出的txt(即上面命令的content_aug.txt)转成了df，代码如下：def transfer_argumentedTxt2csv(argument_txt,content = 'content'): # 将eda生成的增强后的数据转成df label = [] sentence = [] with open(argument_txt, 'r', encoding='utf8') as file: lines = file.readlines() for line in lines: label_i, sent_i = line.split('\\t') label.append(label_i) sent_i = sent_i.replace(' ', '') sent_i = sent_i.replace('\\n', '') sentence.append(sent_i) df = pd.DataFrame({content:sentence,'result':label}) return df三. NER任务的数据增强相较于文本分类任务对于句子进行分类，NER任务是对于句子中每个字/词进行分类。这时候如果还利用EDA简单的数据增强方式增加数据，这些数据极有可能成为噪声导致NER模型脆弱敏感，反而会使得模型效果变差。3.1 DAGA数据增强 这里推荐一篇来自南洋理工及阿里达摩院的针对于NER模型的数据增强论文——DAGA其github开源仓库地址：https://github.com/ntunlp/daga数据处理整片文章的亮点其实是在数据处理的过程，作者很有意思的将BIOSE标签挪到每个字的前面，当然这里对O标签对应的字不做处理。如下图所示这样就能将字和标签一同训练，从而很好的拿到标签和字前后呼应的高概率标签和字。模型文中采用的是简单的语言模型，如下图所示，将待预测文本输入到词嵌入层后，经过简单的1层LSTM后，后面经过分类输出层输出预测的文本。3.2 DAGA的使用（干货）使用的具体步骤： clone仓库的代码到本地 新建数据存储的目录（输入和输出的dir）: mkdir data &amp; mkdir data/input &amp; mkdir data/output 新建模型存放的目录：mkdir model 新建模型训练的shell脚本：touch daga_train.sh &amp; chmod +x daga_train.sh 新建数据生成的shell脚本：touch daga_generate.sh &amp; chmod +x daga_generate.sh 使用daga_train.sh脚本进行训练： ./daga_train.sh data/input model 使用daga_generate.sh脚本进行数据生成：./daga_generate.sh data/output model 5000 这里5000指的是数据生成的个数daga_train.sh脚本#!/bin/bashinput_dir_path=$1model_dir_path=$2# 1. 标注数据线性化cd toolspython3 preprocess.py --train_file ../$input_dir_path/train.txt \\--test_file ../$input_dir_path/test.txt --dev_file ../$input_dir_path/val.txt \\--vocab_size 30000 # 2. 训练语言模型cd ../lstm-lmpython3 train.py --train_file ../tools/train.lin.txt \\--valid_file ../tools/val.lin.txt --model_file ../$model_dir_path/model.pt \\--emb_dim 300 --rnn_size 512 --gpuid 0 --epochs 300daga_generate.sh脚本#!/bin/bashoutput_dir_path=$1model_dir_path=$2gen_sentence_num=$3# 1. 数据生成cd lstm-lmpython3 generate.py --model_file ../$model_dir_path/model.pt \\--out_file ../$output_dir_path/out.txt --num_sentences $gen_sentence_num \\--temperature 1.0 --seed 3435 --max_sent_length 128 --gpuid 0# 2. 数据还原cd ../toolspython3 line2cols.py --inp_file ../$output_dir_path/out.txt \\--out_file ../$output_dir_path/augment.txt遇到的问题 torchtext的版本问题，导致报错module 'torchtext.data' has no attribute XXX。这里需要将本地的daga代码中所有的torchtext.data改成torchtext.legacy.data即可。 模型训练的问题，这里由于默认的是仅有30轮的训练，往往还处理模型欠拟合的时候就训练结束了，导致生成的数据很不理想，因此我们需要加入--epochs，这里我已经在daga_train.sh脚本中改成了300轮，这里不用担心训练轮数过大导致模型过拟合，因为在训练代码中已经加入了early stop会自动停止训练的。"
    } ,
  
    {
      "title"       : "python装饰器及魔法函数",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./python%E8%A3%85%E9%A5%B0%E5%99%A8%E5%8F%8A%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0.html",
      "date"        : "2022-07-20 08:00:00 +0800",
      "description" : "记录python使用中的装饰器及魔法函数",
      "content"     : "一. 装饰器 python的装饰器其实就是在某个函数外面再嵌套一个函数，用以达到增强该函数功能的效果。其实就是套娃。特点： 装饰器输入参数：函数 装饰器输出：函数 装饰器能使代码使用上更加简洁比如下面的例子：def wrapper(f): def a(*args, **kwargs): print(*args, **kwargs) f(*args, **kwargs) return a# 1. 未使用装饰器情况下def call(name): print(f'name:{name}')wrapper(call('张三'))# 2. 使用装饰器@wrapperdef call(name): print(f'name:{name}')call('张三')1.1 classmethod装饰器一般来说，使用一个类的方法，必须先实例化该类，再调用方法。但是@staticmethod则可以直接使用类名.方法()。特点： 方法内部参数不能有self。 类的其他方法调用本方法需要加入self.方法() 在方法内部调用类其他属性为 类名.属性，调用方法则需要先实例化。class A: c = 'c' @staticmethod def a(a = 1): print(a) print(A.c) A().cc() # 需要实例化 def b(self): self.a() # 正常调用 def cc(self): print(self.c)A.a() # 不用实例，即可外部调用1.2 classmethod装饰器和staticmethod一样，可以不需要实例，直接直接使用类名.方法()。特点： classmethod没有类实例化对象的self参数，而是指代类本身的cls参数。 类的其他方法调用本方法需要加入self.方法() 在方法内部调用类其他属性为 cls.属性，调用方法则需要先实例化 cls.方法()。class A: c = 'c' @classmethod def a(cls,a = 1): print(a) print(cls.c) cls().cc() # 需要实例化 def b(self): self.a() # 正常调用 def cc(self): print(self.c)A.a()1.3 property 装饰器作用： 修饰方法，将方法当作类的只读属性一样访问，能防止属性被修改。class A: aaa = 1 @property def a(self): return self.aaa def b(self): return self.aaaa = A()print(a.a) print(a.b())二. 魔法函数2.1 __new__函数场景：一般需要使用单例模式下（比如数据库连接等），需要确保只有单个对象被创建情况下，都需要用到new函数class A: _instance = None def __new__(cls, *args, **kwargs): print('__new__') # 单例模式下，先判断是否存在实例，没有则创建 if cls._instance is None: cls._instance = object.__new__(cls) return cls._instance def __init__(self): print('__init__')# 仅允许一个实例被创建，就算创建其他实例，本质指向的还是同一个实例a = A()b = A() # a ==&gt; b# 打印结果：先是 __new__ 后是__init__。说明new是在实例创建之前，而init则是在实例创建之后2.2 __call__函数场景：将一个类当作一个函数进行使用，在流水线式的数据处理时可以更好的进行封装。class A: def __call__(self, *args, **kwargs): return self.compute(*args, **kwargs) def a1(self,a,b): return a + b def a2(self,a,b): return a * b def compute(self,a,b,c): return self.a1(a,c) + self.a2(a,b)a = A()rs = a(1,2,3) # 首先需要实例化，之后可以直接像函数一样使用2.3 属性操作__add__场景： 需要对2个实例进行运算操作时class A: def __init__(self, a) : self.a = a def __add__(self, other_a): return self.a + other_a.aa1 = A(1)a2 = A(2)aa = a1 + a2"
    } ,
  
    {
      "title"       : "Git-LFS使用",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./Git-LFS%E4%BD%BF%E7%94%A8.html",
      "date"        : "2022-03-09 08:00:00 +0800",
      "description" : "记录对于对git-lfs的使用记录",
      "content"     : "一. git-lfs的优势 对于搞深度学习或者机器学习的人普遍会遇到一个问题：那就是模型太大，以至于git仓库容纳不了。所以，这里给大家推荐一个git的大文件存储工具——GitLFS（git Large File Storage）。 官网：https://git-lfs.github.com/ mac利用brew来下载：brew install git-lfs linux利用apt-get来下载：apt-get install git-lfs Windows利用scoop来下载： scoop install git-lfs二. git-lfs在github中设置我们如果想在github中使用lfs的话，需要付费的！找到settings ==&gt; Billing and plans ==&gt; Git LFS Data，如下图所示当我们开通之后就可以使用lfs咯！三. git-lfs的使用 当我们下载好git-lfs之后，需要开启/初始化lsf功能：git lfs install，之后我们看到Git LFS initialized.说明已经初始化完成了！ 这里推荐2种方式将大型文件添加到lfs管理： 文件形式：git lfs track *.pkl 文件夹形式：git lsf track model/**（包含文件夹本身的）；git lsf track model/*（不包含文件夹本身的） 接下来我们就可以看到在git本地仓库中git给我们构建了一个文件.gitattributes 查看lfs追踪了哪些文件：git lfs ls-files 下面就是把新的文件添加到缓存区：git add .gitattributes 提交缓存区内的文件到本地仓库：git commit -m \"add .gitattributes\" 将本地的大型模型通过git推送到gitlfs中管理：git push origin master"
    } ,
  
    {
      "title"       : "贝叶斯公式及模型的理解",
      "category"    : "",
      "tags"        : "机器学习",
      "url"         : "./%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%A7%A3.html",
      "date"        : "2022-02-10 16:10:00 +0800",
      "description" : "记录对于贝叶斯公式及模型的理解",
      "content"     : "一. 贝叶斯公式1.1 实际的例子推导公式这里我们用比较实际的例子来理解贝叶斯公式。现在我们有2个骰子，每个骰子有6个面。 样本空间：$6\\times6 = 36$ 事件A：其中一个骰子展示的是2 事件B：两个骰子的总和是7如下图所示，将所有的样本空间展示出来，并圈出事件A、B两种情况的所有可能性。那么，针对每个不同的事件所发生的概率如下： 事件A发生的概率： 事件B发生的概率： 事件A、B同时发生的概率： 在A发生的条件下，B发生的概率： 在B发生的条件下，A发生的概率： 因此我们可以发现： $A和B同时发生的概率 = 在B发生条件下A发生的概率 \\times B发生的概率$，如下 $A和B同时发生的概率 = 在A发生条件下B发生的概率 \\times A发生的概率$，如下 由上面2个式子，可得$在B发生条件下A发生的概率 \\times B发生的概率 = 在A发生条件下B发生的概率 \\times A发生的概率$，即得到了贝叶斯公式，如下。 \\[P(A|B)\\times P(B) = P(B|A)\\times P(A)\\]1.2 用图来理解贝叶斯公式接下来我们利用图的更好的理解贝叶斯公式。 白色的矩形：表示整个样本空间 红色的圆：表示A事件 绿色的长方形：表示B事件 圆和长方形交集：表示A事件和B事件同时发生时的事件。那么交集事件的概率就如下所示： $A和B同时发生的概率 = 在B发生条件下A发生的概率 \\times B发生的概率$ $A和B同时发生的概率 = 在A发生条件下B发生的概率 \\times A发生的概率$ 二. 朴素贝叶斯模型2.1 真实案例——泰坦尼克生存者预测我们这里选择kaggle竞赛的一个Titanic生存率预测来说明下朴素贝叶斯是如何构建概率模型的。这里简单的构建了一个小型的数据集如下： survival($Y$) age &gt; 18($x_1$) pclass = 1($x_2$) sex = male($x_3$) 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 那么对于一条新的样本：age = 20 , pclass = 2 , sex = female的乘客而言，她能生存下来么？2.2 朴素贝叶斯模型推导对于上述的新样本的预测问题，我们转化成概率问题，即\\(P(Y|x_1,x_2,x_3) = ?\\) 这里利用贝叶斯公式转化，即：\\[P(Y|x_1,x_2,x_3) = \\frac{P(x_1,x_2,x_3|Y) \\times P(Y)}{P(x_1,x_2,x_3)}\\] 由于$x_1,x_2,x_3$出现的概率为1，因此，上式简化为：\\(P(Y|x_1,x_2,x_3) = P(x_1,x_2,x_3|Y) \\times P(Y)\\) 这里引入朴素的思想：假定每个特征条件（$x_1,x_2,x_3$）之间相互独立。那么: \\[P(Y|x_1,x_2,x_3) = P(x_1|Y) \\times P(x_2|Y) \\times P(x_3|Y) \\times P(Y)\\] 通过计算得到： $P(x_1 = 1 Y = 1) = \\frac{4}{4} = 1$ $P(x_2 = 0 Y = 1) = \\frac{2}{4} = 0.5$ $P(x_3 = 0 Y = 1) = \\frac{2}{4} = 0.5$ $P(Y = 1) = \\frac{4}{8} = 0.5$ 因此，对于age = 20 , pclass = 2 , sex = female的乘客而言，她能生存的概率为：\\(P(Y = 1|x_1 = 1,x_2 = 0,x_3 = 0) = 1 \\times 0.5 \\times 0.5\\times 0.5 = 0.125\\)"
    } ,
  
    {
      "title"       : "Git命令学习",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./Git%E5%91%BD%E4%BB%A4.html",
      "date"        : "2022-01-21 05:11:00 +0800",
      "description" : "记录学习git中遇到的一些常用命令",
      "content"     : "港真，大厂里不会git寸步难行啊，因此还是踏踏实实好好学习一哈吧。一. 配置git及常用命令1.1 安装和配置 安装git： * Windows下安装：推荐使用scoop进行安装scoop install git * Mac下安装：推荐使用brew进行安装brew install git git相关配置 * 配置用户名：git config --global user.name \"XX\" * 配置Email：git config --global user.email \"XX@xx.com\"在安装完成后，我们可以打开~/.gitconfig文件进行查看相关配置1.2 git的常用命令这里推荐一些git相关常用的命令（简单的）： 初始化仓库：git init 查看git缓存区状态：git status 提交所有的修改到缓存区：git add -A 提交指定文件到缓存区：git add file_name 清除缓存区内的所有文件：git rm --cached files 提交缓存区内的文件到本地仓库：git commit -m \"fix\" 将本地仓库和远程仓库进行连接：git remote add origin http:....git 将本地仓库推送到远程仓库：git push -u origin master ,这里讲下orgin其实指的是远程仓库，而master指的是本地分支名。 从远程仓库拉取代码到本地仓库(并合并)：git pull二. 分支和合并2.1 查看日志查看git日志的命令：git log我们可以参考漂亮日志，可以使用图形化一行来展示日志：git log --all --decorate --oneline --graph这里的命令过长，因此我们可以修改.gitconfig文件中的配置，添加如下所示[alias] dog = log --all --decorate --oneline --graph即可使用git dog来查看日志了。2.2 分支和合并命令 分支的意义：将修改记录的整体流程做一个分叉的保存，每个分支都不会受其他分支的影响，且每个分支都可以合并到主分支上。 本地创建一个分支：git branch branch_1，注意这里是在当前分支下新开的小分支，因此新开的分支下有前一个分支的所有内容。注意：我们在本地仓库初始化后想要添加一个分支，注意这里必须要保证master分支下是有文件的，不然会报错fatal: 不是一个有效的对象名：'master'。 查看当前分支：git brach，我们可以看到如下图所示，目前我们创建了一个新分支，但是还在master分支下，并没有切换到branch_1分支下（* 即代表当前所处的分支） 切换到指定分支：git checkout branch_1，这里如果是想直接新建一个分支并切换到该分支下，即可用命令git checkout -b branch_2 删除本地分支：git branch -d branch_1 合并分支branch_1：git merge branch_12.3 应用的小栗子删除分支栗子当我们在分支branch_2 下新建了一个b.txt文件后commit提交了，然后切换会master分支下时，想删除分支branch_2，却发现报错：error: 分支 branch_2没有完全合并。如果您确认要删除它，执行git branch -D branch_2。。这是因为branch_2里面已经做了一些变动，且没有将branch_2的内容合并到master，因此报错。这里你可以： 强制删除：git branch -D branch_2 合并在branch_2中修改的内容到master：git merge branch_2提交分支到远程的栗子我们创建了branch_2和branch_1分支，然后切换到master分支下后git push，那么远程仓库中并不会生成这两个分支，那么如何将两个分支推到远程仓库呢。你可以： 切换到branch_2分支下：git checkout branch_2 向远程仓库推送当前分支：git push origin branch_2 ，这里同时会在远端创建branch_2分支。 如何在远程仓库中删除刚创建的branch_2分支呢：git push origin :branch_2即可。 如何在远程仓库不叫branch_2这个分支名：git push origin branch_2:br2即可。查看git日志的栗子当我们在分支branch_2 下新建了一个b.txt文件后commit提交了之后，通过git merge branch_2来合并到master上，来查看下日志：git dog，如下图所示我们会发现这次提交是master还是branch_2的操作，因此我们需要git帮我们创建一次合并的commit，帮助我们区分啥时候发生的合并：git merge branch_2 --no-ff，这时候看日志就是如下图所示在上图我们可以看到git日志中先是分支branch_2（可以看到发生了分叉）添加了内容，然后合并到了master上，十分清晰明了。分支冲突的栗子我们的branch_2是基于branch_1进行创建的，然后我们在branch_2中修改了a.txt内容，并commit提交。然后在切换回branch_1，同时也修改了a.txt内容，依然commit提交。这个时候branch_2的同学想要基于branch_1的同学来修改代码继续做，因此branch_2会合并branch_1，即git merge branch_1，这时候就会发生分支冲突。那么解决冲突的方式就是去修改a.txt中的内容。这里推荐使用命令git mergetool来进行修改，如下图所示。三. 回滚我们提交之后发现想回滚到之前的提交下，如下图所示 想要回滚一次：git reset master^ 回滚4次：git reset master～4 回滚到指定的版本处：git reset --hard 01ff736 （添加–hard会将工作目录和暂存区的文件都丢弃掉，而–soft 则是仅丢弃暂存区的内容，而工作目录下的文件依旧保存） 回到后面的版本： git reset --hard 6556432，这时又能回到之前的版本，所以这里的reset跟类似于跳转某个版本直接commit提交之后需要撤销掉 撤销git commit，撤销git add，保留编辑器改动代码：git reset --mixed &lt;commit ID&gt;或git reset &lt;commit ID&gt; 撤销git commit，不撤销git add，保留编辑器改动代码：git reset --soft&lt;commit ID&gt; 撤销git commit，撤销git add，删除编辑器改动代码：git reset --hard &lt;commit ID&gt;四. gitignore我们经常发现，在使用git上传到远程仓库的时候，总是会把一些IDE或者一些自己不想放到远程的文件/文件夹放到远程，这时候就需要创建gitignore文件指定某些文件/文件夹不上传远端，即touch .gitignore。*.txt ==&gt; 忽略所有的txt文件model/ ==&gt; 忽略model文件夹!*.py ==&gt; 不要忽略所有的py文件注意：需要记得把.gitignore文件commit！不然无法生效。"
    } ,
  
    {
      "title"       : "picgo下配置github图床",
      "category"    : "",
      "tags"        : "博客",
      "url"         : "./picgo%E4%B8%8B%E9%85%8D%E7%BD%AEgithub%E5%9B%BE%E5%BA%8A.html",
      "date"        : "2022-01-20 19:11:00 +0800",
      "description" : "记录在博客中上传图片的好用工具和github图床相关配置",
      "content"     : "一. 必须的安装 picgo：强大的能快速创建图片url的工具（支持多种图床），简直不要太好用。我们可以直接在官网下载相应版本的picgo，我这里选用的是windows版本的，当然也有mac版本的。 git：这里推荐用scoop进行安装scoop install git二. 搭建属于自己的github图床2.1 新建一个共有仓库首先，要搭建一个github图床，我们需要创建一个共有仓库（注意：如果是创建私有仓库根本无法显示图片出来）来存储上传的图片。2.2 创建admin分支这里如果不会用git创建分支的同学可参考我上一篇文章GIT命令学习。这里注意最好选择帮你添加一个README.md文件。 克隆刚创建的远程仓库：git clone XX.git 创建并切换到admin分支：git checkout -b admin 向远端仓库推送admin分支：git push orgin admin这时，我们就可以在仓库中看到admin分支了，如下图。2.3 设置token这里我们还需要设置token，直接点击该链接即可。注意，这里需要将repo选择。如下图所示。然后点击generate token就完成了。三. 在picgo下配置github图床记住上面刚刚配置好的token和仓库名字及分支名admin，按照下图进行配置即可。至此，我们就可以愉快的上传图片咯！"
    } ,
  
    {
      "title"       : "面试基础总结",
      "category"    : "",
      "tags"        : "面试",
      "url"         : "./%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93.html",
      "date"        : "2021-12-13 05:11:00 +0800",
      "description" : "记录算法面试中遇到的基础问题和过程记录",
      "content"     : "一. 操作系统1.1 并行和并发并发：在操作系统中，某一时间段，几个程序在同一个CPU上运行，但在任意一个时间点上，只有一个程序在CPU上运行。并行：当操作系统有多个CPU时，一个CPU处理A线程，另一个CPU处理B线程，两个线程互相不抢占CPU资源，可以同时进行，这种方式成为并行。知乎的例子： 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。1.2 进程和线程1.2.1 进程​ 进程是资源分配的基本单位，理解为一个程序。所以我们一般都要求进程池的进程数小于等于CPU的核心数。​ 如果问单核CPU能否运行多进程？答案又是肯定的。单核CPU也可以运行多进程，只不过不是同时的，而是极快地在进程间来回切换实现的多进程。进程拥有自己的地址空间，全局变量，文件描述符，各种硬件等等资源。1.2.2 线程​ 线程：线程是依赖于进程的。如果说进程和进程之间相当于程序与程序之间的关系，那么线程与线程之间就相当于程序内的任务和任务之间的关系。​ 一个程序内包含了多种任务。加上了线程之后，线程能够共享进程的大部分资源，并参与CPU的调度。意味着它能够在进程间进行切换，实现并发。1.2.3 为什么要用多进程，适用条件​ 总是在运行一个进程上的任务，就会出现一个现象。就是任务不一定总是在执行 ”计算型“ 的任务，会有很大可能是在执行网络调用，阻塞了，CPU 岂不就浪费了？因此，多进程适用于CPU密集型任务(各种循环处理、计算等等)。多线程适用于IO密集型任务(文件处理、网络爬虫等)。1.2.4 多进程通信管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。类似于python的multiprocessing.Pipe()消息队列MessageQueue：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。类似于python的multiprocessing.Queue()共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。1.2.5 python的多线程是真的多线程么​ Python在设计之初就考虑要在主循环中，同时只有一个线程在执行，就像单CPU的系统中运行多个进程那样，内存中可以存放多个程序，但任意时刻，只有一个程序在CPU中运行。同样地，虽然Python解释器可以运行多个线程，只有一个线程在解释器中运行。​ Python虚拟机的访问由全局解释器锁（GIL）来控制，正是这个锁能保证同时只有一个线程在运行。Python的多进程多线程测试： 在一个4核CPU上开4线程，发现电脑的CPU利用率没有占满，大致相当于单核水平。 在一个4核CPU上开4进程，发现CPU直接飙到了100%，说明进程是可以利用多核的！ Python多线程相当于单核多线程，多线程有两个好处：CPU并行，IO并行，单核多线程相当于自断一臂。所以，在Python中，可以使用多线程，但不要指望能有效利用多核。 1.2.6 多线程如何保证线程安全当多个线程同时操作同一个共享全局变量的时候，就容易出现线程安全问题，线程安全问题只会影响到线程对同一个共享的全局变量的写操作。利用线程锁来保证同一个时刻，有且仅有一个线程对共享的全局变量进行写操作。且开始写操作前，需要加锁，完成后，需要解锁，让其他线程再对其进行写操作。1.2.7 死锁问题​ 死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。死锁的4个必要条件： 互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放。 请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。 不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞如何避免死锁？只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件：一次性申请所有的资源。 破坏不剥夺条件：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。1.3 同步和异步1.3.1 阻塞和非阻塞 阻塞：指调用线程或者进程被操作系统挂起。 非阻塞：指调用线程或者进程不会被操作系统挂起。 1.3.2 同步和异步同步是阻塞模式，异步是非阻塞模式。 同步：指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去。 异步：指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回式系统会通知进程进行处理，这样可以提高执行的效率。1.4 协程协程是一种用户态的轻量级线程。子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。例子：def A(): print('1') print('2') print('3')def B(): print('x') print('y') print('z')假设由协程执行，在执行A的过程中，可以随时中断，去执行B，B也可能在执行过程中中断再去执行A，结果可能是：1 2 x y 3 z但是在A中是没有调用B的，看起来A、B的执行有点像多线程，但协程的特点在于是一个线程执行协程的优势： 极高的执行效率：因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 不需要多线程的锁机制：因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。1.4 内存分配管理1.4.1 分段和分页分段：在段式存储管理中，将程序的地址空间划分为若干段（segment），如代码段，数据段，堆栈段；这样每个进程有一个二维地址空间，相互独立，互不干扰。分页：在页式存储管理中，将程序的逻辑地址划分为固定大小的页（page），而物理内存划分为同样大小的帧，程序加载时，可以将任意一页放入内存中任意一个帧，这些帧不必连续，从而实现了离散分离。1.4.2 分段和分页不同点 段是信息的逻辑单位，它是根据用户的需要划分的，因此段对用户是可见的 ；页是信息的物理单位，是为了管理主存的方便而划分的，对用户是透明的。 段的大小不固定，有它所完成的功能决定；页大大小固定，由系统决定 段向用户提供二维地址空间；页向用户提供的是一维地址空间 段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制。 段式管理的优点是没有内碎片（因为段大小可变，改变段大小来消除内碎片）；页式存储管理的优点是没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）1.4.3 堆和栈对于一个由C/C++编译的程序，其所占用的内存可以划分为以下几个部分： 栈区（stack）—— 由操作系统自动分配和释放，主要用于存放函数参数值，局部变量等。其操作方式类似于数据结构中的栈。 堆区（heap）—— 一般由程序员动态分配和释放(malloc，free)，若程序员不主动释放，则程序结束后由操作系统回收。注意，它与数据结构中的堆是不同的，分配方式类似于链表。 BSS段——主要用于存放未初始化的静态变量和全局变量，可读写，它在程序结束后由操作系统进行释放。 数据段（data）——主要用于存放已初始化的静态变量和全局变量，可读写，它在程序结束后由操作系统释放。 代码段（text）——主要用于保存程序代码，包括CPU执行的机器指令，同时全局常量也是保存在代码段的，如字符串字面值。1.4.4 一个进程的内存结构/main.cppint a = 0; // 全局初始化区域char *p1; // 全局未初始化区域int main(){ int b; // 栈 char s[] = \"adoryn\"; // 栈 char *p2; // 栈 char *p3 = \"zhaobryant\"; // 字符串字面量存放在常量区，p3存放在栈上 static int c = 0; // 全局（静态）初始化区域 p1 = (char *)malloc(10); p2 = (char *)malloc(20); // 分配获得的10和20字节的内存区放在堆区 strcpy(p1, \"zhaobryant\"); // 字符串字面量存放在常量区，编译器可能会将它与p3所指向的\"zhaobryant\"优化为同一个地址 return 0;}二. linux相关2.1 查看CPU和内存占用情况 查看物理内存使用情况：free 查看目前正在运行的进程所占的内存百分比和CPU百分比：top。假如某个进程显示400%，说明该程序利用了4核CPU。 查看磁盘使用情况：df 查看当前进程：ps，ps -aux则是当前所有的正在内存当中的程序 2.2 查找匹配 在某个路径下查找是否有某个文件：find 路径 -name 文件名 根据字符串查找相应匹配的文件：grep。它是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 查找后缀有 py 字样的文件中包含 print 字符串的文件（当前路径下）：grep print *.py 递归查找符合条件的文件，这里是查找指定目录（git_repo）下及其子目录所有文件中包含helloworld字符串的文件 联合ps检查当前进程中python3进程是否运行：ps -aux | grep python3。其中|是管道命令 是指ps命令与grep同时执行。 2.3 网络相关 显示各种网络相关信息：netstat 列出所有处于监听状态的Sockets netstat -l # 只显示监听端口netstat -lt #只列出所有监听 tcp 端口netstat -lu #只列出所有监听 udp 端口netstat -lx #只列出所有监听 UNIX 端口 配合grep查找tcp下的指定端口号信息 netstat -plnt | grep :53# -p 输出中显示 PID 和进程名称# -l 仅列出有在 Listen (监听) 的服务状态# -n 不想让主机，端口和用户名显示，而是用ip显示# -t 只列出所有监听 tcp 端口 2.4 select、poll和epoll用户空间和内核空间操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间（供内核使用），一部分为用户空间（供进程使用）。进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。进程的切换需要经过以下变换： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。进程阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。文件描述符fd文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。缓存IO在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。Socketsocket是一种”打开—读/写—关闭”模式的实现，服务器和客户端各自维护一个”文件”，在建立连接打开后，可以向自己文件写入内容供对方读取或者读取对方内容，通讯结束时关闭文件。IO多路复用IO multiplexing就是我们说的select，poll，epoll。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。select调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024poll不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。epoll相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。epoll对文件描述符的操作有两种模式，LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：　　 LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。　　 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。epoll的优点主要是以下个方面： 监视的描述符数量不受限制 IO的效率不会随着监视fd的数量的增长而下降2.5 TCP三握四挥三次握手如图所示，双方之间的三个蓝色箭头就表示了三次握手过程中所发生的数据交换： 第一次握手：客户端向服务器发送报文段1，其中的 SYN 标志位 (前文已经介绍过各种标志位的作用)的值为 1，表示这是一个用于请求发起连接的报文段，其中的序号字段 (Sequence Number，图中简写为seq)被设置为初始序号x (Initial Sequence Number，ISN)，TCP 连接双方均可随机选择初始序号。发送完报文段1之后，客户端进入 SYN-SENT 状态，等待服务器的确认。 第二次握手：服务器在收到客户端的连接请求后，向客户端发送报文段2作为应答，其中 ACK 标志位设置为 1，表示对客户端做出应答，其确认序号字段 (Acknowledgment Number，图中简写为小写 ack) 生效，该字段值为 x + 1，也就是从客户端收到的报文段的序号加一，代表服务器期望下次收到客户端的数据的序号。此外，报文段2的 SYN 标志位也设置为1，代表这同时也是一个用于发起连接的报文段，序号 seq 设置为服务器初始序号y。发送完报文段2后，服务器进入 SYN-RECEIVED 状态。 第三次握手：客户端在收到报文段2后，向服务器发送报文段3，其 ACK 标志位为1，代表对服务器做出应答，确认序号字段 ack 为 y + 1，序号字段 seq 为 x + 1。此报文段发送完毕后，双方都进入 ESTABLISHED 状态，表示连接已建立。常见面试题 1： TCP 建立连接为什么要三次握手而不是两次？ 防止已过期的连接请求报文突然又传送到服务器，因而产生错误 三次握手才能让双方均确认自己和对方的发送和接收能力都正常 告知对方自己的初始序号值，并确认收到对方的初始序号值常见面试题2： TCP 建立连接为什么要三次握手而不是四次？ 相比上个问题而言，这个问题就简单多了。因为三次握手已经可以确认双方的发送接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值得确认，也就无需再第四次握手了。四次挥手建立一个连接需要三次握手，而终止一个连接要经过 4次握手。这由 TCP 的半关闭( half-close) 造成的。既然一个 TCP 连接是全双工 (即数据在两个方向上能同时传递)， 因此每个方向必须单独地进行关闭。四次挥手详细过程如下： 客户端发送关闭连接的报文段，FIN 标志位1，请求关闭连接，并停止发送数据。序号字段 seq = x (等于之前发送的所有数据的最后一个字节的序号加一)，然后客户端会进入 FIN-WAIT-1 状态，等待来自服务器的确认报文。 服务器收到 FIN 报文后，发回确认报文，ACK = 1， ack = x + 1，并带上自己的序号 seq = y，然后服务器就进入 CLOSE-WAIT 状态。服务器还会通知上层的应用程序对方已经释放连接，此时 TCP 处于半关闭状态，也就是说客户端已经没有数据要发送了，但是服务器还可以发送数据，客户端也还能够接收。 客户端收到服务器的 ACK 报文段后随即进入 FIN-WAIT-2 状态，此时还能收到来自服务器的数据，直到收到 FIN 报文段。 服务器发送完所有数据后，会向客户端发送 FIN 报文段，各字段值如图所示，随后服务器进入 LAST-ACK 状态，等待来自客户端的确认报文段。 客户端收到来自服务器的 FIN 报文段后，向服务器发送 ACK 报文，随后进入 TIME-WAIT 状态，等待 2MSL(2 * Maximum Segment Lifetime，两倍的报文段最大存活时间) ，这是任何报文段在被丢弃前能在网络中存在的最长时间，常用值有30秒、1分钟和2分钟。如无特殊情况，客户端会进入 CLOSED 状态。 服务器在接收到客户端的 ACK 报文后会随即进入 CLOSED 状态，由于没有等待时间，一般而言，服务器比客户端更早进入 CLOSED 状态。三. C++3.1 STL中list,vector,map,set区别vector vector封装了数组，拥有一段连续的内存空间，并且起始地址不变。 存取复杂度：能实现高效的随机存取，其时间复杂度为O(1)。 增删复杂度：由于内存空间是连续的，因此插入和删除时，会造成内存块的拷贝，时间复杂度为O(n)。 特别是当数组空间不足时，会重新申请一块内存空间（2倍）并进行拷贝。 vector支持根据下标随机存取元素。其原因是“循秩访问”这种向量特有的元素访问方式。list list 封装了链表，且是双向链表。因此内存空间是不连续的。 每一个结点都维护一个信息块、一个前向指针和一个后向指针，因此支持前向/后向遍历。 存取复杂度：只能通过指针访问数据，所以list的随机存取非常没有效率，时间复杂度为o(n)。原因是存储的对象是离散的，随机访问需要遍历整个链表 增删复杂度：在已经定位到要增删元素的位置的情况下，增删元素能在常数时间内完成，即O(1) 。可以不分配必须的内存大小方便的进行添加和删除操作。 当要存储的是大型负责类对象时，list要优于vector。 list 容器不支持根据下标随机存取元素。不支持[ ]操作符和vector.at()。 map map的本质其实就是映射，键值（key-value）一一对应。 map的实现是一颗红黑树，因此，map的内部键的数据都是排好序的，查找和删除、插入的效率都是n(logn)。 multimap 允许一个键对应多个值；而map则是一个键对应一个值。set 所得元素的只有key没有value，value就是key。不允许键值的重复，即在set中每个元素的值都唯一。 set的实现是一颗红黑树，因此，set的内部键的数据都是排好序的，查找和删除、插入的效率都是n(logn)。 set不能通过迭代器修改set元素值，其原因是set的值就是键。3.2 虚函数虚函数使用条件​ 当基类指针指向一个子类对象，通过这个指针调用子类和基类同名成员函数的时候，基类声明为虚函数「子类不写也可以」就会调子类的这个函数，不声明就会调用基类的。关键字：virtual构造和析构可以是虚函数么​ 构造不能为虚函数：如果构造函数是虚函数，那么就需要通过vtable（虚函数指针表） 来调用，但此时面对一块 raw memeory，到哪里去找 vtable 呢？毕竟，vtable 是在构造函数中才初始化的啊，而不是在其之前。因此构造函数不能为虚函数。​ 析构为虚函数：此时 vtable 已经初始化了；况且我们通常通过基类的指针来销毁对象，如果析构函数不为虚的话，就不能正确识别对象类型，从而不能正确销毁对象。虚函数可以是内联函数(inline)么 内联函数：由于函数调用耗时长，因此在编译时将所调用的函数的代码直接嵌入到主调函数中，可以理解为内联函数体代码直接替换了调用函数这行代码。 虚函数可以是内联函数，内联是可以修饰虚函数的，但是当虚函数表现多态性的时候不能内联。 内联是在编译器建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时（运行期）不可以内联。3.3 继承和多态C++有其三大特性：封装、继承和多态。继承​ 以让某个类型的对象，获得另一种类型对象属性的方法。实际上就是类与类之间可以共用代码，实现代码重用。可以理解为子类继承父类的方法。​ 例子：父类为Animal，其仅有2个成员函数，分别为bark()和eat()。子类为Dog，继承了父类的bark()和eat()，同时它还有自己的另一个方法run()。多态​ 多种形态，指一个类实例的相同方法在不同情况下有不同表现形式。​ 即子类可以重写父类的某个函数，从而为这个函数提供不同于父类的行为。一个父类的多个子类可以为同一个函数提供不同的实现，从而在父类这个公共的接口下，表现出多种行为。​ 例子：父类为Animal，其仅有2个成员函数，分别为bark()和eat()。子类为Dog，Cat，Snake，对于同一个方法bark()而言，不同的子类存在不同的喊叫方式，即为多态。区别 多态的实现要求必须是共有继承。 继承关系中，并不要求基类方法一定是虚函数。而多态时，要求基类方法必须是虚函数。 多态：子类重写父类的方法，使得子类具有不同的实现。且运行时，根据实际创建的对象动态决定使用哪个方法。四. 数据结构和算法4.1 排序算法排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。这里比如冒泡、插入、快排、归并、堆排序等。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 这里比如计数排序和位图排序。算法复杂度 排序方法 时间复杂度（平均） 时间复杂度（最坏） 时间复杂度（最好） 空间复杂度 稳定性 冒泡排序 稳定 插入排序 稳定 快速排序 不稳定 归并排序 稳定 堆排序 不稳定 计数排序 稳定 位图排序 稳定 冒泡排序 Bubble sort​ 它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。算法步骤： 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。插入排序 Insertion Sort​ 它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。算法步骤： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。快速排序 Quick Sort​ 通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。算法步骤： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。归并排序 Merge Sort​ 该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。算法步骤： 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。堆排序 Heap Sort​ 利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点，大顶堆和小顶堆。算法步骤： 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。计数排序 Counting Sort​ 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。算法步骤： 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。位图排序 Bitmap Sort​ 位图排序是一种效率极高(复杂度可达O(n))并且很节省空间的一种排序方法，但是这种排序方法对输入的数据是有比较严格的要求(数据不能重复，大致知道数据的范围)。 位图排序即利用位图或者位向量来表示集合。参考的面试题：40亿个QQ号码，如果使用O(1)的时间复杂度去查找一个QQ号是否存在。算法步骤： 根据待排序集合中最大的数，开辟一个位数组，用来表示待排序集合中的整数。 待排序集合中的数字在位数组中的对应位置置1，其他的置0。 将对应序号的整数放置到位对应的index上，并在每个对应的位上置位1. 检验每一位，如果该位位1，输出对应的整数。4.2 红黑树二叉查找树(BST)特性： 左子树上所有结点的值均小于或等于它的根结点的值。 右子树上所有结点的值均大于或等于它的根结点的值。 左、右子树也分别为二叉排序树。 根据二分查找的思想，查找的最大次数等同于树的高度。但是二叉查找树有其自身的缺陷：在多次出入新的节点后可能会出现不平衡现象，如下图。红黑树（RBT）红黑树是一种自平衡的二叉查找树。其除了具有二查找树的特性外，还具有以下特性： 节点是红色或黑色。 根节点是黑色。 每个叶子节点都是黑色的空节点（NIL节点）。 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。如下图插入一个14的新节点。红黑树实现自平衡的三种操作： 左旋：逆时针旋转红黑树的两个节点，使得父节点被自己的右孩子取代，而自己成为自己的左孩子。 右旋：顺时针旋转红黑树的两个节点，使得父节点被自己的左孩子取代，而自己成为自己的右孩子。 变色 4.3 回溯算法递归实现-伪代码算法 ReBack(k)if k&gt;n: then &lt;x1,x2...xn&gt;是解else: while S_k != 空集 do 找到S_k中的最小值x_k S_k = S_k - {x_k} 计算S_k+1 ReBack(k+1)迭代实现-伪代码迭代算法 Backtrack输入：n输出：所有的解1. 对于i=1,2,...n 确定x_i # 确定初始值2. k = 13. 计算S_k4. while S_k != 空集 do # 满足约束分支搜索 找到S_k中的最小值x_k S_k = S_k - {x_k} if k &lt; n then k = k+1 计算S_k else &lt;x1,x2...xn&gt;是解5. if k&gt;1 then k = k-1 ; goto4 # 回溯四后问题 问题描述：在4 * 4 的方格棋盘中放置4个皇后，使得没有2个皇后在同一行、同一列、也不在45°的斜线上。问有多少种可能的布局？解是4维向量&lt;x_1,x_2,x_3,x_4&gt;，解为：&lt;2,4,1,3&gt;，&lt;3,1,4,2&gt;其搜索空间是一个4叉树，如下图。 每个结点有4个儿子，分别代表1，2，3，4列位置。 第 i 层选择解向量中第 i 个分量的值 最深层的树叶是解。 按深度优先次序便利树，找到所有的解。0-1背包问题 问题描述：有n种物品，每种物品只有1个。第 i 种物品价值为v_i，重量位w_i，i = 1,2,…,n。问如何选择放入背包的物品，使得总重量不超过B，而价值达到最大？解：n维 0-1 向量&lt;x_1,x_2,…x_n&gt;节点：&lt;x_1,x_2,…x_k&gt;搜索空间： 0-1 取值的二叉树，称为子集树，有2^n 片树叶。可行解：满足约束条件\\(\\sum_{i=1}^{n} w_{i} x_{i} \\leq B\\)最优解：可行解中价值达到最大的解实例：V = {12,11,9,8}，W = {8,6,4,3},B = 13货郎问题 问题描述：有n个城市，已知任两个城市之间的距离，求一条每个城市恰好经过一次的回路，使得总长度最小。实例：City = {1,2,3,4},d(1,2) = 5,d(1,3) = 9,d(1,4) = 4,d(2,3) = 13,d(2,4) = 2,d(3,4) = 7。解：&lt;1,2,4,3&gt; 长度 = 5 + 2 + 7 + 9 = 23搜索空间：排列树，每层有 (n-1)! 片树叶"
    } ,
  
    {
      "title"       : "Jetson Xavier NX 的使用记录",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./JetsonXavierNX%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95.html",
      "date"        : "2021-08-25 05:11:00 +0800",
      "description" : "记录使用Jetson Xavier NX的使用体验中遇到的问题和记录",
      "content"     : "一. 远程桌面 在windows10远程上操作jetson Xavier，远程的前提：jetson xavier和Windows的PC在同一个局域网内（我这里是直接在windows10上开启热点）。 安装xrdp：sudo apt-get install xrdp vnc4server xbase-clients1.1：桌面共享没反应 桌面共享其实就是一个vnc-server（因此没有必要再在linux上安装vnc-server了），如果要远程，必须要先开启共享，允许其他人控制自己电脑。这里发现双击了桌面共享，没反应。解决方案： 安装dconf-editor：sudo apt-get install dconf-editor 运行dconf-editor，更改系统配置，org ==&gt; gnome ==&gt; desktop ==&gt; remote-access，关闭以下两个：promotion-enabled和requre-encryption 开启桌面共享：/usr/lib/vino/vino-server1.2 开启远程 在Windows10上安装vnc-client，官网地址：https://www.realvnc.com/en/connect/download/viewer/windows/ 输入linux的ip后，直接连接即可。 1.3 建立双击可执行文件.desktop 由于每次我开启远程桌面的时候，都需要在命令行输入相关指令，很麻烦，就想说有没有可以直接在像windows一样快捷方式 写一个shell脚本来开启桌面共享：vim ~/vnc-server.sh #!/bin/sh/usr/lib/vino/vino-server 在桌面上新建一个.desktop文件：vim ~/Desktop/vnc-server.desktop [Desktop Entry]Encoding=UTF-8Type=ApplicationCategories=trueVersion=1.0Name=vnc-serverExec=sh /home/yy/vnc-server.sh #注意这里是绝对路径Path=/home/yyTerminal=false # 是否保留终端StartupNotify=true # 开机自启动 给.desktop文件加上可执行权限：sudo chmod +x ~/Desktop/vnc-server.desktop 1.4 开机自动开启 创建开机自启动文件夹：mkdir ~/.config/autostart 复制.desktop文件到该文件夹下：cp ~/Desktop/vnc-server.desktop ~/.config/autostart/ 给.desktop文件加上可执行权限：sudo chmod +x ~/.config/autostart/vnc-server.desktop二. 配置cuda和cuDNN 用JetPack刷机（本人选用的是Jetpack4.4.1），jetpack地址：https://developer.nvidia.com/embedded/jetpack-archive 安装完成后，我们可以查看jetpack的版本：cat /etc/nv_tegra_release，同时已经给我们安装好了CUDA 10.2 、cudnn 8.0 、opencv、python3.6。2.1 设置cuda环境变量​ 查询cuda版本：nvcc -V基本就能看到cuda信息了，但是这里却报错没有nvcc指令。解决方案：将cuda添加到环境变量中 sudo vim /etc/profile ，这里说下其实在/usr/local下有2个cuda相关文件夹，分别是cuda和cuda-10.2，你会发现其实是一致的。 export PATH=/usr/local/cuda-10.2/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64$LD_LIBRARY_PATHexport CUDA_HOME=/usr/local/cuda-10.2 source /etc/profile，再去命令行输入nvcc -V就可以看到cuda10.2的信息了 2.2 设置cuDNN安装好的cuDNN的头文件：/usr/include/cudnn.h安装好的cuDNN的库文件：/usr/lib/aarch64-linux-gnu/libcudnn*(1) 而这些头文件和库文件都不在cuda目录下，因此要复制到cuda目录下： 复制头文件：sudo cp /usr/include/cudnn.h /usr/local/cuda/include 复制库文件：sudo cp /usr/lib/aarch64-linux-gnu/libcudnn* /usr/local/cuda/lib64/(2) 修改文件权限：sudo chmod 777 /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*(3) 重新软连接： cd /usr/local/cuda/lib64 sudo ln -sf libcudnn.so.8.0.0 libcudnn.so.8 sudo ln -sf libcudnn_ops_train.so.8.0.0 libcudnn_ops_train.so.8 sudo ln -sf libcudnn_ops_infer.so.8.0.0 libcudnn_ops_infer.so.8 sudo ln -sf libcudnn_adv_infer.so.8.0.0 libcudnn_adv_infer.so.8 sudo ln -sf libcudnn_cnn_infer.so.8.0.0 libcudnn_cnn_infer.so.8 sudo ln -sf libcudnn_cnn_train.so.8.0.0 libcudnn_cnn_train.so.8 sudo ln -sf libcudnn_adv_train.so.8.0.0 libcudnn_adv_train.so.8 sudo ln -sf libcudnn_ops_train.so.7.3.1 libcudnn_ops_train.so.7 sudo ln -sf libcudnn_ops_infer.so.7.3.1libcudnn_ops_infer.so.7 sudo ln -sf libcudnn_adv_infer.so.7.3.1 libcudnn_adv_infer.so.7 sudo ln -sf libcudnn_cnn_infer.so.7.3.1 libcudnn_cnn_infer.so.7 sudo ln -sf libcudnn_cnn_train.so.7.3.1 libcudnn_cnn_train.so.7 sudo ln -sf libcudnn_adv_train.so.7.3.1 libcudnn_adv_train.so.7(4) 编译与验证：sudo ldconfigsudo cp -r /usr/src/cudnn_samples_v8/ ~/cd ~/cudnn_samples_v8/mnistCUDNNsudo chmod 777 ~/cudnn_samples_v8sudo make cleansudo make./mnistCUDNN # 验证cuDNN(5) 查询cuDNN版本：cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2三. 配置torch和torchvision3.1 配置torch 这里安装torch的方式和普通的windows和linux下安装不一样，pytorch有专门的jetson版本Pytorch的jetson版本下载地址：https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048 下载完成后直接使用pip进行安装：sudo pip3 install numpy torch-1.9.0-cp36-cp36m-linux_aarch64.whl 安装必须的依赖：sudo apt-get install libopenblas-base libopenmpi-dev 3.2 配置torchvision由于安装的是1.9.0版本的pytorch，直接安装于其对应的torchvision(0.10.0)：sudo pip3 install torchvision==0.10.03.3 验证GPU可用 命令行打开python，导入torch包：import torch 查看pytorch可调用的cuda版本：torch.version.cuda 查看cuda是否可用：torch.cuda.is_available()3.4 使用yolo遇到的问题问题：RuntimeError: No such operator torchvision::nms实验过程：在网上搜到的都是torch和torchvision的版本不对，要升级torchvision之类的，实验将torchvision升级到0.10.0，依旧不行。解决方案：其实在使用yolo时的非极大值抑制的时候，可以将utils.general.py中nms相关代码替换 原始代码：i = torch.ops.torchvision.nms(boxes, scores, iou_thres) 替换代码： import torchvisioni = torchvision.ops.nms(boxes, scores, iou_thres)"
    } ,
  
    {
      "title"       : "ZED2相机api使用心得",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./ZED2%E7%9B%B8%E6%9C%BAapi%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97.html",
      "date"        : "2021-08-25 05:11:00 +0800",
      "description" : "记录ZED双目相机的python API使用过程",
      "content"     : "一. ZED相机的选型 STEREOLABS（ZED相机厂家）的官网：https://www.stereolabs.com/zed/ZED双目相机有以下四种型号：ZED 2i：ZED相机最新款（ZED相机二代的进阶版，防尘防水）ZED 2 ：ZED相机二代（相较于1代多了支持IMU）ZED mini：功能上和二代基本一致，尺寸更小，性能上要差，比如这里支持的景深在15m以内，而二代的景深最大支持20m。ZED：ZED相机一代，支持2K视频，景深范围在(0.3m,25m)，无IMU,所以对于需要玩SLAM的这款就不推荐了。二. 安装ZED的SDK2.1 安装SDK ZED相机SDK官网：https://www.stereolabs.com/developers/release/我们可以看到是，所有的SDK基本都需要你安装cuda，因此我选择了cuda11.0进行安装，具体的cuda安装过程可参考我之前的一篇博客：Windows10环境下搭建CUDA10.1和pytorch1.6。直接安装即可，将下载下来的exe双击运行ZED_SDK_Windows10_cuda11.0_v3.5.2_4.exe即可，如下图所示一直往下走即可。2.2 安装python API安装完成之后，可以在 C:\\Program Files (x86)\\ZED SDK\\下看到get_python_api.py，如下图直接进行安装python的api：python get_python_api.py三. python API的使用 导入zed的python包 import pyzed.sl as sl 查看zed相机版本，这里说下，如果检测到是ZED一代的化，是不支持拿IMU数据的。 zed = sl.Camera()info = zed.get_camera_information()print(\"Camera Model: \" + str(info.camera_model)) 拿到相机的温度信息（本人亲测可用，讲真别用官方教程里的例子，得到的温度全是0，坑） sensors_data = sl.SensorsData() if zed.get_sensors_data(sensors_data, sl.TIME_REFERENCE.CURRENT) == sl.ERROR_CODE.SUCCESS: # 这里分别拿到左、右相机，imu，气压计4者的温度 temperature_left = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.ONBOARD_LEFT) temperature_right = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.ONBOARD_RIGHT) temperature_imu = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.IMU) temperature_barometer = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.BAROMETER) print(\"Left: {:.2f}, Right: {:.2f}, IMU: {:.2f}, Barometer: {:.2f}\\r\\n\".format(temperature_left, temperature_right, temperature_imu, temperature_barometer)) 拿到相机的惯导(IMU)信息，注意这里使用的是Y轴向上的右手定律，这里即Z轴方向是车头/飞机头的方向。 # 初始化zed = sl.Camera() # Create a ZED camera objectinput_type = sl.InputType() # Set configuration parameters init = sl.InitParameters(input_t=input_type) # 初始化 self.init.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP # 右手定律Y轴向上 while True: runtime_parameters = sl.RuntimeParameters() if zed.grab(runtime_parameters) == sl.ERROR_CODE.SUCCESS: zed_imu = zed_sensors.get_imu_data() zed_imu_pose = sl.Transform() ox = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[0], 3) oy = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[1], 3) oz = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[2], 3) ow = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[3], 3) print(\"IMU Orientation: Ox: {0}, Oy: {1}, Oz {2}, Ow: {3}\\n\".format(ox, oy, oz, ow)) 这里拿到的数据是四元数，如果我们想转成欧拉角，这里推推荐使用scipy的转换函数Rotation.from_quat。 from scipy.spatial.transform import Rotation# 将上面拿到的四元数转成欧拉角rot = Rotation.from_quat([ox,oy,oz,ow])eular_angle = rot.as_euler('xyz', degrees=True)print(f'翻滚角：{eular_angle[2]},俯仰角：{eular_angle[0]},方位角：eular_angle[1]') 拿取当前时间（毫秒级） # 当前时间戳(毫秒)time_zed = zed_pose.timestamp.get_milliseconds() 拿到前景图 # 定义图像数据image_size = zed.get_camera_information().camera_resolutionimage_zed = sl.Mat(image_size.width, image_size.height, sl.MAT_TYPE.U8_C4) while True： # 从zed相机中拿到前景图 zed.grab() # 拿取图像 zed.retrieve_image(image_zed, sl.VIEW.LEFT, sl.MEM.CPU, image_size) # 转成numpy image_np = image_zed.get_data() 拿到景深图（当然也可以拿到3D点云数据） # 这里需要在初始化中加入深度模式init.depth_mode = sl.DEPTH_MODE.ULTRA # 深度模式 (默认-PERFORMANCE)init.coordinate_units = sl.UNIT.MILLIMETER # 毫米级 (默认-MILLIMETER) # 定义测量数据depth_zed = sl.Mat(image_size.width / 2, image_size.height / 2) # 16位进行保存point_cloud_zed = sl.Mat(image_size.width / 2,image_size.height / 2) while True： # 从zed相机中拿到景深图 zed.grab() # 拿取景深图像 zed.retrieve_measure(depth_zed, sl.MEASURE.DEPTH,sl.MEM.CPU,image_size) # 拿取3D点云图像 zed.retrieve_measure(point_cloud_zed, sl.MEASURE.XYZRGBA) # 转成numpy depth_np = depth_zed.get_data() point_cloud_value = point_cloud_zed.get_value(x, y)[1]"
    } ,
  
    {
      "title"       : "python的打包神器——Nuitka",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./python%E7%9A%84%E6%89%93%E5%8C%85%E7%A5%9E%E5%99%A8Nuitka.html",
      "date"        : "2021-08-10 23:11:00 +0800",
      "description" : "对比pyinstaller和nuitka打包工具及nuitka的使用过程",
      "content"     : "一. pyinstaller和Nuitka使用感受1.1 使用需求 这次也是由于项目需要，要将python的代码转成exe的程序，在找了许久后，发现了2个都能对python项目打包的工具——pyintaller和nuitka。这2个工具同时都能满足项目的需要： 隐藏源码。这里的pyinstaller是通过设置key来对源码进行加密的；而nuitka则是将python源码转成C++（这里得到的是二进制的pyd文件，防止了反编译），然后再编译成可执行文件。 方便移植。用户使用方便，不用再安装什么python啊，第三方包之类的。1.2 使用感受2个工具使用后的最大的感受就是： pyinstaller体验很差！ 一个深度学习的项目最后转成的exe竟然有近3个G的大小（pyinstaller是将整个运行环境进行打包），对，你没听错，一个EXE有3个G！ 打包超级慢，启动超级慢。 nuitka真香！ 同一个项目，生成的exe只有7M！ 打包超级快（1min以内），启动超级快。 二. Nuitka的安装及使用2.1 nuitka的安装 直接利用pip即可安装：pip install Nuitka 下载vs2019(MSVS)或者MinGW64，反正都是C++的编译器，随便下。2.2 使用过程对于第三方依赖包较多的项目（比如需要import torch,tensorflow,cv2,numpy,pandas,geopy等等）而言，这里最好打包的方式是只将属于自己的代码转成C++，不管这些大型的第三方包！以下是我demo的一个目录结构（这里使用了pytq5框架写的界面）：├─utils //源码1文件夹├─src // 源码2文件夹├─logo.ico // demo的图标└─demo.py // main文件使用以下命令（调试）直接生成exe文件：nuitka --standalone --show-memory --show-progress --nofollow-imports --plugin-enable=qt-plugins --follow-import-to=utils,src --output-dir=out --windows-icon-from-ico=./logo.ico demo.py这里简单介绍下我上面的nuitka的命令： --standalone：方便移植到其他机器，不用再安装python --show-memory --show-progress：展示整个安装的进度过程 --nofollow-imports：不编译代码中所有的import，比如keras，numpy之类的。 --plugin-enable=qt-plugins：我这里用到pyqt5来做界面的，这里nuitka有其对应的插件。 --follow-import-to=utils,src：需要编译成C++代码的指定的2个包含源码的文件夹，这里用,来进行分隔。 --output-dir=out：指定输出的结果路径为out。 --windows-icon-from-ico=./logo.ico：指定生成的exe的图标为logo.ico这个图标，这里推荐一个将图片转成ico格式文件的网站（比特虫）。 --windows-disable-console：运行exe取消弹框。这里没有放上去是因为我们还需要调试，可能哪里还有问题之类的。经过1min的编译之后，你就能在你的目录下看到：├─utils //源码1文件夹├─src // 源码2文件夹├─out // 生成的exe文件夹 ├─demo.build └─demo.dist └─demo.exe // 生成的exe文件├─logo.ico // demo的图标└─demo.py // main文件当然这里你会发现真正运行exe的时候，会报错：no module named torch,cv2,tensorflow等等这些没有转成C++的第三方包。这里需要找到这些包（我的是在software\\python3.7\\Lib\\site-packages下）复制（比如numpy,cv2这个文件夹）到demo.dist路径下。至此，exe能完美运行啦！"
    } ,
  
    {
      "title"       : "Mask_RCNN在TF2下跑通自己的数据集",
      "category"    : "",
      "tags"        : "CV, 深度学习",
      "url"         : "./Mask_RCNN%E5%9C%A8TF2%E4%B8%8B%E8%B7%91%E9%80%9A%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86.html",
      "date"        : "2021-07-26 20:11:00 +0800",
      "description" : "讲述Mask RCNN在tensorflow2.x下如何跑通自己的数据集",
      "content"     : "论文原文地址：https://arxiv.org/abs/1703.06870 MaskRCNN官方的git地址：https://github.com/matterport/Mask_RCNN一. 构建数据集这里参考官方推荐的气球语义分割的例子，这里选用的是和他一致的打标工具 VIA (VGG Image Annotator)。个人感觉这个比labeme好用太多。 直接在VIA官网下载即可，下载完成后如下图所示，直接用浏览器打开via.html即可开箱使用。 选择Add Files添加图片后，选择Attributes设置打标的label，然后用多边形工具进行打标，如下图。 输出json格式的打标结果 将图片和json结果保存在同一个目录下，构建自己的数据集时可参考我的目录结构。├─dataset│ ├─train│ │ ├─1.jpg│ │ ├─2.jpg│ │ ├─3.jpg│ │ └─annotations.json│ └─val│ │ ├─1.jpg│ │ ├─2.jpg│ │ ├─3.jpg│ │ └─annotations.json二. 准备工作 由于官方的代码只支持tensorflow的版本都是1.x的版本，对于tensorflow 2.x的版本官方代码中很多地方不能调用，这里需要大量修改。2.1 环境搭建本人是Windows系统，安装的是11.0版本的cuda，以下是我安装的项目必须的python包。 scikit-image==0.16.2 这里说一句，如果安装的是更高版本的，最好降低成0.16的，不然可能后面训练的时候会报Input image dtype is bool. Interpolation is not defined with bool data type，这里参考。 tensorflow==2.2.0 就是因为tensorflow的1.x版本不支持cuda11.0，才折腾好久…. keras==2.4.0 numpy==2.1.02.2 源码的替换 之前本人找到一个TF2.0版本的Mask RCNN，哎，但是发现没卵用！ 下载官方源码：git clone https://github.com/matterport/Mask_RCNN.git 替换官方的核心代码mrcnn下的所有代码。PS：由于这里改的官方代码中mrcnn/model.py,mrcnn/config.py,mrcnn/utils.py,parallel_model.py需要修改的文件太多，直接给出git地址，大家可以直接下在mrcnn这个文件夹及其下面的py文件对官方的mrcnn这个文件夹进行替换（不然太折腾人了）。2.3 训练代码修改 这里训练的代码参考的是官方的samples/ballon/ballon.py这个文件，直接复制到主目录Mask_RCNN下，修改成自己想要的名字，比如train.py。 修改根地址，这里由于从samples/ballon/目录到主目录下，因此修改ROOT_DIR = '.' 修改config类成自己的类，注意我这里是改成了动物类，然后一共是识别2个种类，然后最好这里的NAME和利用VIA构建数据集中的Attribute一致。 class AnimalConfig(Config): # Give the configuration a recognizable name NAME = \"animal\" # Adjust down if you use a smaller GPU. IMAGES_PER_GPU = 1 # Number of classes (including background) NUM_CLASSES = 1 + 2 # Background + balloon # Number of training steps per epoch STEPS_PER_EPOCH = 100 # Skip detections with &lt; 90% confidence DETECTION_MIN_CONFIDENCE = 0.9 修改Dataset类成自己的Dataset类，同时修改load函数class AnimalDataset(utils.Dataset): def load_animal(self, dataset_dir, subset): # Add classes. We have only one class to add. self.add_class(\"animal\", 1, \"pig\") self.add_class(\"animal\", 2, \"cow\") ..... self.add_image( \"animal\", image_id=a['filename'], path=image_path, width=width, height=height, polygons=polygons) 修改load_mask函数 def load_mask(self, image_id): image_info = self.image_info[image_id] if image_info[\"source\"] != \"animal\": return super(self.__class__, self).load_mask(image_id) 修改image_reference函数 def image_reference(self, image_id): \"\"\"Return the path of the image.\"\"\" info = self.image_info[image_id] if info[\"source\"] == \"animal\": return info[\"path\"] else: super(self.__class__, self).image_reference(image_id) 修改训练函数def train(model): \"\"\"Train the model.\"\"\" # Training dataset. dataset_train = AnimalDataset() dataset_train.load_animal(args.dataset, \"train\") dataset_train.prepare() # Validation dataset dataset_val = AnimalDataset() dataset_val.load_animal(args.dataset, \"val\") dataset_val.prepare()三. 训练直接在命令行中python train.py train --dataset=./dataset --weights=coco即可看到模型开始训练了。这里注意2点： 用VIA打标好的数据集注意是放到Mask RCNN主目录下，所以这里的命令参数--dataset=./dataset 权重用的是coco形式，即--weights=coco，如果不是的话，你会遇到这类问题mrcnn_bbox_fc/kernel:0' shape=(1024, 8) dtype=float32_ref&gt; has shape (1024, 8), but the saved weight has shape (1024, 324) ，具体可参考"
    } ,
  
    {
      "title"       : "ORB-SLAM3在windows下的编译使用",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./orb_slam3.html",
      "date"        : "2021-05-12 10:11:00 +0800",
      "description" : "讲述ORB-SLAM3在windows下的编译及其使用",
      "content"     : "一. 数据集1.1 数据集介绍Euroc：基于室内的MAV(Micro Aerial Vehicle，微型飞行器)，一共两个场景（Machine Hall + Vicon Room）,其中每个数据集包含2个下载连接： ROS(Robot Operating System) bag：机器人操作库，适用于嵌入式，这里推荐一个很好的双目+IMU应用在jetson nano上的git。 ASL Dataset Format：数据集结构，包含传感器文件和双目相机的图片。1.2 数据使用介绍可用的数据包含： Visual-Inertial Sensor Unit（视觉惯性传感器单元） Stereo Images（双目图片） IMU（惯导数据） Ground-Truth位姿轨迹 Vicon 6轴运动姿态捕捉系统 Leica MS50 3维姿态镭射追踪 Leica MS50 3维重构 1.3 IMU数据介绍 w_RS_S_x [rad s^-1] ：MAV在R坐标系下的x轴角速度信息，单位rad/s w_RS_S_y [rad s^-1] ：MAV在R坐标系下的y轴角速度信息，单位rad/s w_RS_S_z [rad s^-1] ：MAV在R坐标系下的z轴角速度信息，单位rad/s a_RS_S_x [m s^-2]：MAV在R坐标系下x轴的线加速度信息，单位m/s^2 a_RS_S_y [m s^-2]：MAV在R坐标系下y轴的线加速度信息，单位m/s^2 a_RS_S_z [m s^-2]：MAV在R坐标系下z轴的线加速度信息，单位m/s^2二. 第三方包编译 ORB_SLAM3论文地址：https://arxiv.org/pdf/2007.11898.pdf 使用ORB_SLAM3官方git，推荐使用的系统为ubuntu 18.04，本人用win 10下进行测试的，这里推荐一个在win 10下编译ORB_SLAM3的git，目前在该仓库下编译运行没毛病！2.1 前期依赖的第三方包 eigen：线性算术的C++模板库（属于g2o的依赖），这里直接用vcpkg安装vcpkg install eigen boost：后面编译ORB_SLAM3库需要，这里也是直接用vcpkg安装vcpkg install boost。 opencv3.4.11：编译DBoW2和ORB_SLAM3需要。直接上官网下载exe即可，当然也可以利用vcpkg进行安装。2.2 DBoW2用于SLAM回环检测，这里需要配置opencv环境。具体过程如下： 给Thirdparty/DBoW2/CMakeLists.txt配置opencv3.4.11的路径 set(OpenCV_DIR \"D:/software/opencv/opencv/build\") 在Thirdparty/DBoW2路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/DBoW2.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT) 右键项目 ==&gt; 生成，即可看到生成好的lib文件Thirdparty/DBoW2/lib/Release/DBoW2.lib 2.3 g2o用于图优化的框架。具体过程如下： 在Thirdparty/g2o路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/g2o.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT)；C/C++ ==&gt; 预处理器 ==&gt;最上面加入WINDOWS 右键项目 ==&gt; 生成，即可看到生成好的lib文件Thirdparty/g2o/build/Release/g2o.lib2.4 Pangolin用于3D视觉和3D导航的视觉图和用户之间的交互。这里其实和编译ORB_SLAM3没有关系，但是我们使用ORB_SLAM3库的时候应用的例子上是需要这个库的。具体过程如下： 在Thirdparty/g2o路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/Pangolin.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT) 这里是需要下载它依赖的其他库的，最好对git设置代理 git config --global http.proxy http://127.0.0.1:1080git config --global https.proxy http://127.0.0.1:1080 右键ALL_BUILD ==&gt; 生成，即可看到生成好的lib文件ThirdParty/Pangolin/lib/Release/pangolin.lib 三.编译ORB_SLAM3 给orbslam3-windows/CMakeLists.txt配置opencv3.4.11的路径 set(OpenCV_DIR \"D:/software/opencv/opencv/build\") 在orbslam3-windows的路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/ORB_SLAM3.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT)；C/C++ ==&gt; 预处理器 ，添加以下预编译器定义 WINDOWSCOMPILEDWITHC11 右键项目 ==&gt; 生成，即可看到生成好的lib文件ORB_SLAM3/build/Release/ORB-SLAM3.lib 四. 编译测试案例及展示4.1 编译stereo_inertial_euroc 用vs2019打开build/ORB_SLAM3.sln 将配置改成release，同时右键项目stereo_inertial_tum_vi ==&gt; 属性 ==&gt; C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT)；C/C++ ==&gt; 预处理器 ，添加以下预编译器定义COMPILEDWITHC11；链接器 ==&gt; 高级 ==&gt; 导入库，改为空；链接器 ==&gt; 输入 ==&gt; 去掉..\\Thirdparty\\boost_1_67_0\\lib64-msvc-14.1\\libboost_serialization-vc141-mt-s-x64-1_67.lib（由于这里是vcpkg安装的boost，因此该路径下根本没有这个lib）。 右键项目 ==&gt; 生成，即可看到生成好的exe文件ORB_SLAM3/Examples/Stereo-Inertial/Release/stereo_inertial_tum_vi.exe4.2 使用展示案例stereo_inertial_euroc这个案例是双目 + 惯导的Euroc数据集的应用。 将下载好的数据集文件夹名字改成MH01(这里是由于本人下载是MH_01_easy.zip) 进入到生成好的exe文件夹下cd orbslam3-windows\\Examples\\Stereo-Inertial\\Release，可以看到生成好的stereo_inertial_euroc.exe 开启程序： .\\stereo_inertial_euroc.exe ..\\..\\..\\Vocabulary\\ORBvoc.txt ..\\EuRoC.yaml ..\\MH01\\ ..\\EuRoC_TimeStamps\\MH01.txt dataset-MH01_stereoi 结果展示如下图所示："
    } ,
  
    {
      "title"       : "目标检测(one stage)-从FPN到DSSD",
      "category"    : "",
      "tags"        : "CV, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_4.html",
      "date"        : "2021-04-28 05:11:00 +0800",
      "description" : "讲述目标检测(one stage)-从FPN到DSSD",
      "content"     : "一. FPN特征金字塔网络 论文地址：https://arxiv.org/pdf/1612.03144.pdf这篇论文发布的时间是2017年4月19号，可以说在此之后，对于目标检测（小物体）而言，提升巨大，基本之后的模型比如DSSD，yolov3等都参考过该模型架构。1.1 解决的问题 目标检测的基本挑战（问题）：识别多尺度变化的目标能力不足。这里解决了一下两个方面的难点： 相机距离目标远近不同导致拍摄的图片中目标尺寸不同而导致识别效率低下。 小目标物体的识别较难。 1.2 图像特征金字塔特征金字塔是在不同大小尺寸的目标检测中的一个基础组件。 如下图所示，经过多次特征抽取后，越到高层的feature map所囊括的细节信息就越少，对于底层信息（比如小的目标）预测就越难。 那么图像特征金字塔做了什么呢？既对同一张图片进行多次下采样，从而得到多张不同尺寸的图片，进而生成不同尺寸的feature map，从而使模型拥有对不同尺度大小的物体进行检测的能力。但是论文中也提出它的问题：消耗太大的内存和计算量。 看到上面这张图的时候，你是不是觉得很熟悉？和某个网络的推理层很像？是的，就是SSD。SSD和YOLOv1最大的差别（之前的文章中有讲，有兴趣可以查看本人之前的文章）其实是推理层的不同，SSD用的就是多尺度的特征图综合来预测目标，从而达到对于小物体也能够检测的目的。 如下图所示，是不是感觉和上面的图像特征金字塔很像？差别还是有的，SSD是对同一张图片进行卷积抽取其不同尺度的feature map进行分别做预测，而特征金字塔是对不同尺度的图片分别做特征抽取得到不同尺度的feature map在分别做预测。这里论文中也提到SSD的缺点：失去了高层语义信息重用的机会，导致低层语义信息不足。 1.3 特征金字塔网络（FPN） 这里推荐一篇个人认为不错的git仓库：easy-fpn.pytorch，是由pytorch复现的fpn网络。FPN的网络结构如下图所示：我们可以看到，其实说白了FPN相较于SSD和特征金字塔厉害的地方： 强于SSD：重用了高层语义信息，每次预测都是结合了当前层和上一层的语义信息，使每一层不同尺度的特征图都具有较强的语义信息。 强于特征金字塔：同样通过上下采样（这里不同的是采样的feature map），使模型拥有对不同尺度大小的物体进行检测的能力，却又构建了一个端到端的网络。下面是来源于我上面推荐的git仓库的FPN网络的细节架构图。方便我们很好的理解FPN中到底做了什么。如下图所示，从底层的feature map（1067 * 800）到高层的feature map(34 * 25)，每层feature map经过1 * 1的卷积之后和经过up sample之后的上层feature map做一个add操作，在进行推理。下面是细节复现的pytorch代码：# Bottom-up pathwayc1 = self.conv1(image)c2 = self.conv2(c1)c3 = self.conv3(c2)c4 = self.conv4(c3)c5 = self.conv5(c4)# Top-down pathway and lateral connectionsp5 = self.lateral_c5(c5)p4 = self.lateral_c4(c4) + F.interpolate(input=p5, size=(c4.shape[2], c4.shape[3]), mode='nearest')p3 = self.lateral_c3(c3) + F.interpolate(input=p4, size=(c3.shape[2], c3.shape[3]), mode='nearest')p2 = self.lateral_c2(c2) + F.interpolate(input=p3, size=(c2.shape[2], c2.shape[3]), mode='nearest')# Reduce the aliasing effectp4 = self.dealiasing_p4(p4)p3 = self.dealiasing_p3(p3)p2 = self.dealiasing_p2(p2)p6 = F.max_pool2d(input=p5, kernel_size=2)二. DSSD 论文地址：https://arxiv.org/abs/1701.06659.pdf这篇论文发布的时间是2017年1月23号。DSSD(Deconvolutional Single Shot Detector)，听名字就知道了，是SSD的升级版本，而且其实就是SSD + FPN的结合体，但是很奇怪，为啥你还在别人FPN后面发布呢？2.1 和SSD的差别下图是SSD和DSSD的网络架构示意图。SSD和DSSD在前面特征抽取层的backbone都是一样的，差别在于后面推理的过程： SSD的推理过程经过多个卷积得到的不同尺寸的特征图来进行预测。 DSSD的推理过程是FPN网络架构的复现：由底层特征结合经up sampling之后的上层特征，做一个结合的操作，论文中提到2中结合方式： Eltw-sum：也叫broadcast add，将浅层和深层的特征图在对应的通道上做加法运算。 Eltw-prod：也叫broadcast mul，将浅层和深层的特征图在对应的信道上做乘法运算。 2.2 DSSD在模型效果上的提升下图是SSD（左1和右1）和DSSD（左2和右2）的模型在同一张图片上的检测效果。可以明显的发现： DSSD能检测到更多的目标。 DSSD能检测到更小的目标。"
    } ,
  
    {
      "title"       : "C++下消息队列（多消费者模式）的实现",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84C++%E5%AE%9E%E7%8E%B0.html",
      "date"        : "2021-04-24 07:21:00 +0800",
      "description" : "讲述在C++下实现的消息队列（多消费者模式）",
      "content"     : "不允许用现成的消息队列比如rabbitmq等，非要造轮子！一. 生产者-消费者模式 生产者：这里由于是通过txt文件来进行交互，相当于txt文件的内容就是生产者，同时还需要实时监控txt文件，将其新消息放入队列。 消费者：从队列中取消息，并需要告诉队列该条消息已经被消费。 断点续传：需要考虑到程序崩溃之后，知道从哪开始消费。1.1 多消费者——多线程这里如果如果是IO操作较多的话，推荐使用多线程来创建消费者。具体创建和消费的过程如下： 我们可以看到其实这里的多消费者其实是串联的形式来进行消费，因此如果是CPU资源低，IO操作多的话，推荐这种形式。 多线程之间的内存变量交互很友善，不像多进程（哎，难受啊）。 1.2 多消费者——多进程 考虑到这里是需要像CPU请求较多资源，甚至是需要使用GPU的资源和利用CUDA加速（深度学习模型占用资源较多），因此我这里使用的是多进程来构建消费者。具体创建和消费过程如下:二. 过程的实现 具体代码可以参考本人的git 定时器：每隔一段时间扫描txt文件，并将文件中新加入的消息存放入队列中。 // timer扫描int wait_sec = 1;// 单独启动一个线程持续扫描文件（每5秒）string path = \"video.txt\";Timer timer1;timer1.start(2000, std::bind(getVideoFromTxt, path, wait_sec, &amp;output)); 中间件：需要存放已经消费的消息，这样知道消费的具体位置，且支持程序崩溃/断掉之后，重启后知道在哪开始消费，同样用txt进行保存到本地。 多消费者：使用多进程创建消费者，这里考虑到进程中间的共享内存不好交互，直接使用txt来交互数据（反正进程之间的内存交互其实也是通过一个共享文件映射来完成的）。这里直接使用调用命令行的方式来构建消费者。 void gen_multiProcess(int id, string input_txt,string output_txt) { STARTUPINFO si; si.cb = sizeof(si); PROCESS_INFORMATION pi; ZeroMemory(&amp;si, sizeof(si)); ZeroMemory(&amp;pi, sizeof(pi)); string cmdLine = \"D:/vs_project/setTimer/x64/Debug/setTimer.exe \" + input_txt + \" \" + output_txt; cout &lt;&lt; cmdLine &lt;&lt; endl; wstring str = StringToWString(cmdLine); BOOL bSuccess = CreateProcess(NULL, const_cast&lt;LPWSTR&gt;(str.c_str()), NULL, NULL, FALSE, 0, NULL, NULL, &amp;si, &amp;pi); if (bSuccess) { //handleOfProcess[id] = pi.hProcess; cout &lt;&lt; \"Process-\" &lt;&lt; id &lt;&lt; \"completed!\" &lt;&lt; endl; } else { cout &lt;&lt; \"Error:\" &lt;&lt; id &lt;&lt; endl; }} 三. 收获其实手写消息队列，帮助自己更多的了解消息消费机制，以及多进程和多线程的使用，当然了也更了解了C++的标准库。所以推荐大家还是亲手动手手写一哈。哎，作为一个python调包侠突然写这种偏底层，有点难受好吧。"
    } ,
  
    {
      "title"       : "目标检测(one stage)-YOLOv2",
      "category"    : "",
      "tags"        : "CV, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_3.html",
      "date"        : "2021-04-21 04:21:00 +0800",
      "description" : "目标检测（one stage）——YOLOv2",
      "content"     : "一. 与V1的不同之处YOLOv2相较于YOLOv1在VOC2007数据集上表现从63.4%提升到78.6%。YOLOv2与YOLOv1的不同之处一共体现在下面几个方面： Batch Normalization（批归一化层）：在模型不过拟合的前提下，可以拿掉Dropout层；同时加速模型训练。在VOC2007数据集上效果mAP提升2.4%。 High Resolution Classifier（提高分辨率）:原本YOLOv1中模型训练的使用的是224 * 224分辨率的图像，现在resize成448 * 448的图片，最后经过10个epoch的微调。在VOC2007数据集上效果mAP提升3.7%。 Convolutional With Anchor Boxes（提高检测目标数量）：原本yolov1中将其中一个pool层拿掉后， feature map的大小由7 * 7 变成了13 * 13，然后每个1 * 1的grid里面增加了K个anchor boxes，因此从yolov1只能检测7 * 7 = 49个目标，增加到了13 * 13 * K个目标。在VOC2007数据集上效果虽然mAP下降了0.3%，但是在Recall上提升了7%。 New Network（DarkNet-19）：提出了一个新的网络架构DarkNet-19（19个Conv 和 5个MaxPool），能够在YOLOv1的基础上减少33%的计算量。在VOC2007数据集上效果mAP提升0.4%。 Dimension Clusters（尺度聚类）：如果Anchor boxes一开始就能和实际的物体的宽高比很接近，那么对于模型的收敛是否有帮助呢？YOLOv2这里就是使用了KMeans对所有的图片的宽高比（利用IOU计算之间的距离）进行聚类，结果是聚类个数K取5效果最好，从而每个grid中找到5个最好的预选框。 Direct Location Prediction（绝对位置预测）：模型原本的anchor box在预测物体的x,y坐标时候会发生数值不稳定的现象（而R-CNN网络的boundingbox并非随机，而是由RPN网络生成），毕竟随机初始化的anchor box的位置，肯定需要花费大量时间才能学习到合适的位置。那么yolov2是如何完成？这里红色为anchor box，蓝色为模型的预测框，这里引入sigmoid函数来缩小到(0,1)之间，同时对计算后的结果进行归一化的处理， 使用尺度聚类和直接位置预测两种方式后模型在VOC2007数据集上效果mAP提升4.8%。 Fine-Grained Features（颗粒度特征）：将feature map拆解成更小的feature map。借由前面DarkNet得到高解析度的特征拆解成小解析度的特征，因此来检测较小的物体。VOC2007数据集上效果mAP提升1%。 Muti-Scale Training（多尺度训练）：每10个batch去做一个resize的动作，从一开始的320 * 320到最终的608 * 608，最后得到的feature map从开始的10 * 10到19 * 19。VOC2007数据集上效果mAP提升1.2%。 High Resolution Detector（输入大size的input）：通过输入图像尺寸更大，使得模型检测到更小物体。下图是从288到544尺寸之间mAP提升效果，当然fps会相应的减少。VOC2007数据集上效果mAP提升2%。 二. 其他模型的比较下图是YOLOv2在COCO2015数据集上的表现。可以看到YOLOv2相较于SSD还有一些的差距的。"
    } ,
  
    {
      "title"       : "windows下安装python-pcl",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./Windows%E4%B8%8B%E5%AE%89%E8%A3%85PCL.html",
      "date"        : "2021-04-10 18:21:00 +0800",
      "description" : "介绍如何在win10下安装python版本的PCL点云库",
      "content"     : "一. 准备工作 python 版本：3.7.9 cython numpy python-pcl:1.9.1 python-pcl源码：后面需要进行编译 PCL1.9.1的All-In-One Installer ：目前安装仅支持1.6到1.9的版本 visual studio 2019 Windows Gtk二. 安装 将下载好的ALL-In-One Installer进行安装，这里会要求你添加到环境变量（必须添加啊），并且会安装OpenNI这个工具。 解压下载好的windows Gtk，将bin目录下所有文件复制到python-pcl源码目录下的pkg-config目录下。 在pkg-config目录下，运行脚本InstallWindowsGTKPlus.bat，该脚本会下载必须的内容，下载完成后会多出这些文件夹，如下图所示 安装python的pcl包： cd 你安装python-pcl源码目录 python setup.py build_ext -i python setup.py install 三. 安装遇到的坑3.1 坑一：cannot find PCL 问题：当你运行python setup.py build_ext -i的时候报出：setup.py: error: cannot find PCL, tried pkg-config pcl_common-1.7 pkg-config pcl_common-1.6 pkg-config pcl_common 解决方案：这里就是上面说的，别下除了1.6到1.9版本的pcl的All-In-One Installer啊。3.2 坑二：DLL load failed 问题：全部安装完成之后，一切没有问题了，当你打开python，运行import pcl的时候报出：DLL load failed。 解决方案：重启电脑！四. python版本的使用4.1 点云数据的展示（python）构建点云–Point_XYZRGBA格式(需要点云数据是N*4，分别表示x,y,z,RGB ,其中RGB 用一个整数表示颜色)，下面是python版本的点云数据展示import pclimport pcl.pcl_visualization as viewer #可视化库import numpy as np# cloud = pcl.load(\"cloud.pcd\")cloud_np = np.load(\"cloud.npy\")cloud = pcl.PointCloud_PointXYZRGBA(cloud_np)visual = pcl.pcl_visualization.CloudViewing()visual.ShowColorACloud(cloud)v = Truewhile v: v = not (visual.WasStopped())4.2 命令行展示由于上面已经下载了PCL1.9.1了，可以直接在命令行中进行展示：pcl_viewer_release H cloud.PCD，下面的是来自Middlebury 2014数据集中经过立体匹配后的3D点云图。"
    } ,
  
    {
      "title"       : "目标检测(one stage)-SSD",
      "category"    : "",
      "tags"        : "CV, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_2.html",
      "date"        : "2021-04-04 04:21:00 +0800",
      "description" : "目标检测（one stage）——SSD",
      "content"     : "一. YOLO和SSD的对比yolo和ssd两个模型结构如下图所示：两个模型之间最主要的差别： 在特征抽取层其实相差不大：YOLO用的是器自己的conv架构；SSD用的是VGG-16 主要差别在结果预测上：YOLO用的是全连接层后得到7*7的grid，利用每个grid的boundingbox来做目标检测；SSD利用不同大小的feature map来做目标检测。二. 模型结构2.1 特征抽取层那么如何从VGG-16的结构变成SSD的结构呢?下图是一个VGG-16的示意图。将VGG-16的最后一层pooling层变成3*3 的卷积层，再接一个atrous conv（空洞卷积）拿到不同大小的feature map。如下所示。2.2 空洞卷积这里运用atrous conv layer而不是普通的conv layer的目的： 在相同的感受野的同时，能获得更快的运算速度如下图所示，是5 * 5 的卷积的kernel和3 * 3的atrous conv的kernel的感受野。可以看到，如果是3 * 3的conv层接5 * 5的conv层，那么feature map中单一点的感受野其实是7个像素点；而如果是3 * 3的conv层接3 * 3的atrous conv层，能达到相同的感受野，且计算速度更快。2.2 推理层下图是SSD的推理层的示意图。可以看到，图片经过vgg16之后，首先会得到较浅的feature map,随后经过几层卷积之后，得到较为深层的feature map（所以在上图中仅有较深层的能检测到车这种大物体），同时每层的feature map都会经过一个检测器和分类器得到检测结果，最后经过NMS得到最终的检测结果。那么整个SSD的anchor box的数量是：\\(38*38*3+19*19*6+10*10*6+5*5*6+3*3*6+1*1*6 = 7308\\)三. 模型训练3.1训练lossSSD和YOLO的loss中的检测类别值有所不同：假定检测目标一共A个类别，那么YOLO的预测类别数位A个，而SSD的预测类别则是A+1个（包含了背景类）。如下图所示。3.2 难负例挖掘对于正负样本不均衡的情况，SSD采用了hard negative mining(难负例挖掘)技巧来解决。hard negative是指在图片中容易将负样本（背景）看成是正样本（前景）的样本。而mining的操作就是将这类样本放入模型进行学习，从而减少模型的false positive。那么SSD是如何引用hard negative mining技巧呢？如下图，其中蓝色的box的我们希望它的confidence较低，而绿色的confidence较高。 对于一张图而言，选出其中anchor box中negative置信度较高的box。 正负比例的anchor box = 1：33.2 数据增强SSD模型在论文中也使用了很多不同的data augmentation(数据增强)的操作。方式一： 针对原始输入图片和ground truth进行IOU的操作 对其中iou = 0.1，0.3，0.5，0.7和0.9来进行采样。 对采样后的图片进行resize成相同大小的图片，然后进行水平翻转的操作。方式二（Random Expansion-得到的小目标训练样本）： 对原始图像做不同比例的缩小。 然后放在相同大小图片中不同的地方。四. 结果比较可以看到，SSD相较于YOLO在准确性上有很大的提升，同时预测速度上也能达到很高的fps。"
    } ,
  
    {
      "title"       : "目标检测(one stage)-YOLOv1",
      "category"    : "",
      "tags"        : "CV, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_1.html",
      "date"        : "2021-03-11 04:21:00 +0800",
      "description" : "目标检测（one stage）的开始——YOLOv1",
      "content"     : "一. 目标检测算法的分类及历史1.1 目标检测算法的分类目标检测算法主要分为2大类： one-stage(one-shot object detectors) ：直接预测目标的bounding box及其类别。特点是一步到位，速度很快。比如：YOLO，SSD等系列模型。 two-stage：需要先使用启发式方法(selective search)或者CNN网络(RPN)产生Region Proposal，然后再在Region Proposal上做分类与回归。特点是：慢，但是准确率高。比如：RCNN系列模型。由于在工业应用中，往往对模型预测速度有要求，而two-stage目标检测模型由于先天的不足，因此本文仅考虑one-stage目标检测模型。1.2 目标检测发展流程目标检测（one-stage）的总体发展流程： 2015.06 — YOLOv1：第一个one-stage目标检测器。 2015.12 — SSD：结合anchor box和多尺度特征的one-stage目标检测器。 2016.12 — YOLOv2：YOLO的第二版。 2016.12 — FPN：特征金字塔（结合不同尺寸的特征图像） 2017.01 — DSSD：SSD结合FPN。 2017.08 — RetinaNet：Focal Loss解决正负样本不均衡 2018.04 — YOLOv3：YOLO的第三版。 2018.07 — CBAM：Attention机制的目标检测。 2019.11 — EfficientDet：Google提出的目标检测器。 2020.04 — YOLOv4：YOLO的第四版。 2020.06 — YOLOv5：YOLO第五版。二. YOLO当我最初学习图像分类的时候，就一直疑惑：如果我利用卷积层抽取目标特征后直接把分类任务做成回归任务（包含目标的位置和类别信息）可以作为目标检测器么？答案来了——YOLO（You Look Only Once）。2.1 模型结构YOLO模型的结构如上所示： 输入为一个448*448的一个图片输入。 一共是经过24层的卷积层抽取特征，使用relu作为每一层的激活函数。 最后通过全连接层，且output形式为[7,7,30]的输出。模型输出的理解： 将448 * 448的图像分为7 * 7的grid（网格），每个grid都会进行判断：是否为前景，且会构建2个boundingbox来框出物体。因此，一共是有7 * 7 * 2个框。而每个grid都会输出x,y,w,h,c；这里的confidence的计算就是前景目标的概率 * iou的值。 除了boundingbox的计算外，当然还需要输出目标是哪个类别，即输出检测到的目标是某个类别的概率。这样就可以计算每个grid属于某个类别下的iou情况了。 最后利用NMS（非极大值抑制：顾名思义就是不是最大的置信度就不要了）找到每个目标的最合适的框。具体NMS的算法步骤如下： （1）首先拿到的是YOLO模型输出的结果，即7 * 7 * 2个框，每个框都是由5个元素（x,y,w,h,c）。这里需要知道一张图片中有多少个目标且目标confidence最高的结果。 （2）通过计算两两框之间的IOU（交并比），用来划分一张图片中有多少个目标（如果IOU&gt;0说明属于同一目标下的框）。 （3）对同一目标下的所有框的confidence进行排序，找到最大的的confidence对应的框。 2.2 模型训练这里主要讲述模型训练过程中loss的定义过程。2.2.1 Location Loss定义如下所示：如上所示，假定是将图片划分为3 * 3个grid，每个grid有且仅有一个预测框，由于只计算和前景目标匹配的框，因此只会计算grid5和grid7的location loss。 grid5的loss： grid7的loss： 但是这里看大大猫和小猫的loss竟然是一样的，大猫的loss应该明显要小一些，而小猫的loss明显要大一些。因此这种loss的计算还需要提升。这里就将w,h的分别先进行开根号处理。 grid5的loss： grid7的loss：2.2.2 Object Loss定义如下：那么上图的每个grid的confidence的值如下：object loss的值为：但是这个是只划分了3 * 3个grid的，那么如果是原论文中的7 * 7的情况下呢，此时的object loss的值为：我们可以看到，0.96这个检测的背景的loss就过大了，那么在反向传播的过程中，梯度的变化很大程度就着重在背景的部分，以至于学习前景的能力较差。因此，重新定义object loss（其实就是在背景loss引入一个系数，比如0.5）：2.2.3 classification loss定义如下：2.4 YOLO存在问题2.4.1 同一个grid却是多个目标的中心点如上图所示，人和车的中心点基本都落在中心的grid中，对于yolo而言，就无法分辨到底是人还是车？一个grid下只能预测1个目标。2.4.2 同一个grid中存在多个小目标如上图所示，同一个grid下有多个鸟（小目标），而对于yolo而言，一个grid下只能预测1个目标。"
    } ,
  
    {
      "title"       : "Scoop软件推荐",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./Scoop%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90.html",
      "date"        : "2021-02-04 04:21:00 +0800",
      "description" : "介绍Windows平台软件管理工具",
      "content"     : "一. scoop包管理器 scoop官网：https://scoop.sh/一直以来觉得linux的apt-get和macos的brew来安装软件特别方便，而windows下总是要去一个个找软件，然后一个个安装，一个个设置环境变量，很麻烦。这里就推荐一款类似的windows的包管理器——scoop。1.1 安装scoop 确保是powershell 5（及其以上）进行安装：Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') iwr -useb get.scoop.sh | iex 如果这里报错，需要需要改变执行规则：Set-ExecutionPolicy RemoteSigned -scope CurrentUser1.2 scoop必备软件 aria2：多线程下载软件（scoop下载软件的时候也用这个来加速，scoop会默认设置） sudo：感觉和linux的sodu一样，windows不是有个“以管理员身份运行”么。 git：下载git的时候，需要你下载7zip。1.3 scoop常用命令 寻找软件：scoop search 软件名 安装软件：scoop install 软件名 删除软件：scoop uninstall 软件名 查看已安装的软件：scoop list 清理缓存：scoop cache rm 软件名 或者scoop cache rm * 查看可添加仓库：scoop bucket known 添加额外仓库：scoop bucket add 软件名 二. 美化命令行是不是总是觉着无论是windows的cmd.exe还是powershell都特别丑？那就来美化他吧！2.1 美化终端这里推荐的是微软自带的windows-terminal，反正个人感觉挺好用也挺好看的。 直接用scoop进行安装即可：scoop install windows-terminal，但是注意一点：这个终端需要你的windows系统最少为Windows 10 183622.2 美化命令行（主题）不知道有多少人是喜欢linux的oh my zsh这个主题的，我反正是大爱，因此一直在windows上找它的替代品，没想到还真被我找到了——oh-my-posh主题框架。 scoop进行安装：scoop install oh-my-posh 安装oh-my-posh模块：Install-Module oh-my-posh 设置robbyrussell主题：Set-Theme robbyrussell，这里由于本人比较喜欢linux的oh my zsh这个主题，因此选择这个和他接近的主题，大家也可以根据自己喜欢来选择自己的主题。2.3 设定每次开启都启用主题选择好了自己喜欢的主题，但是发现每次都需要打这一行Set-Theme robbyrussell代码才能启动主题，是不是觉着老蛋疼了，所以下面是介绍一劳永逸的方案。 建立一个profile： if (!(Test-Path -Path $PROFILE )) { New-Item -Type File -Path $PROFILE -Force } 修改下这个profile：vim $profile 添加自己喜欢的主题风格到profile中：Set-Theme robbyrussell接下来，你每次开启终端后发现自动加载自己喜欢的主题哦！三. 必备的一些软件推荐这里推荐的软件主要是利用scoop可以直接完成下载的。 语言：java、python、ruby等等你没听错，是的，可以直接用scoop来安装，环境变量都给你配好咯。 编辑器：vscode、pycharm、eclipse等等你能想到的免费的它都有！ 大数据：spark、hadoop、rabbitmq、kafka、flume等等 数据库：redis、mongodb、mysql 压测：jmeter、postman 其他需要的软件：cmake、tar、typora、vim、touch、youtube-dl、chrome、OpenSSL"
    } ,
  
    {
      "title"       : "Libtorch的GPU使用问题记录",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Libtorch%E7%9A%84GPU%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95.html",
      "date"        : "2021-01-31 04:21:00 +0800",
      "description" : "介绍pytorch的C++版本的gpu使用的解决问题的过程记录",
      "content"     : "这里得吹逼下自己领导，10min解决了困扰我2天的问题（好吧，也许是我太蠢）。一. 问题描述由于项目需要使用libtorch（pytorch的C++版本）的GPU版本，但是发现无法使用GPU，因此将问题和解决过程记录下来，方便日后观看和反思。二. 解决问题的过程2.1 使用的torch版本这里需要说下pytorch和libtorch的版本一定要一致，且和cuda的版本一致。这里都是通过pytorch官网上进行安装即可。 pytorch1.6.0（GPU）：使用pip安装 # CUDA 10.1pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html pytorch1.6.0（CPU）：使用pip安装 # CPU onlypip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html libtorch1.6.0（GPU）：选择使用release版本即可（据说debug有问题） https://download.pytorch.org/libtorch/cu101/libtorch-win-shared-with-deps-1.6.0%2Bcu101.zip libtorch1.6.0（CPU）：选择使用release版本即可（据说debug有问题） https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-1.6.0%2Bcpu.zip 2.2 使用cmakelist的搭建工程cmake_minimum_required(VERSION 3.12 FATAL_ERROR)project(torch_gpu_test)# add CMAKE_PREFIX_PATHlist(APPEND CMAKE_PREFIX_PATH \"D:/software/opencv/opencv/build/x64/vc15/lib\")list(APPEND CMAKE_PREFIX_PATH \"D:/software/libtorch_gpu\")list(APPEND CUDA_TOOLKIT_ROOT_DIR \"D:/software/cuda/development\")find_package(Torch REQUIRED)find_package(OpenCV REQUIRED)if(NOT Torch_FOUND) message(FATAL_ERROR \"Pytorch Not Found!\")endif(NOT Torch_FOUND)message(STATUS \"Pytorch status:\")message(STATUS \" libraries: ${TORCH_LIBRARIES}\")message(STATUS \"Find Torch VERSION: ${Torch_VERSION}\")message(STATUS \"OpenCV library status:\")message(STATUS \" version: ${OpenCV_VERSION}\")message(STATUS \" libraries: ${OpenCV_LIBS}\")message(STATUS \" include path: ${OpenCV_INCLUDE_DIRS}\")add_executable(torch_gpu_test torch_gpu_test.cpp)target_link_libraries(torch_gpu_test ${TORCH_LIBRARIES} ${OpenCV_LIBS})set_property(TARGET torch_gpu_test PROPERTY CXX_STANDARD 11) 这里利用vs2019生成项目之后，编写以下代码进行测试：#include &lt;torch/torch.h&gt;#include &lt;torch/script.h&gt;using namespace torch;int main(){ torch::DeviceType device_type = at::kCPU; if (torch::cuda::is_available()) { cout &lt;&lt; \"cuda!\" &lt;&lt; endl; torch::DeviceType device_type = at::kCUDA; } else { cout &lt;&lt; \"cpu\" &lt;&lt; endl; } } 到了这里我开始了我的问题之旅：由于是Release版本，不能debug，只能主观的认为这里应该是cuda的环境没配好导致torch无法使用gpu的，因此一直在找cmake的cuda环境配置问题。2.3 Release with Debug改变了我的想法 这里得说下本人第一次知道release版本也可以debug！（本人也算一C++小白哈，别计较）这里顺带记录下如何使用vs2019的Release with debug的过程： 直接在项目中将Release版本选择为RelWithDebInfo 禁用代码优化功能：这里是防止出现“变量已被优化掉 因而不可用”这种问题在这里debug的时候发现，device这个我定义的变量是可以加载cuda的！因此可以推翻我之前想的（cuda环境的问题）。2.4 libtorch1.6GPU版本问题这里就可以肯定是libtorch的GPU问题了。为啥torch::cuda::is_available()会是false呢？ 网上的思路是：在“属性 –&gt; 链接器 –&gt; 命令行 –&gt; 其他选项”中添加： /INCLUDE:?warp_size@cuda@at@@YAHXZ 本人实验了下，按照网上的添加会报错，因此以下是本人实验可行的结果：在“链接器 –&gt; 输入 –&gt; 附加依赖项”中进行添加：D:\\software\\libtorch_gpu\\lib\\torch_cuda.libD:\\software\\libtorch_gpu\\lib\\torch_cpu.lib-INCLUDE:?warp_size@cuda@at@@YAHXZ 这里很奇怪，cmakelist明明已经配置好了libtorch的gpu，但是这里却没有torch_cuda.lib至此，问题解决了！"
    } ,
  
    {
      "title"       : "GCN在多标签分类中的应用",
      "category"    : "",
      "tags"        : "深度学习, 图算法",
      "url"         : "./GCN%E5%9C%A8%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html",
      "date"        : "2021-01-11 04:21:00 +0800",
      "description" : "介绍在图卷积网络在多标签分类任务中的应用",
      "content"     : "一. Torch的图神经网络库pyG torch_geometric 官方文档：https://pytorch-geometric.readthedocs.io/en/latest/index.html1.1 安装及使用这里参考官网的安装过程。 确定自己安装的pytorch版本：pip list进行查看，例如本人的torch版本为1.6.0+cu101（这里的cu101是指cuda10.1） 安装相关的第三方包，这里注意要匹配上面的torch版本，因此：${TORCH} = 1.6.0，${CUDA} = cu101 pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install torch-geometric 安装完成之后，测试下import torch_geometric导包没有报错说明安装完成了。 1.2 图数据导入torch_geometric.data这个模块包含了一个叫Data的类，而这个类可以很方便的构建属于自己的数据集。data实例有以下属性： x：节点的特征矩阵，shape = [节点个数，节点的特征数]。 edge_index：这里可以理解为图的邻接矩阵，但是要注意这里要将邻接矩阵转换成COO格式，shape = [2, 边的数量]，type = torch.long。 edge_attr：边的特征矩阵，shape = [边的个数，边的特征数] y：标签，如果任务是图分类，shape = [1, 图的标签数]；如果是节点分类，shape = [节点个数，节点的标签数]。（这里注意一哈：在torch中如果是多分类任务，不用转成onehot形式哦，因此标签数为1） is_directed()：是否是有向图(1) 下面是edge_index的具体从邻接矩阵生成COO模式的代码。from scipy.sparse import coo_matrix # 转化成COO格式coo_A = coo_matrix(adj_arr)edge_index = [coo_A.row, coo_A.col](2) 构建自己的数据集，只需要用list来封装这些Data即可。具体代码如下：dataset = [Data(x,edge_index,y) for _ in range(10)]1.3 图数据的转换及展示我们可以利用networkx来对Data这个图进行展示和转换成networkx的图结构。from torch_geometric.utils.convert import to_networkximport networkx as nxdef draw(Data): G = to_networkx(Data) nx.draw(G) nx.write_gexf(G, \"test.gexf\") plt.savefig(\"path.png\") plt.show()同时，还可以将gexf格式的图数据文件经过Gephi这个开源的图数据展示软件来进行节点的渲染。二. 图卷积网络GCN在多标签分类中的应用 论文参考：Semi-Supervised Classification with Graph Convolutional Networks2.1 GCN在模型应用上的优缺点。本次探究的是图卷积网络在图分类（多标签）上的应用，因此不涉及到节点的分类任务。GCN的优点：可以捕捉图的全局信息，很好的表征节点的特征，边的特征。GCN的缺点：若是新增节点，整个图发生变化， 那么GCN的结构就会发生变化。因此对于节点不固定的图结构来说，不适用。GCN的主要作用：抽取图中节点的拓扑信息（节点的邻接信息）。这里学到的是每个节点的一个唯一确定的embedding。如下图所示，多层的GCN抽取的是每个节点的唯一确定的embedding。GCN的特性： 局部参数共享，算子是适用于每个节点（圆圈代表算子），处处共享。 感受域正比于层数，最开始的时候，每个节点包含了直接邻居的信息，再计算第二层时就能把邻居的邻居的信息包含进来，这样参与运算的信息就更多更充分。层数越多，感受域就更广，参与运算的信息就更多。2.2 GCN在图分类的模型搭建图分类任务下的模型搭建过程如下：因此，利用pytorch_geometric来搭建图分类任务（多标签）的模型。这里代码中引入了两次图卷积和池化。在输入的数据中，除了包含节点的特征，还包含了边的特征。import torchimport torch.nn.functional as Ffrom torch_geometric.nn import GraphConv, TopKPoolingfrom torch_geometric.nn import global_mean_pool as gapfrom torch_geometric.nn import global_max_pool as gmpclass Net(torch.nn.Module): def __init__(self, num_features,multi_label): super(Net, self).__init__() self.conv1 = GraphConv(num_features, 8) # self.conv1.weight.data.normal_() self.pool1 = TopKPooling(8, ratio=0.5) self.conv2 = GraphConv(8, 8) self.pool2 = TopKPooling(8, ratio=0.5) self.lin1 = torch.nn.Linear(16, 64) self.lin2 = torch.nn.Linear(64, 128) self.lin3 = torch.nn.Linear(128, multi_label) def forward(self, data): x, edge_index,edge_attr, batch = data.x, data.edge_index, data.edge_attr,data.batch x = F.relu(self.conv1(x,edge_index,edge_attr)) x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, edge_attr, batch) x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1) x = F.relu(self.conv2(x, edge_index,edge_attr)) x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, edge_attr, batch) x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1) x = x1 + x2 x = F.relu(self.lin1(x)) x = F.dropout(x, p=0.3, training=self.training) x = F.relu(self.lin2(x)) x = F.dropout(x, p=0.3, training=self.training) x = torch.sigmoid(self.lin3(x)) return x2.3 多标签（Multi-Label）分类任务在多标签分类任务中： 输入的y：shape = [batch_size, multi-label的个数]，其中multi-label的形式都是[0,1,0,0,1….]，即每个类别之间都互不影响，且结果只有0和1。这里在torch_geometric.data.y的shape = [1,multi-label的个数]。 分类模型的最后一层激活函数：torch.sigmoid()函数（即二分类常用的激活函数）,这里对于多标签分类任务同样适用。 损失函数的定义： torch.nn.BCELoss() 准确率的定义：在训练的时候，一般除了看训练集和验证集的loss以外，acc其实也可以当作模型好坏的指标。但是对于多标签分类而言，这里和一般的多分类，二分类任务定义的准确率不太一样。个人的理解（可能不对蛤）：对于一个样本（多标签）而言，有且仅有每个标签都预测对了，这个样本才能算预测正确了，因此，定义了以下acc。pred = torch.where(pred&gt;acc_thread ,torch.ones_like(pred),torch.zeros_like(pred))acc = 0for i in range(pred.shape[0]): if pred[i].int().equal(data.y[i]): acc +=1 epoch_accuracy += acc"
    } ,
  
    {
      "title"       : "windows下搭建libtorch和paddle的C++环境搭建",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./windows-%E4%B8%8B%E6%90%AD%E5%BB%BAlibtorch%E5%92%8Cpaddle%E7%9A%84C++%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html",
      "date"        : "2020-12-26 04:21:00 +0800",
      "description" : "介绍在C++平台下搭建torch和paddle的环境",
      "content"     : "参考文章：NSTALLING C++ DISTRIBUTIONS OF PYTORCH，安装与编译 Windows 预测库，在C++中加载PYTORCH模型一. 必要软件 vs2019：paddle和torch这里的编译都是由Visual Studio 2019完成的 libtorch：直接在官网上进行下载压缩包，这里说明下分为release和debug版本，直接下载release版本即可。 paddle：这里选择2.0-rc1的cpu版本的直接进行解压安装。 opencv：windows下直接安装exe到本地即可。 cmake：直接用scoop安装scoop install cmake二. 安装libtorch环境2.1 构建一个C++项目目录层级如下：├─example-app ├─build // 新建一个空目录 ├─CMakeLists.txt // 构建一个cmakelist └─example-app.cpp // 构建一个cpp文件用于测试其中，CMakeList.txt具体设置如下：cmake_minimum_required(VERSION 3.12 FATAL_ERROR)project(example-app)# add CMAKE_PREFIX_PATH#增加opencv和libtorch的路径list(APPEND CMAKE_PREFIX_PATH \"D:/software/opencv/opencv/build/x64/vc15/lib\") # 注意这里如果是vs2015的版本，需要改成 /build/x64/vc14/liblist(APPEND CMAKE_PREFIX_PATH \"D:/software/libtorch\")find_package(Torch REQUIRED)find_package(OpenCV REQUIRED)if(NOT Torch_FOUND) message(FATAL_ERROR \"Pytorch Not Found!\")endif(NOT Torch_FOUND)message(STATUS \"Pytorch status:\")message(STATUS \" libraries: ${TORCH_LIBRARIES}\")message(STATUS \"OpenCV library status:\")message(STATUS \" version: ${OpenCV_VERSION}\")message(STATUS \" libraries: ${OpenCV_LIBS}\")message(STATUS \" include path: ${OpenCV_INCLUDE_DIRS}\")add_executable(example-app example-app.cpp)target_link_libraries(example-app ${TORCH_LIBRARIES} ${OpenCV_LIBS})set_property(TARGET example-app PROPERTY CXX_STANDARD 11)C++测试代码（example-app.cpp）如下（测试opencv和libtorch）：#include &lt;torch/torch.h&gt;#include &lt;iostream&gt;#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;using namespace std;using namespace cv;int main() { torch::Tensor tensor = torch::rand({2, 3}); std::cout &lt;&lt; tensor &lt;&lt; std::endl; std::cout &lt;&lt; \"ok!\" &lt;&lt; std::endl; Mat img = imread(\"1.jpg\"); imshow(\"1\",img); waitKey(0); return 0;}2.2 编译和生成项目 进入到build目录：cd build 利用cmake进行编译： cmake .. 编译顺利的话，就可以看到build目录下生成了如下所示： 利用vs2019打开项目example-app.sln 点击example-app 右键选择设为启动项，并且将版本选择release版本，点击本地Windows调试器2.3 调试问题的解决 报错信息：由于找不到c10.dll，torch.dll这种找不到dll文件的，直接将dll文件(这些dll文件都在libtorch/lib路径下)复制到build/release文件夹下 opencv_world3411.dll和opencv_ffmpeg3411_64.dll等都在opencv的opencv\\opencv\\build\\x64\\vc15\\lib路径下。 这里注意测试opencv的时候，需要将图片放置到和example-app.vcxproj同级目录下2.4 exe生成文件的平台移植 如果需要将生成的exe文件移植到其他PC上面，只需要将release文件夹下所有文件（包括dll文件和exe文件）复制到其他PC即可。 生成的exe文件在找图片的时候也是同级目录下找，因此需要将图片放置到exe文件的同级目录下。2.5 pytorch模型在C++平台的使用PyTorch模型从Python到C++的转换由Torch Script实现。Torch Script是PyTorch模型的一种表示，可由Torch Script编译器理解，编译和序列化。一般利用trace将PyTorch模型转换为Torch脚本,必须将模型的实例以及样本输入传递给torch.jit.trace函数。这将生成一个 torch.jit.ScriptModule对象，并在模块的forward方法中嵌入模型评估的跟踪。三. 安装paddle的C++环境3.1 下载安装paddle这里官网有2种方式在windows上安装paddle环境：一个是通过git下载paddle源码进行编译安装，另一种直接从官网下载zip编译好的文件（本文使用该种方式）。3.2 结合paddleOCR测试并使用paddle预测库 paddleOCR的git地址：https://github.com/PaddlePaddle/PaddleOCR 下载到本地之后，cd PaddleOCR\\deploy\\cpp_infer，修改CMakeList.txt文件SET(PADDLE_LIB \"D:/software/paddle_inference_install_dir\") # 这里是下载的paddle预测库的路径SET(OPENCV_DIR \"D:/software/opencv/opencv\") # 这里是下载的opencv的路径find_package(OpenCV REQUIRED) 新建一个build文件夹：mkdir build 进入build：cd build ， 编译：cmake .. 同样的利用vs2019打开项目ocr_system.sln，生成即可。 这里注意需要将paddle_fluid.dll放入到Release目录下。"
    } ,
  
    {
      "title"       : "C++的包管理工具——VCPKG",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./C++%E7%9A%84%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7-VCPKG.html",
      "date"        : "2020-12-19 04:21:00 +0800",
      "description" : "介绍如何利用vcpkg来管理C++的库",
      "content"     : "在我最初学C/C++的时候，一直为要下各种第三方库而烦恼，为啥C++没有像python一样简单的包管理工具呢？于是，它来了——VCPKG。一. windows 下安装和配置VCPKG的官方git地址：https://github.com/microsoft/vcpkg1.1 软件及平台要求 windows7及其以上（本人的是Windows10） git VS2015及其以上（本人的是VS2019）1.2 windows下安装 git克隆下官方的git仓库：git clone https://github.com/microsoft/vcpkg 进入到仓库中：cd vcpkg，注意下这里官方建议把vcpkg目录放到C:\\src\\下。 安装vcpkg：boostrap-vcpkg.bat1.3 安装库的示例 比如需要安装opencv，可以先搜索下vcpkg是否支持：vcpkg.exe search opencv 进行库的编译安装：vcpkg.exe install opencv 需要在visual studio中直接使用opencv：vcpkg.exe integrate install二. 环境变量配置这里需要注意两点： 把vcpkg的路径添加到环境变量中：path = C:\\src\\vcpkg ，这样就可以随时随地使用vcpkg.exe咯，而不用每次到C:\\src\\vcpkg下执行命令。 vcpkg默认是安装32位的库，我是需要安装x64的库，因此需要添加一个系统变量：VCPKG_DEFAULT_TRIPLET=x64-windows三. VCPKG下载过慢解决方案3.1 先下载后编译vcpkg直接执行命令vcpkg.exe install opencv 的时候，会先下载需要的第三方包（下一个编译一个），那么有时候网速不好的时候，就会下不了，从而断掉，因此可以先把所有需要的库全部下载下来，再进行编译。 下载包，并编译：vcpkg.exe install opencv --only-downloads 对下载好的包继续编译：vcpkg.exe install opencv3.2 直接手动下载不好下载的包在VCPKG下载包的时候，总是会碰到下载突然卡住，其中一个包下载不下来的情况，直接手动去下载(如下图框出的链接地址)下来，然后放在C:\\src\\vcpkg\\downloads 下面，重新再次执行下载命令即可。"
    } ,
  
    {
      "title"       : "利用Jasper2主题和Netlify完善个人博客",
      "category"    : "",
      "tags"        : "博客",
      "url"         : "./Jasper2%E5%8D%9A%E5%AE%A2%E8%B7%9F%E6%96%B0.html",
      "date"        : "2020-11-27 04:21:00 +0800",
      "description" : "介绍如何利用Jasper2主题（jekyll）完成个人博客设置和利用netlify加速访问速度",
      "content"     : "一. 更换Jekyll主题 之前用的是jekyll主题：Flexible-Jekyll，如下图所示： 觉得有点过于单调和简单了，于是找到了命中注定：Jasper2,如下图所示： 如果也想使用这个Jasper2主题的话，最好多读读其作者的readme哦！1.1 在git仓库中推送生成好的html文件​ 克隆Jasper2项目（master分支）到自己本地，然后利用jekyll开启本地预览模式：bundle exec jekyll server，这里你会发现在本地的上级目录下生成一个jasper2-pages文件夹。 如果是和我一样是利用username.github.io建立的仓库，那么就直接在项目中新建一个_site文件夹，并将jasper2-pages内容复制到该目录下即可。 如果是利用github pages来展示自己的项目的话，那就建立一个gh-pages 分支，同时将jasper2-pages内容复制到该目录下即可。注意：每次跟新仓库代码后，都需要重新生成一次，并替换掉_site文件夹内容！1.2 内容替换 _config.yml：用来修改主页上的一些个人信息 about/index.md：修改关于的个人说明 _post：将其中的文章替换掉自己文章1.3 前端页面的修改 ps:本人也是个前端新手，也是根据李小肥的指导才知道怎么修改前端的，给你们撒狗粮，哈哈哈！这里就不讲修改细节部分了，主要讲述如何快速找到想要修改的代码，毕竟授人以鱼不如授人以渔嘛！ 进入到jasper2的展示主页，右键页面中的facebook图标点击检查,就可以看到以下界面： 在这个check界面上可以看到有个social-link social-link-fb类，那么如果我们想要修改这些内容，可以直接在项目代码中全文搜索，就可以在author.html中找到咯，但是这和我们想修改图标或者是内容不符啊，那么就往这个类的上面继续找site-nav-right,这里就指向了我们想要修改的图标。 如果还想修改css文件呢，可以看到上图中Styles下面中就是，还可以直接在这里修改，就可以同时在网页中预览哦！如何找这个css文件呢，旁边的screen.css:279就是，而且连行号都给你了！二. Netlify加速访问目前有一些提供域名解析、CDN加速的免费网站，其还可以在GitHub中挂载触发器，一旦发现GitHub Pages仓库变化了，立即同步编译发布，减少人工操作。而Netlify就是一个，当然还有Vecel这个。 经过本人的亲自实验，Jasper2这个无法在Vecel应用上（但是之前的flexible主题可以哦），毕竟Jasper2的作者都推荐你用Netlify，你还不用么！2.1 拥有一个专属于自己的域名如果你已经有自己的域名了，或者你不想要啥域名，直接用netlify提供的或者gitpages就挺好的，那就跳过这段吧！ 去阿某云买一个新鲜热乎的域名，有好多后缀可以选的，反正我是选了个便宜的，当然还有比我这个.website还low的.xyz和.top，反正看个人喜好和收入吧，毕竟是要花钱的。 注意这里一定要实名注册，买完域名才能使用！2.2 利用netlify进行加速 注册：这里无论是vercel还是netlify都是可以直接关联github账户的，因此直接用你的github账户进行注册即可。 添加github仓库：这里添加你的username.github.io，然后等待他发布即可。 添加自己的域名，然后按照要求，去你买域名的网站设置CNAME进行关联即可。三. 搜索引擎的个人站点入口除了能够不翻墙被人访问自己的博客，但是别人用搜索引擎搜不到你咋办呢？这就需要让各个搜索引擎收录自己的站点咯。3.1 搜索引擎提交个人站点的入口 百度 ： https://ziyuan.baidu.com/site/index 谷歌 ： https://www.google.com/webmasters/tools/home?hl=en 搜狗 ： http://zhanzhang.sogou.com/index.php/dashboard/index 360　： http://info.so.360.cn/site_submit.html Bing ： https://www.bing.com/toolbox/webmaster/3.2 验证站点的所有权这里几乎每个搜索引擎都会要你验证下自己是不是提交的站点的所有者，因此需要你验证下身份。我这里是通过添加主页的tag来进行验证的（可以添加好几个搜索引擎的哦）&lt;head&gt; &lt;meta name=\"baidu-site-verification\" content=\"自己的百度验证码\" /&gt; &lt;meta name=\"sogou_site_verification\" content=\"自己的搜狗验证码\"/&gt;&lt;/head&gt;"
    } ,
  
    {
      "title"       : "Keras训练模型部署到C++环境并生成DLL",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Keras%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0C++%E7%8E%AF%E5%A2%83%E7%94%9F%E6%88%90DLL.html",
      "date"        : "2020-11-03 22:21:00 +0800",
      "description" : "主要讲述将Keras训练好的.h5模型转换成.pt格式，并将其转换后的模型部署到C++环境中，并生成DLL",
      "content"     : "一. 准备工作 在Windows上搭建完成Tensorflow的C++环境，这里参考本人的上一篇博客 在windows上部署opencv3的C++环境 python3的keras和tf2.X的环境：pip3 install keras==2.4.3和pip3 install tensorflow==2.3.1 python3的opencv环境：pip3 install opencv-python 二. python模型训练 python模型构建：Kaggle猫狗分类 python完整代码放在本人的git仓库上面2.1 模型训练，保存.h5格式的模型文件在keras的model.save()模型保存的函数中，只支持2种保存方式： .h5格式的文件进行保存模型整个结构及其权重。 以文件夹（包含assets saved_model.pb variables）来保存，模型架构和训练配置（包括优化器、损失和指标）存储在 saved_model.pb 中。权重保存在 variables/ 目录下。 因此使用.h5格式来存储模型结构及权重。2.2 h5文件转pt文件将训练好的模型以.h5格式进行存储，再通过转成.pt格式的模型文件，提供C++的tensorflow调用模型。from keras.models import load_modelimport tensorflow as tfimport tensorflow.python.keras.backend as Kfrom tensorflow.python.framework import graph_io# 针对tf2.x来说不支持freezegraph的，这里需要使用tf1的方式tf.compat.v1.disable_eager_execution()def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True): from tensorflow.python.framework.graph_util import convert_variables_to_constants graph = session.graph with graph.as_default(): freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or [])) output_names = output_names or [] output_names += [v.op.name for v in tf.compat.v1.global_variables()] input_graph_def = graph.as_graph_def() if clear_devices: for node in input_graph_def.node: node.device = \"\" frozen_graph = convert_variables_to_constants(session, input_graph_def, output_names, freeze_var_names) return frozen_graph\"\"\"----------------------------------配置路径-----------------------------------\"\"\"h5_model_path = 'model/model.h5'pb_model_name = 'model.pt'output_path = '.'\"\"\"----------------------------------导入keras模型------------------------------\"\"\"K.set_learning_phase(0)net_model = load_model(h5_model_path)print('input is :', net_model.input.name)print('output is:', net_model.output.name)\"\"\"----------------------------------保存为.pb格式------------------------------\"\"\"sess = K.get_session()frozen_graph = freeze_session(K.get_session(),output_names=[out.op.name for out in net_model.outputs])graph_io.write_graph(frozen_graph, output_path, pb_model_name, as_text=False)2.3 测试pt模型文件对同一个图片分别利用.h5模型文件和.pt模型文件进行预测。.pt格式的模型预测代码如下：def pred_with_pt(img, pb_file_path): with tf.Graph().as_default(): output_graph_def = tf.compat.v1.GraphDef() # 打开.pb模型 with open(pb_file_path, \"rb\") as f: output_graph_def.ParseFromString(f.read()) tensors = tf.import_graph_def(output_graph_def, name=\"\") print(\"tensors:\", tensors) # 在一个session中去run一个前向 with tf.compat.v1.Session() as sess: init = tf.compat.v1.global_variables_initializer() sess.run(init) op = sess.graph.get_operations() input_x = sess.graph.get_tensor_by_name(\"conv2d_input:0\") # 具体名称看上一段代码的input.name print(\"input_X:\", input_x) out_softmax = sess.graph.get_tensor_by_name(\"dense/Softmax:0\") # 具体名称看上一段代码的output.name print(\"Output:\", out_softmax) img_out_softmax = sess.run(out_softmax, feed_dict={input_x: img}) return img_out_softmax三. C++ 模型预测3.1 将模型预测类进行封装并生成动态链接库DLL VS2019部署（参考本人的上一篇博客）：新建一个动态链接库(DLL)，新建一个头文件tf_clf.h和源文件tf_clf.cpp，这里是生成DLL包（点击生成 ==&gt; 重新生成DLLTF） 这里要把tensorflow_cc.dll放到生成的x64/release里面 这里还需要在Release属性页里配置C/C++的预处理器(防止后面编译时出现这种错误tstring.h(350,40): error C2589: “(”:“::”)： _XKEYCHECK_HNOMINMAX 头文件tf_clf.h中声明类的导出 class __declspec(dllexport) TFClf;class TFClf {private:vector&lt;float&gt; mean = { 103.939,116.779,123.68 };int resize_col = 224;int resize_row = 224;string input_tensor_name = \"conv2d_input\";string output_tensor_name = \"dense/Softmax\";Point draw_point = Point(50, 50);public:string image_path, model_path;TFClf(string img, string model) :image_path(img), model_path(model) {}void mat_to_tensor(Mat img, Tensor* output_tensor);Mat preprocess_img(Mat img);void model_pred();void show_result_pic(Mat img, int output_class_id, double output_prob);}; 源文件tf_clf.cpp完成类的具体实现，注意这里要#include \"pch.h\"3.2 新建一个项目测试DLL 新建一个控制台的空项目，并将打包好的DllTF.dll和DllTF.lib复制到工程中 配置属性管理器：这里需要在Release | x64添加之前配置好的opencv_release.props和tf_release.props。 这里要把tensorflow_cc.dll放到生成的x64/release里面 新建一个头文件tf_clf.h #pragma once#ifndef TF_CLF_H#endif // !TF_CLF_H#pragma comment(lib,\"DllTF.lib\")class __declspec(dllexport) TFClf;class TFClf {private: vector&lt;float&gt; mean = { 103.939,116.779,123.68 }; int resize_col = 224; int resize_row = 224; string input_tensor_name = \"conv2d_input\"; string output_tensor_name = \"dense/Softmax\"; Point draw_point = Point(50, 50);public: string image_path, model_path; TFClf(string img, string model) :image_path(img), model_path(model) {} void mat_to_tensor(Mat img, Tensor* output_tensor); Mat preprocess_img(Mat img); void model_pred(); void show_result_pic(Mat img, int output_class_id, double output_prob);}; 新建一个源文件main.cpp# include \"tf_clf.h\"int main() {string model_path = \"D:/yeyan/pycharm_project/dogcat/model/model.pt\";string img_path = \"D:/yeyan/pycharm_project/dogcat/data/test_set/test_set/cats/cat.4001.jpg\";TFClf clf = TFClf(img_path, model_path);clf.model_pred();}"
    } ,
  
    {
      "title"       : "Window下搭建Tensorflow的C++环境",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Window%E4%B8%8B%E6%90%AD%E5%BB%BATensorflow%E7%9A%84C++%E7%8E%AF%E5%A2%83.html",
      "date"        : "2020-11-02 03:21:00 +0800",
      "description" : "介绍如何在windows下搭建tensorflow的C++环境",
      "content"     : "参考Tensorflow官网安装文章：https://www.tensorflow.org/install/source_windows?hl=zh-cn一. 下载需要的软件 bazel：Google 的一款可再生的代码构建工具，类似于Cmake。使用scoop进行安装：scoop install bazel python3.7：这里最好用pip 安装下必要的第三方包，比如tensorflow,kears,numpy等。 下载官方源码：git clone https://github.com/tensorflow/tensorflow.git二. 进行bazel源码编译2.1 配置build cd到源码目录：cd tensorflow-master 通过在 TensorFlow 源代码树的根目录下运行以下命令来配置系统构建：python3 ./configure.py 这里选择的是cpu版本的，每个配置的选择如下：You have bazel 3.7.0 installed.Please specify the location of python. [Default is C:\\soft\\python3.7.9\\python3.exe]:Found possible Python library paths: C:\\soft\\python3.7.9\\lib\\site-packagesPlease input the desired Python library path to use. Default is [C:\\soft\\python3.7.9\\lib\\site-packages]Do you wish to build TensorFlow with ROCm support? [y/N]: nDo you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: yEigen strong inline overridden.Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Preconfigured Bazel build configs. You can use any of the below by adding \"--config=&lt;&gt;\" to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=mkl_aarch64 # Build with oneDNN support for Aarch64. --config=monolithic # Config for mostly static monolithic build. --config=numa # Build with NUMA support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects. --config=v2 # Build TensorFlow 2.x instead of 1.x.Preconfigured Bazel build configs to DISABLE default on features: --config=noaws # Disable AWS S3 filesystem support. --config=nogcp # Disable GCP support. --config=nohdfs # Disable HDFS support. --config=nonccl # Disable NVIDIA NCCL support.2.2 bazel编译 修改bazel中间文件存储的路径（磁盘可用空间 Release 版本 &gt;= 16G , Debug版本 &gt;= 40G 编译的中间文件默认会放到 C:\\用户\\你的账号名\\ _bazel_你的账号名 下. C 盘可能没有那么大的空间, 所以要改一下输出文件的路径），打开tensorflow文件夹，vim .bazelrc，在最后一行加上startup --output_user_root=D:/tf，如果不修改路径，可能会编译到一半就卡死。 bazel编译动态链接库命令（这里加上使用的最大内存）： bazel build --config=opt //tensorflow:tensorflow_cc.dll --local_ram_resources=1024 编译的过程可能会很长，千万不要以为有问题就Ctrl C了（分2个过程：下中间资源+编译），编译完成后会出现 Build completed successfully 编译好的库文件在tensorflow-master\\bazel-bin\\tensorflow目录下，分别是tensorflow_cc.dll和tensorflow_cc.dll.if.lib。 bazel编译头文件命令： bazel build --config=opt //tensorflow:install_headers --local_ram_resources=1024 编译好的头文件在tensorflow-master\\bazel-bin\\tensorflow\\include目录下。三. 新建项目测试 注意： ​ 1. 这里编译的是tensorflow的release版本，因此构建项目的时候把环境从debug变成release ​ 2. 在新建项目属性表（这里无论是opencv还是tensorflow）中，要选择release版本的x64（64位） 新建一个项目 在项目中新建一个文件夹存放之前编译好的头文件，库文件，具体结构如下所示├── tf_test// 整个项目 ├── x64 // 这里是生成解决方案得到的 ├── tf // 这里存放所有编译好的文件 ├──bin // 存放dll动态库文件 ├──tensorflow_cc.dll ├──lib // 存放静态库文件 ├──tensorflow_cc.lib ├──include // 直接是tensorflow编译好的include目录 ├──main.cpp 属性管理器 —— Release X64 —— 添加新项目属性表（如果代码中还需要添加opencv库的可以参考本人另一篇博客） VC++目录中的包含目录中添加：D:tf_test\\tf\\include VC++目录中的库目录中添加：D:tf_test\\tf\\lib 链接器——输入——附加依赖项中添加：tensorflow_cc.lib 选择项目为release和x64平台。 使用以下代码进行测试 #include &lt;iostream&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/opencv.hpp&gt;#include\"tensorflow/core/public/session.h\"#include\"tensorflow/core/platform/env.h\"using namespace std;using namespace tensorflow;using namespace cv;int main(){ const string model_path = \"D:\\\\code\\\\yinbao_face\\\\live.pb\"; const string image_path = \"0.jpg\"; Mat img = imread(image_path); cvtColor(img, img, COLOR_BGR2RGB); resize(img, img, Size(112, 112), 0, 0, INTER_NEAREST); int height = img.rows; int width = img.cols; int depth = img.channels(); // 图像预处理 img = (img - 0) / 255.0; // img.convertTo(img, CV_32FC3, 1.0 / 255, 0); // 取图像数据，赋给tensorflow支持的Tensor变量中 const float* source_data = (float*)img.data; Tensor input_tensor(DT_FLOAT, TensorShape({ 1, height, width, 3 })); auto input_tensor_mapped = input_tensor.tensor&lt;float, 4&gt;(); for (int i = 0; i &lt; height; i++) { const float* source_row = source_data + (i * width * depth); for (int j = 0; j &lt; width; j++) { const float* source_pixel = source_row + (j * depth); for (int c = 0; c &lt; depth; c++) { const float* source_value = source_pixel + c; input_tensor_mapped(0, i, j, c) = *source_value; //printf(\"%d\"); } } } Session* session; Status status = NewSession(SessionOptions(), &amp;session); if (!status.ok()) { cerr &lt;&lt; status.ToString() &lt;&lt; endl; return -1; } else { cout &lt;&lt; \"Session created successfully\" &lt;&lt; endl; } GraphDef graph_def; Status status_load = ReadBinaryProto(Env::Default(), model_path, &amp;graph_def); if (!status_load.ok()) { cerr &lt;&lt; status_load.ToString() &lt;&lt; endl; return -1; } else { cout &lt;&lt; \"Load graph protobuf successfully\" &lt;&lt; endl; } // 将graph加载到session Status status_create = session-&gt;Create(graph_def); if (!status_create.ok()) { cerr &lt;&lt; status_create.ToString() &lt;&lt; endl; return -1; } else { cout &lt;&lt; \"Add graph to session successfully\" &lt;&lt; endl; } cout &lt;&lt; input_tensor.DebugString() &lt;&lt; endl; //打印输入 vector&lt;pair&lt;string, Tensor&gt;&gt; inputs = { { \"input_1:0\", input_tensor }, //input_1:0为输入节点名 }; // 输出outputs vector&lt;Tensor&gt; outputs; vector&lt;string&gt; output_nodes; output_nodes.push_back(\"output_1:0\"); //输出有多个节点的话就继续push_back double start = clock(); // 运行会话，最终结果保存在outputs中 Status status_run = session-&gt;Run({ inputs }, { output_nodes }, {}, &amp;outputs); Tensor boxes = move(outputs.at(0)); cout &lt;&lt; boxes.DebugString() &lt;&lt; endl; //打印输出 double end = clock(); cout &lt;&lt; \"time = \" &lt;&lt; (end - start) &lt;&lt; \"\\n\"; if (!status_run.ok()) { cerr &lt;&lt; status_run.ToString() &lt;&lt; endl; return -1; } else { //cout &lt;&lt; \"Run session successfully\" &lt;&lt; endl; }}四. 测试中出现的问题4.1 生成解决方案的时候报错无法打开包括文件：解决方式：在本地的通过python pip安装后的tensorflow文件夹中（C:\\soft\\python3.7.9\\Lib\\site-packages\\tensorflow\\include）将google文件夹复制到D:tf_test\\tf\\include下面，即可解决4.2 生成解决方案的时候报错Link1120:解决方式：将vs2019上报错信息复制，cd到tensorflow-master\\tensorflow\\tools\\def_file_filter(这里的tensorflow-master是自己下载tensorflow源码的地方），编辑def_file_filter.py.tpl文件：# Header for the def file. (找到这一行代码) if args.target: def_fp.write(\"LIBRARY \" + args.target + \"\\n\") def_fp.write(\"EXPORTS\\n\") def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\") # 下面两个就是复制的错误信息 def_fp.write(\"\\t ?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z\\n\") def_fp.write(\"\\t ??0SessionOptions@tensorflow@@QEAA@XZ\\n\")重新编译DLL，头文件（虽然很麻烦，但是还是得做啊）4.3 有太多的错误导致IntelliSense引擎无法正常工作,其中有些错误无法在编辑器解决方式：在项目-&gt;属性-&gt;配置属性-&gt;C/C++-&gt;预处理器-&gt;预处理器定义中加入_XKEYCHECK_H就消失了4.4 找不到tensorflow_cc.dll文件解决方式：将tensorflow_cc.dll文件复制到x64/release文件夹下。"
    } ,
  
    {
      "title"       : "pytorch使用YOLOv5训练数据",
      "category"    : "",
      "tags"        : "深度学习, CV",
      "url"         : "./pytorch%E4%BD%BF%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE.html",
      "date"        : "2020-10-27 03:21:00 +0800",
      "description" : "介绍如何利用官方开源的Yolov5训练一个属于自己的模型",
      "content"     : "一. 需要下载的资源 Fork 下官方的开源项目：https://github.com/ultralytics/yolov5 git clone 下Fork之后的项目到自己本地仓库中。 采用的训练集（简单的，仅有一个类）：源自Kaggle的小麦数据集 如果有gpu的话，最好安装cuda进行训练加速，这里可以参考本人的另一篇文章：Windows10环境下搭建CUDA10.1和pytorch1.6二 . 构建属于自己的目标检测模型2.1 在官网的开源yolo5项目的基础上进行构建 git 克隆到本地仓库：git clone https://github.com/xx/yolov5.git 进入项目中，并安装需要的第三方依赖：pip install -r requirements.txt 新建一个原始数据的目录：：mkdir ori_data，将下载好的小麦数据集解压后放到项目。 创建输出一个文件输出目录：mkdir wheat_data，并在此目录下新建以下目录，如下图所示 ├─images│ ├─train│ └─val└─labels ├─train └─val 构建数据集，新建一个munge_data.py文件import osimport pandas as pdimport numpy as npimport ast from sklearn.model_selection import train_test_splitfrom tqdm import tqdmimport shutilDATA_PATH = 'ori_data'OUTPUT_PATH = 'wheat_data'def process_data(data,data_type = 'train'): for _,row in tqdm(data.iterrows(),total=len(data)): image_name = row['image_id'] bounding_boxes = row['bbox'] yolo_data = [] for bbox in bounding_boxes: x = bbox[0] y = bbox[1] w = bbox[2] h = bbox[3] x_center = x+w/2 y_center = y+h/2 # 这里需要将图像数据归一化处理（yolo需要的输入为归一化后的数据,且为浮点数） x_center /= 1024.0 y_center /= 1024.0 w /= 1024.0 h /= 1024.0 yolo_data.append([0,x_center,y_center,w,h]) yolo_data = np.array(yolo_data) # 保存bbox的图片信息 np.savetxt( os.path.join(OUTPUT_PATH,f'labels/{data_type}/{image_name}.txt'), yolo_data, fmt=['%d','%f','%f','%f','%f'] ) # 将目标图片文件保存到指定文件中 shutil.copyfile( os.path.join(DATA_PATH,f'train/{image_name}.jpg'), os.path.join(OUTPUT_PATH,f'images/{data_type}/{image_name}.jpg'), )if __name__ == \"__main__\": df = pd.read_csv(os.path.join(DATA_PATH,'train.csv')) # 将string of list 转成list数据 df.bbox = df.bbox.apply(ast.literal_eval) # 利用groupby 将同一个image_id的数据进行聚合，方式为list进行，并且用reset_index直接转变成dataframe df = df.groupby(['image_id'])['bbox'].apply(list).reset_index(name = 'bbox') # 划分数据集 df_train,df_val = train_test_split(df,test_size=0.2,random_state=42,shuffle=True) # 重设 index （这里数据被打乱，index改变混乱） df_train = df_train.reset_index(drop=True) df_val = df_val.reset_index(drop=True) process_data(df_train,data_type='train') process_data(df_val,data_type='val') 运行构建数据的py文件：python munge_data.py 这里可以看到在输出结果目录中，放入了需要的整理后的数据集 新建一个wheat.yamlyaml文件，指定模型训练时候的输入及其类别（注意这里冒号后面要加空格，yamal格式问题） train: wheat_data/images/train // 指定训练目录val: wheat_data/images/val // 指定验证目录nc: 1 // 指定类别names: [\"wheat\"] // 指定类别名字 进行模型训练：python3 train.py --img 1024 --batch 8 --epoch 100 --data wheat.yaml --cfg .\\models\\yolov5s.yaml --name wm 这里可能会报错（Dataloader中设置了多进程导致的），报错信息如下所示：File \"C:\\soft\\python3.7.9\\lib\\multiprocessing\\reduction.py\", line 60, in dump ForkingPickler(file, protocol).dump(obj) BrokenPipeError: [Errno 32] Broken pipe这里可以参考文章：https://github.com/pytorch/pytorch/issues/2341。解决方案：将utils\\datasets.py文件中num_workers改成0即可（代码第68行）。训练完成后如下图： 可以查看本地tensorboard训练过程：tensorboard --logdir=runs，如果这里出现问题，可以换成一下命令：python -m tensorboard.main --logdir logs 这里还可以使用coco数据集的预训练模型进行训练，可能效果会更好python3 train.py --img 1024 --batch 8 --epoch 100 --data wheat.yaml --cfg .\\models\\yolov5s.yaml --name wm --weights 将训练好的模型放到当前文件夹下：cp runs/exp0_wm/weights/best.pt . 选择测试图片的文件夹进行生成测试：python detect.py --source ./test_data --weights best.pt ，这里可以看到新生成一个文件夹inference/output中就是测试后标记bbox后的图片。"
    } ,
  
    {
      "title"       : "Windows10环境下搭建CUDA10.1和pytorch1.6",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%AD%E5%BB%BACUDA10.1%E5%92%8Cpytorch1.6.html",
      "date"        : "2020-10-20 18:18:00 +0800",
      "description" : "介绍如何在Windows10环境中搭建GPU使用环境和pytorch",
      "content"     : "一. 安装CUDA10.1 这里需要安装鲁大师，查看自己电脑的显卡型号，这里是gtx 1060 ，6g1.1 安装Nvidia驱动 首先在Nvidia官网上安装显卡驱动，连接地址：https://www.nvidia.com/Download/index.aspx 这里需要去nVidia官网 https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html 查看cuda和显卡驱动对应表上cuda10.1对应驱动的版本号。 查看自己安装驱动的版本号：NVIDIA控制面板-帮助-系统信息中查看，这里可以看到我的驱动的版本号为456，这里是大于官网安装cuda10.1需要的418的驱动的。1.2 安装CUDA10.1 安装CUDA：适合哪个版本的CUDA，就可以去官网下载对应的CUDA了，但是官网首页的CUDA一般是最新版，我们可能需要下载旧版本比如CUDA10.1，可以cuu点击下面连接进行下载：https://developer.nvidia.com/cuda-toolkit-archiv，这里直接选择CUDA Toolkit 10.1 (Feb 2019)即可。这里一般选择自定义安装，可以通过命令行查看是否安装成功：nvcc -V，这里如果提升没有整个命令，说明还没有将cuda的路径添加到环境变量中，需要设置环境变量，添加CUDA安装目录下的bin和libnvvp目录，如下所示： 安装cuDNN：选择对应的版本号和系统，这里注意官网最前面的几个连接中都是windows10 的x86（32位）的，这里需要选择老一点的cuDNN的版本，如下所示 下载后，将压缩包解压得到cuda文件夹，文件夹下有三个文件夹，复制这三个文件夹到CUDA安装的目录’D:\\soft\\cuda’下，会自动将cudnn的三个文件夹的文件合并到其三个同名文件夹bin、include和lib中。 查看自己电脑中Nvidia的GPU信息：nvidia-smi二. 安装pytorch pytorch官网安装地址：https://pytorch.org/get-started/locally/2.1 修改pip源，提升下载包速度如果本地pip下载很慢，修改pip源：Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)内容如下：[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.comwindows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini。内容同上。2.2 通过pip安装pytorch这里可以直接访问pytorch官网来选择适合自己的版本：pip安装：pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html2.3 查看是否安装成功，并查看能否驱动cudaimport torchprint(torch.cuda.is_available())"
    } ,
  
    {
      "title"       : "jekyll和github pages搭建个人博客",
      "category"    : "",
      "tags"        : "博客",
      "url"         : "./Mac%E4%B8%8B%E5%88%A9%E7%94%A8jekyll%E5%92%8Cgithub-pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html",
      "date"        : "2020-10-18 03:21:00 +0800",
      "description" : "在Windows环境中利用jekyll来本地测试jekyll主题，并结合github pages来搭建个人的博客",
      "content"     : "一. 搭建环境1.1 下载软件 jekyll：这个是将纯文本转化为静态网站和博客，使用gem安装下载：gem install bundler jekyll。 github pages：免费开源，并且可以自动生成域名，自己去构建一个属于自己的github账号和新建一个仓库（名字为：XX.github.io，这里XX就是你自己的账号名称） mac或者linux或者Windows平台。1.2 选择一个适合自己的博客主题 jekyll主题：http://jekyllthemes.org/ jekyll插件：http://www.jekyll-plugins.com/ 本人选择的jekyll主题：Flexible-Jekyll，如下图所示： 选择一个地方存放下载的主题（直接git下载）：git clone https://github.com/artemsheludko/flexible-jekyll 这里直接在本地利用jekyll生成一个网页进行测试和调试：bundle exec jekyll server或者是bundle exec jekyll s，这里会生成一个本地的博客地址：http://127.0.0.1:4000/，可以直接查看1.3 将主题放入自己的仓库中 git下载创建好的xx.github.io该仓库到自己本地：git clone https://github.com/xx/xx.github.io.git 将之前下载的主题放到自己创建的github.io这个仓库中 git上传修改的地方： git添加修改的地方：git add . git提交：git commit -m \"修改\" git推送：git push 这里可以直接在网页上打开https://xx.github.io/查看自己的博客，当然这里还是别人的内容，还需要自己修改成属于自己专属的博客。 这里注意：如果无法访问github pages，这里需要修改dns服务器就可以了，修改为114.114.114.114，具体如下所示：二. 创建一个专属于自己的个人博客2.1 了解下载主题的文件和文件夹的作用和内容整个下载jekyll主题的代码结构如下图所示： _drafts文件夹：主要是存放一些自己还没有写好的markdown文档，这里是不会在网页上展示的，但是没有写完的却可以通过git来保存 _posts文件夹：保存已经写好的markdown文章 assets文件夹：保存一些图片和css的一些文件 _config.yml：用来修改主页上的一些个人信息 Gemfile：这里是该主题下需要的一些gem依赖，这里直接在当前目录下bundle install即可下载安装依赖，注意这里如果gem下载很慢，可以设定source源，直接在Gemfile文件开头写source 'https://gems.ruby-china.com/'2.2 如何在博客中展示公式如果自己的博客中需要展示数学公式的，那么需要在_layouts/default.html文件中进行添加：&lt;/script&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"&gt;&lt;/script&gt;2.3 打开自己博客网页响应很慢这里一般是自己博客中展示的图片资源过大导致的，这里需要将图片进行无损压缩，重新上传即可。这里推荐几个图片无损压缩的网站。 TinyPNG：https://tinypng.com/ Compressor：https://compressor.io/三. Windows下安装jekyll这里由于jekyll一般在mac和linux上安装较为方便，官方也有指导，但是windows下官方写的不详细，因此这里主要介绍下Windows下如何安装jekyll。3.1 软件安装 Ruby+Devkit 2.7.2(x64)：安装的时候注意添加到Path中，其需要安装MSYS2 and MINGW development toolchain RubyGems：这里直接选择zip进行安装即可。3.2 Jekyll及gem依赖的安装 cd到解压后的RubyGems的文件中：ruby setup.rb 安装jekyll：gem install jekyll 安装jekyll-paginate：gem install jekyll-paginate 安装bundler（注意这里在后面安装gem依赖需要）：gem install bundler 验证下jekyll和bundler：jekyll -v和bundler -v3.3 在本地运行静态博客 cd到自己博客中，安装gem依赖：bundle install 本地运行服务：bundle exec jekyll s"
    } ,
  
    {
      "title"       : "Windows下安装mysql和导入sql文件",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./Windows%E4%B8%8B%E5%AE%89%E8%A3%85mysql%E5%92%8C%E5%AF%BC%E5%85%A5sql%E6%96%87%E4%BB%B6.html",
      "date"        : "2020-10-17 03:21:00 +0800",
      "description" : "介绍在Windows环境下本地安装mysql和导入sql文件的使用",
      "content"     : "一. 下载软件 mysql：这里使用的是scoop来进行安装：scoop install mysql，这里的优势是自动帮你配好环境了 安装Navicat Premium 破解Navicat Premium二. 初始化mysql 初始化数据库：mysqld --initialize --console,并记录红色标注的字符，这是随机生成的密码 输入mysqld -install将mysql安装为Windows的服务： 启动mysql：net start mysql 首次进入mysql：mysql -u root -p，输入第一次的系统生成的密码 输入ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'mysql的密码';回车 别漏了后面有个分号 mysql的密码是安装mysql时设置的密码 输入FLUSH PRIVILEGES;，这里一定要输入，不然用navicat链接的时候会报1251连接不成功 修改my.ini文件：首先进入scoop安装的mysql文件夹中（C:\\Users\\Administrator\\scoop\\apps\\mysql\\8.0.21），修改my.ini文件，如果不加secure_file_priv=''，会导致无法导入导出数据。[mysqld]datadir=D:/yeyan/mysql_data/datasecure_file_priv='' [client]user=root三. 导入.sql文件 启动mysql：net start mysql 首次进入mysql：mysql -u root -p，输入自己的密码 查看数据库：show databases; 使用某个数据库：use test 查看该数据库下的表：show tables; 导入sql文件：source D:/git_repo/Trace/data.sql;"
    } ,
  
    {
      "title"       : "Visual Studio 2019 下搭建opencv3.4.11的C++环境",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./Visual-Studio-2019-%E4%B8%8B%E6%90%AD%E5%BB%BAopencv3.4.11%E7%9A%84C++%E7%8E%AF%E5%A2%83.html",
      "date"        : "2020-10-16 03:21:00 +0800",
      "description" : "在Windows下利用Visual Studio2019 来搭建opencv3.4.11的C++环境",
      "content"     : "一. 下载需要的软件 visual studio 2019 社区版 opencv3.4.11二. 基于C++的环境搭建2.1 创建系统环境变量 解压opencv，到D:\\software 配置系统变量：Path下添加Opencv的路径D:\\software\\opencv\\opencv\\build\\x64\\vc15\\bin（这里选择vc15更适合vs2019，如果是vs2015就选择vc14） 2.2 在Visual Studio2019中配置Opencv 选择视图-属性管理器- 选择Debugx64-添加新项目属性表-这里选择保存的名称和位置 选择VC++目录-包含目录中添加以下 D:\\software\\opencv\\opencv\\build\\includeD:\\software\\opencv\\opencv\\build\\include\\opencvD:\\software\\opencv\\opencv\\build\\include\\opencv2 选择VC++目录-库目录中添加D:\\software\\opencv\\opencv\\build\\x64\\vc15\\lib 选择链接器-输入-附加依赖项中添加opencv_world3411d.lib 保存即可，注意这里构建的新建项目属性表可以保存下来，直接其他的项目直接导入用即可（视图-属性管理器- 选择Debugx64-添加现有属性表） 回到解决方案资源管理器-项目-属性-配置管理器-活动解决方案平台-选择x64-Debug三. 构建代码测试 构建cpp源码：解决方案-源文件-添加-新建项-cpp文件用以下代码进行测试#include&lt;iostream&gt;#include&lt;opencv2/core/core.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/opencv.hpp&gt;#include&lt;math.h&gt;using namespace cv;using namespace std;int main(){ //Mat img = imread(\"D:\\\\vs_project\\\\opencvtest\\\\1.jpg\"); Mat img = imread(\"D:/vs_project/opencvtest/1.jpg\"); if (img.empty()) { cout &lt;&lt; \"Could not load img...\" &lt;&lt; endl; return -1; } namedWindow(\"ori_img\", WINDOW_AUTOSIZE); imshow(\"ori_img\", img); // 图像转成灰度图像 Mat gray_img; cvtColor(img, gray_img, CV_RGB2GRAY); namedWindow(\"gray_img\", WINDOW_AUTOSIZE); imshow(\"gray_img\", gray_img); waitKey(0); return 0;}"
    } ,
  
    {
      "title"       : "Xcode搭建Opencv3环境",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./Xcode%E6%90%AD%E5%BB%BAOpencv3%E7%8E%AF%E5%A2%83.html",
      "date"        : "2020-10-14 03:21:00 +0800",
      "description" : "在Mac中利用Xcode神器搭建opencv3的C++环境",
      "content"     : "1. 下载opencv 使用简单粗暴的方式——brew进行安装：brew install opencv@3，注意这里通过brew下载的opencv3的地址为：/usr/local/Cellar/opencv@3/3.4.9_1（后面配置include和lib有用）。 这里存在很大的问题：brew除了下载opencv以外还需要下载opencv的依赖包（很多），这里强力推荐换brew的镜像源（本人用的清华的，当然也可以用中科大的）。具体配置方式如下： 第一步：替换brew.git： cd \"$(brew --repo)\"git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git 第二步：替换 homebrew-core.git： powershell cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git 2. 在Xcode上搭建opencv的环境 新建项目：macOS - Command Line Tool - 这里选择语言为C++ 点击项目，选择Build Settings- 在搜索框中搜索search。 在头文件路径Header Search Paths中debug中添加一下/usr/local/Cellar/opencv@3/3.4.9_1/include/usr/local/Cellar/opencv@3/3.4.9_1/include/opencv/usr/local/Cellar/opencv@3/3.4.9_1/include/opencv2 在Library Search Paths中添加/usr/local/Cellar/opencv@3/3.4.9_1/lib 在项目中添加动态链接库文件：选择项目- 右键New Group - 新建一个名字（比如lib）- 右键lib - Add files to - 按下/会直接提示到那个目录下找dylib，这里是/usr/local/Cellar/opencv@3/3.4.9_1/lib，把当前目录下的所有dylib都添加进去即可，如下图。 以上就是整个opencv3在Xcode的环境了。3. 测试案例#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;using namespace std;using namespace cv;int main(int argc, const char * argv[]) { // insert code here... cout &lt;&lt; \"This is my first try C++ in xcode!\\n\"; Mat img = imread(\"/Users/xcode_project/C++_project/opencvTutorial/test.jpeg\"); if (img.empty()){ cout &lt;&lt; \"Could not open image ...\"&lt;&lt; endl; return -1; } namedWindow(\"test\",CV_WINDOW_AUTOSIZE); imshow(\"test\", img); waitKey(0); return 0;}"
    } ,
  
    {
      "title"       : "深度学习算法面试总结",
      "category"    : "",
      "tags"        : "机器学习, 深度学习, 面试",
      "url"         : "./%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93.html",
      "date"        : "2020-06-12 23:21:00 +0800",
      "description" : "面试中机器学习，深度学习中常问的算法总结",
      "content"     : "面试官会根据自己简历中提到的一些点进行提问，这里先自己对某些点进行深挖。一.数据处理海量数据： （1）数据量太大，无法短时间内处理完成 （2）无法一次性将数据放入内存中。1.1 缺失值处理 填充固定值：选取某个固定值/默认值填充缺失值。 填充均值：对每一列的缺失值，填充当列的均值。 填充中位数：对每一列的缺失值，填充当列的中位数。 填充众数：对每一列的缺失值，填充当列的众数。由于存在某列缺失值过多，众数为nan的情况，因此这里取的是每列删除掉nan值后的众数。 填充上下条的数据：对每一条数据的缺失值，填充其上下条数据的值。 填充插值得到的数据：用插值法拟合出缺失的数据，然后进行填充。插值是离散函数逼近的重要方法，利用它可通过函数在有限个点处的取值状况，估算出函数在其他点处的近似值。二.机器学习2.1 SVM和LR的区别与联系？SVM 和 LR 都是属于分类算法，不过 SVM 是通过划分超平面的方法来进行分类，而 LR 则是通过计算样本属于哪个类别的概率，从而达到分类效果2.2 交叉熵函数系列问题？与最大似然函数的关系和区别？在二分类中，交叉熵函数和负最大似然函数的表达式是相同的，但是交叉熵函数是从信息论角度得到的，而最大似然函数则是从概率论角度得到的交叉熵涉及到2点： 信息量：假设X是一个离散型随机变量，其取值集合为X，概率分布函数为p(x)=Pr(X=x),x∈X，我们定义事件X=x0的信息量为：I(x0)=−log(p(x0))，可以理解为，一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。 熵：假设小明的考试结果是一个0-1分布XA只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。 相对熵：称为KL散度，是两个随机分布间距离的度量。越小说明分布越一致。 交叉熵：交叉熵与KL距离在行为上是等价的，都反映了分布p，q的相似程度。特别的，在logistic regression中，p:真实样本分布，服从参数为p的0-1分布，即X∼B(1,p)X∼B(1,p)q:待估计的模型，服从参数为q的0-1分布，即X∼B(1,q)两者的交叉熵为2.3 SVM的核函数使用非线性核的支持向量机可以处理线性不可分的问题。通过核函数，支持向量机可以将特征向量映射到更高维的空间中，使得原本线性不可分的数据在映射之后的空间中变得线性可分，如下图所示，原本二维空间的线性不可分（异或问题）转成三维空间，就可以线性可分了。常用的核函数：线性核、多项式核、高斯核（RBF）、拉普拉斯核等等。核函数的选择其实才是SVM模型的最大变数。2.4 L1和L2范数范数的定义：\\(\\|\\mathbf{x}\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}\\)L1范数就是p=1,即：\\(\\|\\boldsymbol{x}\\|_{1}:=\\sum_{i=1}^{n}\\left|x_{i}\\right|\\)L2范数就是p = 2，即:\\(\\|\\boldsymbol{x}\\|_{2}:=\\sqrt{x_{1}^{2}+\\cdots+x_{n}^{2}}\\)这里如果需要求解如何使得上述式子最小，无可避免三步走：求导，置零，解方程。因此L2范数计算就比L1范数计算更容易，因此L2范数应用较多。L1 和 L2 范数在机器学习上最主要的应用大概分下面两类： 作为损失函数使用(计算回归问题中需要计算拟合的线和点之间的距离)，这里L1是LAD（最小绝对偏差），L2是最小二乘法 作为正则项使用（防止过拟合）也即所谓 L1-regularization 和 L2-regularization：这里就是将x替换成权重w，这两个正则项最主要的不同，包括两点：如上面提到的，L2 计算起来更方便，而 L1 在特别是非稀疏向量上的计算效率就很低；还有就是 L1 最重要的一个特点，输出稀疏，会把不重要的特征直接置零，而 L2 则不会；最后，如之前多次提过，L2 有唯一解，而 L1 不是。2.5 决策树决策树属于典型的“白盒模型”，如下图所示，我是否应该接收一个新的offer？这里可以通过构建一个个节点，来判断我是否应该接收offer。比较常用的决策树有ID3，C4.5和CART（Classification And Regression Tree）。熵：熵是随机变量的不确定程度。越混乱熵值越高，说明越混乱，分类越混乱。当熵值为0时，说明是纯物质。以下是熵的公式。\\(H(X)=-\\sum_{i=1}^{n} p_{i} \\log p_{i}\\)​ 当Entropy最大为1的时候，是分类效果最差的状态，当它最小为0的时候，是完全分类的状态。因为熵等于零是理想状态，一般实际情况下，熵介于0和1之间。熵的不断最小化，实际上就是提高分类正确率的过程。例子：A = [1,1,1,1,2,2,1,1,1,1] , B = [1,2,3,4,5,6,6,7,8,0] ，显然A集合的熵值小很多。信息增益 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 C4.5决策树学习应用信息增益准则选择特征。信息增益的定义：给定训练数据集D和特征A，经验熵H（D）表示对数据集D进行分类的不确定性。而经验条件熵H（D|A）表示在特征A 给定的条件下对数据集D进行分类的不确定性，那么它们的差，即信息增益。如下公式：\\(g(D, A)=H(D)-H(D \\mid A)\\)表示由于特征A而使得对数据集D的分类的不确定性减少的程度。不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。基尼指数 CART决策树采用基尼指数(Gini index)来选择划分特征。基尼指数的定义：在数据集中随机抽取2个样本，其类别不一样的概率。因此Gini越小，数据集D纯度越高。\\(\\operatorname{Gini}(D)=\\sum_{k=1}^{Y} \\sum_{k \\neq j} p_{k} p_{j}=1-\\sum_{k=1}^{Y} p_{k}^{2}\\)决策树的过拟合决策树很容易发生过拟合的现象。原因是由于可以通过不断的分枝使得信息熵为0.如何解决该现象？进行剪枝： 预剪枝：在决策树生成过程中，对每个节点划分前估计出验证集的精度决定是否划分。 后剪枝：先训练完成一个完整的决策树，再自底而上进行剪枝。2.6 随机森林随机森林属于集成学习的Bagging方法，同时也是由多个弱分类器构建的强分类器。森林： 随机森林是由很多决策树并行构成的，决策树之间没有关联。 当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。随机： 样本的随机：在训练过程中，输入到每个决策树的样本是从总体样本中随机抽样的。 决策树节点的随机：对每个决策树而言，其节点属性都是从总的属性中随机抽取的。特点： 由于是并行模型，训练快。 得到不同特征的对模型的重要程度。 不容易过拟合。 如果有很大一部分的特征遗失，仍可以维持准确度。三. NLP3.1 什么是TF-IDF?词频-逆文档频率TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。\\(\\text { 词频(TF) }=\\frac{\\text { 某个词在文章中的出现次数 }}{\\text { 文章的总词数 }}\\)\\(\\text { 逆文档频率(IDF) }=\\log \\left(\\frac{\\text { 语料库的文档总数 }}{\\text { 包含该词的文档数 } x+1}\\right)\\)当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。3.2 什么是word2vec判断一个词的词性（动词，名词）这里可以用word2vec，嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec是词嵌入的一种。 Skip-gram 模型：用一个词语作为输入，来预测它周围的上下文 CBOW 模型：拿一个词语的上下文作为输入，来预测这个词语本身3.3 fastText word2vec的CBOW模型架构和fastText模型非常相似 fastText 和CBOW差别：CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。3.4 NER named-entity-recognition（命名实体识别，又叫“专名识别”）。指识别文本中具有特定意义的实体，主要包括人名，地名，机构名，专有名词。NER系统就是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体，比如产品名称、型号、价格等。学术上NER所涉及的命名实体一般包括3大类（实体类，时间类，数字类）和7小类（人名、地名、组织机构名、时间、日期、货币、百分比）。货币、百分比等数字类实体可通过正则搞定。 NER是NLP中一项基础性关键任务。从自然语言处理的流程来看，NER可以看作词法分析中未登录词识别的一种，是未登录词中数量最多、识别难度最大、对分词效果影响最大问题。同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。 原句：姚明在NBA打篮球 如下标签：姚/B-PER 明/I-PER 在/O NBA/B_ORG 打/O 篮/O 球/O其中常见的方法是对字或者词打上标签。B-type, I-type, O， 其中B-type表示组成该类型实体的第一个字或词。I-type表示组成该类型实体的中间或最后字或词，O表示该字或词不组成命名实体，当然有的地方也采用B-type, I-type, E-type，O形式。整体结构如下： 字（词嵌入）==&gt; BiLSTM（拿到字的每一个标签的所有得分）==&gt; CRF（输出预测标签值） 这里问什么要用到CRF(直接用全连接分类即可)？==&gt; CRF层能从训练数据中获得约束性的规则：CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。在训练数据训练过程中，这些约束可以通过CRF层自动学习到。 这些约束可以是： I：句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” II：标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列. III：标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。 有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。 CRF（条件随机场）：属于判别式模型，条件随机场对多个变量在给定观测值后的条件概率进行建模。概率图模型是以某些可观测的变量为条件分布进行推断。假设某个字的前后（x_1,x_2,x_3）,推断问题的目标就是计算2在1的条件下发生的概率，然后所有条件概率相加。3.5 文本增强技术 词汇和短语进行替换：选择同义词进行替换；空间中找到相邻的词汇进行替换；利用TF-IDF对哪些非核心词汇（分值很低的）进行替换 随机噪音：随机插入一些词汇，占位符；交换词汇或者shuffle句子；随机删除词汇或者句子 混合增强：起源于图像的mixup（猫和狗的混合）。提出了wordMixup和sentMixup将词向量和句向量进行Mixup。 回译：中文翻译成英文表达，然后再由英文翻译回中文。 GAN对抗生成网络：GAN 主要分为两部分：生成模型和判别模型。生成模型的作用是模拟真实数据的分布，判别模型的作用是判断一个样本是真实的样本还是生成的样本。GAN 的目标是训练一个生成模型完美的拟合真实数据分布使得判别模型无法区分。四. 图像算法 参考文章：图像总常用的变换边缘检测图像增强技术4.1 图像特征提取的方法有哪些？ SIFT（尺度不变特征变换）—— 图像拼接： HOG（方向梯度直方图（Histogram of Oriented Gradient, HOG））—— 行人检测：特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。）本质：梯度的统计信息，而梯度主要存在于边缘的地方。 LBP(Local Binary Pattern局部二值模式)：种描述图像局部纹理的特征算子，具有旋转不变性与灰度不变性等显著优点。LBP特征将窗口中心点与邻域点的关系进行比较，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了复杂场景下（光照变换）特征描述问题（局部纹理特征提取）。4.2 为什么要图像的灰度化? 图像识别中要识别物体：找到edge ==&gt; 计算梯度 ==&gt; 需要用到灰度图 有利于图像特征提取：RGB采用的是三通道，而灰度图用的是单通道，能加快特征抽取。4.3 为什么预处理中要归一化和标准化 取值范围从0～255已经转化为0～1之间了，这个对于后续的神经网络或者卷积神经网络处理有很大的好处，加快梯度下降求解的速度 减小了几何变换和仿射变化的影响。4.4 为什么要中值滤波和均值滤波? 目的：消除图像中的噪声成分叫作图像的平滑化或滤波操作。图像的能量大部分集中在幅度谱的低频和中频段是很常见的，而在较高频段，感兴趣的信息经常被噪声淹没。 中值滤波：一连串数字｛1，4，6，8，9｝中，数字6就是这串数字的中值.椒盐噪声很好的被平滑了，而且也没均值那样模糊化太过于严重。 均值滤波：图片中一个方块区域（一般为3*3）内，中心点的像素为全部点像素值的平均值。一般均值滤波过于模糊化了。4.5 边缘检测算子 Roberts算子：基于x轴和y轴的\\(s_{x}=\\left[\\begin{array}{cc}1 &amp; 0 \\\\0 &amp; -1\\end{array}\\right]\\)\\(s_{y}=\\left[\\begin{array}{cc}0 &amp; -1 \\\\1 &amp; 0\\end{array}\\right]\\) Prewitt算子：\\(s_{x}=\\left[\\begin{array}{ccc}-1 &amp; 0 &amp; 1 \\\\-1 &amp; 0 &amp; 1 \\\\-1 &amp; 0 &amp; 1\\end{array}\\right]\\)\\(s_{y}=\\left[\\begin{array}{ccc}1 &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 0 \\\\-1 &amp; -1 &amp; -1\\end{array}\\right]\\) Sobel算子：\\(s_{x}=\\left[\\begin{array}{ccc}-1 &amp; 0 &amp; 1 \\\\-2 &amp; 0 &amp; 2 \\\\-1 &amp; 0 &amp; 1\\end{array}\\right]\\)\\(s_{y}=\\left[\\begin{array}{ccc}1 &amp; 2 &amp; 1 \\\\0 &amp; 0 &amp; 0 \\\\-1 &amp; -2 &amp; -1\\end{array}\\right]\\) 基本的边缘算子如Sobel求得的边缘图存在很多问题，如噪声污染没有被排除、边缘线太过于粗宽等 Canny算子：目标是找到一个最优的边缘。具有以下优势 低错误率：标识尽可能多的实际边缘，剑豪噪声产生的误报。 高定位性：标识出的边缘要与图像的实际边缘尽可能的接近。 最小响应：图像中的边缘只能标识一次。 canny检测的步骤： 使用高斯滤波器降噪。 利用Sobel算子进行卷积（x和y反向） 将像素点上x和y卷积之后的平方求根，并计算x，y方向上的角度， \\(G=\\sqrt{G_{x}^{2}+G_{y}^{2}}\\)，\\(\\theta=\\arctan \\left(\\frac{G_{y}}{G_{x}}\\right)\\) 非极大值抑制，进一步排除非边缘的像素，仅保留一些细线条。 滞后阈值：高于某阈值，保留为边缘像素，反之排除。 4.6 常用的插值方法在图像几何变换时，无法给有些像素点直接赋值，例如，将图像放大两倍，必然会多出一些无法被直接映射的像素点，对于这些像素点，通过插值决定它们的值。于是，产生了图像插值算法。 线性插值：最近邻插值，双线性插值以及双三次插值等，\\(f(x)=a_{1} x+a_{0}\\)4.7 深度学习和传统目标检测方法的优缺点传统的目标检测算法对光照，明暗，数据传输，物体遮挡等上模型的鲁棒性不强。4.8 图像增强技术增强技术也可以有多种分类，如，可以分为平滑（抑制高频成分）与锐化（增强高频成分），空间域与频域。 空间域增强就是指增强构成图像的像素，是直接对这些像素进行操作的过程。 频域则是修改图像的傅立叶变换。4.9 SSD和Yolo SSD：将物体检测这个问题的解空间，抽象为一组预先设定好（尺度，长宽比）的bounding box。在每个bounding box，预测分类label，以及box offset来更好的框出物体。对一张图片，结合多个大小不同的feature map的预测结果，以期能够处理大小不同的物体。 （优点）相比Fast RNN系列，删除了bounding box proposal这一步，及后续的重采样步骤，因而速度较快，达到59FPS。 （优点） YOLO：将物体检测这个问题定义为bounding box和分类置信度的回归问题。将整张图像作为输入，划分成SxS grid，每个cell预测B个bounding box（x, y, w, h）及对应的分类置信度（class-specific confidence score）。分类置信度是bounding box是物体的概率及其与真实值IOU相乘的结果。 （优点）速度快，45FPS （优点）YOLO使用图像的全局信息做预测，因而对背景的误识别率低。 （缺点） 每个cell只能拥有一个label和两个bounding box，这个空间局限性，使得对小物体检测效果不好 （缺点）对于物体长宽比的泛化能力较弱，当一类物体新的长宽比出现时，检测准确率减低。 二者之间的差别：YOLO在卷积层后接全连接层，即检测时只利用了最高层Feature maps（包括Faster RCNN也是如此）而SSD采用金字塔结构，即利用了conv4-3/fc7/conv6-2/conv7-2/conv8_2/conv9_2这些大小不同的feature maps，在多个feature maps上同时进行softmax分类和位置回归。SSD还加入了Prior box4.10 零样本学习（Zero-shot Learning）和单样本学习（One-shot Learning） 零样本学习：基于可见标注数据集&amp;可见标签集合（seen），学习并预测不可见（unseen，无标注）数据集结果。4.11 前景背景分割4.12 工业相机CCD和CMOS CCD（电荷耦合元件）：输出节点统一输出数据，信号一致性好；CCD采用逐个光敏输出，速度较慢 CMOS（金属氧化物半导体元件）：CMOS芯片中每个像素都有自己的信号放大器，各自进行电荷到电压的转换，输出信号的一致性较差，比CCD的信号噪声更多。CMOS每个电荷元件都有独立的装换控制器，读出速度很快，FPS在500以上的高速相机大部分使用的都是CMOS。4.13 小目标检测在深度学习目标检测中，特别是人脸检测中，小目标、小人脸的检测由于分辨率低，图片模糊，信息少，噪音多，所以一直是一个实际且常见的困难问题。FPN特征金字塔网络：参考文章：https://zhuanlan.zhihu.com/p/920059274.14 目标检测中的mAP具体参考文章：https://www.cnblogs.com/itmorn/p/14193729.html五. 深度学习5.1 梯度消失的原因和解决办法有哪些？ 梯度消失：每一层非线性层都可以视为是一个非线性函数 f(x)f(x)(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数。那么根据“链式求导”法则，比如rnn来说，其激活函数为tanh，那么tanh的导数的最大值是1，那么如果连乘0.8的100次方，无线接近于0，导致梯度消失。 梯度爆炸：tanh导数 * W权重，这里如果W的值太大了，随着序列长度的增加，连乘无限大，导致梯度爆炸。 解决方案：一个是激活函数比如relu系列，一个初始化权重 ，一个是梯度裁剪5.2 RNN 和LSTM的差别在哪？RNN的前向推导公式：LSTM的三种门控制如下：如上图所示，它们的名字、表示的计算过程及输出分别是： 遗忘门： \\(f_i=\\sigma\\left(W_f\\left[x_i, h_{i-1}\\right]+b_f\\right)\\) 输入们： \\(i_i=\\sigma\\left(W_i\\left[x_i, h_{i-1}\\right]+b_i\\right)\\) 输出们： \\(o_i=\\sigma\\left(W_o\\left[x_i, h_{i-1}\\right]+b_o\\right)\\) 可以看到，除了参数不同，它们计算公式是一样的。啰嗦一句，上图中 [公式] 表示sigmoid函数， [公式] 表示tanh函数： RNN来说，它能够处理一定的短期依赖，但无法处理长期依赖问题。原因：当序列较长时，序列后部的梯度很难反向传播到前面的序列，比如10个元素以前，这就产生了梯度消失问题 当然，RNN也存在梯度爆炸问题，但这个问题一般可以通过梯度裁剪（gradient clipping）来解决 RNN没有细胞状态；LSTM通过细胞状态记忆信息。 RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添加求和操作，减少梯度消失和梯度爆炸的可能性。 RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。5.3 注意力机制是为了解决什么问题？为什么选用了双向循环神经网络？ 人脑在工作时具有一定注意力，当欣赏艺术品时，既可以看到全貌，也可以关注 细节，眼睛聚焦在局部，忽略其他位置信息。说明人脑在处理信息的时候有一定权重划分。而注意力机制的提出正是模仿了人脑的这种核心特性。 实际使用中，随着输入序列长度的增加，模型性能显著下降。因为编码时输入序列的全部信息被压缩到一个向量表示中去。序列越长，句子越前面的词的信息丢失就越严重。以100词的句子为例，编码时将整个句子的信息压缩到一个向量中去，而在解码时(比如翻译)，目标语言第一个单词大概率与源语言第一个单词对应，这就意味着第一步的解码需要考虑到100步之前的信息。一个小技巧是可以将源语言句子逆向输入，或者重复输入两遍，得到一定的提升，也可以使用LSTM缓解这个问题。但对于过长序列仍难以有很好表现。5.4 Batch Normalization和Dropout差别 BN训练和测试时的参数是一样的嘛？BN是对每一批训练数据进行归一化，使用每一批数据的均值和方差；测试的时候，每一批数据中仅有一个样本，没有batch概念了，这里的均值和方差就是全量数据均值和方差。 BN训练时为什么不用全量训练集的均值和方差呢？对于BN，是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。BN操作把分布压缩在[-1,1],服从均值为0,方差为1的正太分布，相当于把大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。 Dropout的作用是什么？ 在训练的过程中以一定概率使得神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。 Dropout 在训练和测试时都需要嘛？dropout仅在训练的时候采用，为了减少神经元对部分上层神经元的依赖，类似于将多个不同的网络结构的模型集成起来，减少过拟合和增强其鲁棒性。测试的时候用到的是整个训练完成的模型，不需要dropout。 Dropout 如何平衡训练和测试时的差异呢？假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。 BN和Dropout共同使用时会出现的问题BN和Dropout单独使用都能减少过拟合并加速训练速度，但如果一起使用的话并不会产生1+1&gt;2的效果，相反可能会得到比单独使用更差的效果。5.5 Batch Normalization和Layer Normalization的差别 LN和BN都是一种归一化方式，差别是：BN是取的是不同样本的同一个特征进行归一化；LN取得是同一个样本的不同特征。 应用场景不同：LN适用于RNN或者batchsize较小；BN适用于CNN。 对于RNN来说，每个样本的长度都是不同的，那么当BN需要统计靠后的时间片段的时候，可能都没有这方面的信息，那么只基于某些长时间片段的样本的统计信息无法反应出全局分布，所以就不合适了。 5.6 bert的具体网络结构，以及训练过程，及其优势在哪 bert处理句子是整体处理的，不是逐字处理的，解决了不受长期依赖问题困扰的主要原因（不存在过去信息丢失的风险），同时提高了训练效率。 多头注意力和位置嵌入：提供了有关不同单词之间的关系信息。 总结：完全避免了递归操作，通过整体处理句子以及学习单词之间的关系来感谢多头注意机制和位置嵌入。5.7 albert和bert的差别在哪 albert的核心：训练出更小但效果更好的模型! 想让模型更轻，训练更快，效果更好！（期望的是用更少量的数据，得到更好的结果）。ALBERT提出了三种优化策略，做到了比BERT模型小很多的模型，但效果反而超越了BERT， XLNet。 Factorized Embedding Parameterization. 他们做的第一个改进是针对于Vocabulary Embedding。在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。为了解决这些问题他们提出了一个基于factorization的方法。他们没有直接把one-hot映射到hidden layer, 而是先把one-hot映射到低维空间之后，再映射到hidden layer。这其实类似于做了矩阵的分解。 Cross-layer parameter sharing. 每一层的layer可以共享参数，这样一来参数的个数不会以层数的增加而增加。所以最后得出来的模型相比BERT-large小18倍以上。 Inter-sentence coherence loss. 在BERT的训练中提出了next sentence prediction loss, 也就是给定两个sentence segments, 然后让BERT去预测它俩之间的先后顺序，但在ALBERT文章里提出这种是有问题的，其实也说明这种训练方式用处不是很大。 所以他们做出了改进，他们使用的是setence-order prediction loss (SOP)，其实是基于主题的关联去预测是否两个句子调换了顺序。 5.8 CNN和RNN的差别 训练速度上：CNN快很多。RNN慢的原因是每个timestep的计算，都要依赖前一个时刻的输出。而cnn的卷积的时候，和空间上其他的点没有任何联系，适合并行计算。 数据约束：CNN对于数据的约束就很强了，图像识别，input的纬度是48*48的，必须定死了，而RNN其实对于数据的长度（句子的长度）没有要求（TF里面有动态rnn来在输入rnn之前去掉pad为0的地方） 卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。共享参数是深度学习一个重要的思想，其在减少网络参数的同时仍然能保持很高的网络容量(capacity)。卷积层在空间方向共享参数，而循环神经网络(recurrent neural networks)在时间方向共享参数。5.9 优化器和超参调节SGD(随机梯度下降)​ 在随机梯度下降算法（SGD）中，优化器基于小批量估计梯度下降最快的方向，并朝该方向迈出一步。由于步长固定，因此 SGD 可能很快停滞在平稳区（plateaus）或者局部最小值上。\\(w_{t+1}=w_{t}-\\alpha \\cdot g_{t}\\)​ 基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲。5.9 深度学习平台 阿里NASA计划的机器学习平台PAI（17年） 全面兼容TF，Caffe，MXNet深度学习框架 提供云端的计算资源 集成很多机器学习算法（分类，回归，聚类） 支持大规模的分布式数据训练 百度paddlepaddle飞桨(18年) 支持大规模的分布式数据训练 多平台部署 产业级的开源模型库（语义理解，图像分类，目标检测，图像分割等多种场景） 微软Microsoft Custom Vision Services（17年） 针对的是图像分类器 提供迁移学习的模型 谷歌的Cloud AutoML 针对的是图像分类器 提供迁移学习的模型"
    } ,
  
    {
      "title"       : "Postman和Jmeter进行上传文件及压力测试",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./Postman%E5%92%8CJmeter%E8%BF%9B%E8%A1%8C%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%8F%8A%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95.html",
      "date"        : "2020-03-23 23:21:00 +0800",
      "description" : "讲述如何利用Postman和Jmeter对网络接口进行压力测试",
      "content"     : "一. 准备工作 postman下载链接：https://www.postman.com/downloads/ Jmeter下载链接：http://jmeter.apache.org/download_jmeter.cgi flask代码地址：https://github.com/yy2lyx/FlaskTutorial/tree/master/Flask-7-upload windows下scoop下载jdk(这里是由于Jmeter需要)：scoop install ojdkbuild二. 构建接口的flask服务其中包含前端表单index.html文件如下和flask的后端&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;upload&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action = \"/success\" method = \"post\" enctype=\"multipart/form-data\"&gt; &lt;input type=\"file\" name=\"file\"&gt; &lt;input type = \"submit\" value=\"Upload\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;三. Postman的http接口测试postman分别在header和body中填入下图： headers中需要填入value:multipart/form-data body 中需要填入key：file(这里参考index.html文件中name=”file”)，value:eml文件地址 然后将写好的保存在collections当中，并构建tests选项（如果不填入，后面的串行压力测试无法开始，报错） 通过collections中选中保存好的请求，run即可四. Jmeter的http接口测试在下载解压好的jmeter二进制文件中打开：apache-jmeter-5.3\\bin\\jmeter.bat 新建一个线程组，如下图，包括http请求及监听 线程中填入线程总数，和全部线程开启总的时间（这里由于需要测试并发1小时2万次访问） 在http请求页面填入请求的参数 http页面下面不用填Parameters和BodyData，在Files Upload中填入下图，其中file和上面一致，而MIME Type需要访问https://www.freeformatter.com/mime-types-list.html，找到其中.eml格式前面的 运行，即可看到并行的接口请求情况 五. 总结 一般的网络接口测试，功能性测试postman较为好用。 需要测试高并发的情况下，只能用Jmeter来进行测试，因为postman是串行，而Jmeter是多线程并行测试。"
    } ,
  
    {
      "title"       : "Neo4j数据库与图数据挖掘算法结合",
      "category"    : "",
      "tags"        : "机器学习, 图算法",
      "url"         : "./Neo4j%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E5%9B%BE%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E7%BB%93%E5%90%88.html",
      "date"        : "2020-02-27 23:21:00 +0800",
      "description" : "讲述Neo4j图数据库的使用，图挖掘算法（社区算法）的实现",
      "content"     : "一. 环境准备 neo4j python包：pip3 install neo4j 和pip3 install py2neo（这里的py2neo 是python对Neo4j的驱动库,同时这里必须是py2neo版本必须是最新版4，不然会报连接数据库的错误，老版本不兼容的问题） Java8：这里由于neo4j 数据库是依赖于java8的。 Neo4j_3.5.14：这里由于neo4j 在中国地区下载慢，并且neo4j3.X版本才支持java8，到4.0版本就是需要java11了。 Neo4j_Desktop：neo4j的桌面端（可以远程数据库和连接本地数据库，同时包含很多额外的扩展） 二. 连接本地图数据库 py2neo V4 官方文档：https://py2neo.org/v4/index.htmlNeo4j 一共有3种连接方式： Bolt：bolt://localhost:11005 HTTP：http://localhost:11006 HTTPS：https://localhost:11007这里可以通过Neo4j Desktop来查看新建的图数据库（同时设置密码）2.1 Neo4j数据库语法Cypher 创建 create (:Movie {title:\"ABC\",released:2016}) return p; 查询match (p: Person) return p; 查询Person类型的所有数据match (p: Person {name:\"sun\"}) return p; 查询名字等于sun的人match( p1: Person {name:\"sun\"} )-[rel:friend]-&gt;(p2) return p2.name , p2.age 查询sun的朋友的名字和年龄match (old) ... create (new) create (old)-[rel:dr]-&gt;(new) return new 对已经存在的节点和新建的节点建立关系 更新 MERGE (m:Movie { title:\"Cloud Atlas\" })ON CREATE SET m.released = 2012RETURN m 筛选过滤 match (p1: Person)-[r:friend]-&gt;(p2: Person) where p1.name=~\"K.+\" or p2.age=24 or \"neo\" in r.rels return p1,r,p2 聚合函数（支持count,sum,avg,min,max） MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations 排序和分页MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a,count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10;2.2 图数据库的基本操作py 这里是通过导入py2neo这个neo4j的第三方库来连接from py2neo import Graph,Nodegraph = Graph( \"http://localhost:11006\", username=\"neo4j\", password=\"yy\") 清空数据库 graph.delete_all() 定义节点关系a = Node('Person', name='Alice')b = Node('Person', name='Bob')r = Relationship(a, 'KNOWNS', b)s = a | b | rgraph.create(s) Node查询# 用CQL进行查询，返回的结果是listdata1 = graph.data('MATCH(p:PersonTest) return p')print(\"data1 = \", data1, type(data1))# 用find_one()方法进行node查找，返回的是查找node的第一个nodedata2 = graph.find_one(label='PersonTest', property_key='name', property_value=\"李四\")print (\"data2 = \", data2, type(data2))# 用find()方法进行node查找,需要遍历输出，类似于mongodbdata3 = graph.find(label='PersonTest')for data in data3: print (\"data3 = \", data) 关系查询relationship = graph.match_one(rel_type='KNOWNS')print (relationship, type(relationship)) 更新pushnode1 = graph.find_one(label='PersonTest', property_key='name', property_value=\"张三\")node1['age'] = 21graph.push(node1)data4 = graph.find(label='PersonTest')for data in data4: print (\"data4 = \", data) #基于上面的操作，再次定义node1[‘age’] = 99,并执行graph.push(node1)，发现已经更新node1['age'] = 99graph.push(node1)data5 = graph.find(label='PersonTest')for data in data5: print (\"data5 = \", data) 删除Node和Relationshipnode = graph.find_one(label='PersonTest', property_key='name', property_value=\"李四\")relationship = graph.match_one(rel_type='KNOWNS')graph.delete(relationship)graph.delete(node)data6 = graph.find(label='PersonTest')for data in data6: print (\"data6 = \", data) 多条件查询a = Node('PersonTest', name='张三', age=21, location='广州')b = Node('PersonTest', name='李四', age=22, location='上海')c = Node('PersonTest', name='王五', age=21, location='北京')r1 = Relationship(a, 'KNOWS', b)r2 = Relationship(b, 'KNOWS', c)s = a | b | c | r1 | r2graph.create(s)data7 = graph.find(label='PersonTest')for data in data7: print (\"data7 = \", data) 单条件查询# 单条件查询，返回的是多个结果selector = NodeSelector(graph)persons = selector.select('PersonTest', age=21)print(\"data8 = \", list(persons)) 多条件查询selector = NodeSelector(graph)persons = selector.select('PersonTest', age=21, location='广州')print(\"data9 = \", list(persons)) 复杂查询orderby# orderby进行更复杂的查询selector = NodeSelector(graph)persons = selector.select('PersonTest').order_by('_.age')for data in persons: print (\"data10 = \", data)三. 中心性算法实验（社区算法）3.1 中心性算法 度中心性：度中心性是最简单度量，即为某个节点在网络中的联结数。 MATCH (c:Character)-[:INTERACTS]-()RETURN c.name AS character, count(*) AS degree ORDER BY degree DESC 加权度中心性：指的是每个节点的权重后的中心性 MATCH (c:Character)-[r:INTERACTS]-()RETURN c.name AS character, sum(r.weight) AS weightedDegree ORDER BY weightedDegree DESC 介数中心性:在网络中，一个节点的介数中心性是指其它两个节点的所有最短路径都经过这个节点，则这些所有最短路径数即为此节点的介数中心性。MATCH (c:Character)WITH collect(c) AS charactersCALL apoc.algo.betweenness(['INTERACTS'], characters, 'BOTH') YIELD node, scoreSET node.betweenness = scoreRETURN node.name AS name, score ORDER BY score DESC 紧密度中心性：指到网络中所有其他角色的平均距离的倒数。 MATCH (c:Character)WITH collect(c) AS charactersCALL apoc.algo.closeness(['INTERACTS'], characters, 'BOTH') YIELD node, scoreRETURN node.name AS name, score ORDER BY score DESC 3.2 PageRank 算法PageRank算法源自Google的网页排名。它是一种特征向量中心性(eigenvector centrality)算法。UNWIND {nodes} AS nMATCH (c:Character) WHERE c.name = n.nameSET c.pagerank = n.pg可以在Neo4j的图中查询最高PageRank值的节点：MATCH (n:Character)RETURN n.name AS name, n.pagerank AS pagerank ORDER BY pagerank DESC LIMIT 10"
    } ,
  
    {
      "title"       : "无监督异常检测模型",
      "category"    : "",
      "tags"        : "机器学习",
      "url"         : "./%E6%97%A0%E7%9B%91%E7%9D%A3%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B.html",
      "date"        : "2019-12-05 03:20:00 +0800",
      "description" : "主要讲述无监督的异常检测模型",
      "content"     : "异常检测模型一般分为五大类：统计和概率模型、线性模型、非线性模型、基于相似度衡量的模型、基于聚类的异常检测模型。一. 统计和概率模型主要是假设和检验。假设数据的分布，检验异常。比如对一维的数据假设高斯分布，然后将3sigma以外的数据划分为异常，上升到高维，假设特征之间是独立的，可以计算每个特征维度的异常值并相加，如果特征之间是相关的，也就变成了多元高斯分布，可以用马氏距离衡量数据的异常度。这类方法要求对问题和数据分布有较强的先验知识。1.1 3σ原则如果特征服从正态分布，那么，在\\(\\pm 3 \\sigma\\)的范围内包含了99.73%的“几乎所有”的内容（所有正常的值都在平均值正负三个标准差的范围内），而可能存在的异常值都在其之外。1.2 3σ原则的适用条件 数据分布满足正态分布（在业务逻辑上母样本满足）。 特征之间相互独立。二. 线性模型2.1 PCAPCA是最常见的线性降维方法，它们按照某种准则为数据集 \\(\\left\\{x_{i}\\right\\}_{i=1}^{n}\\)找到一个最优投影方向 W 和截距和截距 b ，然后做变换 \\(z_{i}=W x_{i}+b\\)得到降维后的数据集 \\(\\left\\{z_{i}\\right\\}_{i=1}^{n}\\)。因为\\(z_{i}=W x_{i}+b\\)是一个线性变换（严格来说叫仿射变换，因为有截距项），因此PCA属于线性模型（处理线性问题）。假设数据在低维空间上有嵌入，那么无法、或者在低维空间投射后表现不好的数据可以认为是异常点。PCA有两种检测异常的方法，一种是将数据映射到低维空间，然后在特征空间不同维度上查看每个数据点和其他数据点的偏差，另一种是看重构误差，先映射到低维再映射回高维，异常点的重构误差较大。这两种方法的本质一样，都是关注较小特征值对应的特征向量方向上的信息。可以利用PCA将二维的特征数据映射到一维上（尽量保持原始信息），然后再映射到二维空间，对比重构的二维数据和初始的二维数据的误差值（MSE），设定阈值，大于阈值的为异常，小于等于阈值的为正常。 模型构建过程： 原始数据： \\(x_{i} \\in R^{d}\\) 编码后的数据： \\(z_{i}=W^{T}\\left(x_{i}+b\\right) \\in R^{c}\\) 解码后的数据： \\(\\hat{x}_{i}=W z_{i}-b \\in R^{d}\\) 重构的误差： \\(\\sum_{i=1}^{n}\\left\\|x_{i}-\\hat{x}_{i}\\right\\|_{p}^{p}\\) 2.2 OneClass-SVM （1）与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。 （2）与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。 三. 非线性模型3.1 AutoEncoder非线性降维的代表方法是AutoEncoders，AutoEncoders的非线性和神经网络的非线性是一回事，都是利用堆叠非线性激活函数来近似任意函数。事实上，AutoEncoders就是一种神经网络，只不过它的输入和输出相同，真正有意义的地方不在于网络的输出，而是在于网络的权重。 模型构建过程： 原始数据： \\(x_{i} \\in R^{d}\\) 编码后的数据：\\(z_{i}=\\sigma\\left(W^{T} x_{i}+b\\right) \\in R^{c}\\) 解码后的数据： \\(\\hat{x}_{i}=\\hat{\\sigma}\\left(\\hat{W} z_{i}+\\hat{b}\\right) \\in R^{d}\\) 重构的误差： \\(\\sum_{i=1}^{n}\\left\\|x_{i}-\\hat{x}_{i}\\right\\|_{p}^{p}\\) 这里sigma是非线性激活函数。AutoEncoder一般都会堆叠多层（多层神经层同样也能增加模型的非线性），方便起见我们只写了一层。Autoencoder可以参与构建2种不同的异常检测模型。 （1）非线性降维：利用训练好的模型参数，通过输入到Encoder中，直接输出中间的CODE，而这个CODE就是数据经过非线性降维后的数据，然后利用降维后的数据放入到常用的聚类算法中（比如KMeans），搭建无监督异常检测模型。 （2）本身作为异常值的判别器：在训练好的模型之后，数据通过Encoder映射到低维空间后，利用Decoder重构回高维空间，而当输入数据是异常数据的时候，重构的高维数据会和原始数据的loss（MSE）很高，在这里设定一个阈值，大于阈值的为异常，小于等于阈值的为正常。3.2 VAE（Variational AutoEncoder）Variational AutoEncoder（VAE）是由 Kingma 和 Welling 在“Auto-Encoding Variational Bayes, 2014”中提出的一种生成模型。VAE其实可以看作AutoEncoder的一个变种，它在AutoEncoder区别在于在Encoder映射到低维空间中映射成了2个Vector，一个属于原始的CODE，而另一个作为噪声增加到新的CODE中。上图中，M_1，M_2，M_3作为Original Code，sigma_1，sigma_2，sigma_3这种变分的噪点则是通过模型自动训练学习得到的，e_1,e_2,e_3定义为服从一个标准的正太分布的向量，那么在AutoEncoder中的低维空间code在VAE中就是上图中的c_i了。code的定义公式如下：\\(c_{i}=\\exp \\left(\\sigma_{i}\\right) \\times e_{i}+m_{i}\\)当然，对于VAE来说，定义loss的function也和AutoEncoder不一样：\\(loss=\\sum_{i=1}^{3}\\left(\\exp \\left(\\sigma_{i}\\right)-\\left(1+\\sigma_{i}\\right)+\\left(m_{i}\\right)^{2}\\right)\\)VAE相较于AutoEncoder的优势在于：映射到低维空间的code增加了噪声，导致重构的高维空间中的值可以和原始数据不太一样，比如AutoEncoder模型原始输入是满月和斜月，那么输出一定是满月和斜月，而VAE则是可以生成满月和斜月的结合体。四. 基于划分超平面的模型——Isolation Forest4.1 孤立森林的思想是假设我们用一个随机超平面来切割（split）数据空间（data space）, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。4.2 孤立森林的优势孤立森林算法具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。4.3 孤立森林的劣势孤立森林不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。五. 基于聚类的异常检测模型5.1 常用的聚类算法 （1）KMeans：利用找到的簇的中心点和每一个样本的距离值，找到最偏离簇中心的点作为异常点。 （2）DBSCAN–基于密度的聚类：由于需要涉及到算法本身两个参数（min_samples和eps），这里模型会直接输出超过半径eps和确定好最小数的min_samples的样本点作为异常值（label = -1）。 （3）Birch–基于层次的聚类：BIRCH算法利用了一个树结构来帮助我们快速的聚类，这个数结构类似于平衡B+树，一般将它称之为聚类特征树(Clustering Feature Tree，简称CF Tree)。建立好CF Tree后把那些包含数据点少的MinCluster当作outlier。5.2 聚类算法的适应性每一种算法对于不同的数据分布可能存在不同优势。K-Means算法对于凸性数据具有良好的效果，能够根据距离来讲数据分为球状类的簇，但对于非凸形状的数据点，就无能为力了，比如环形数据等等，此时基于密度的算法DBSCAN就更令人满意了。5.2 评价聚类效果指标聚类算法的目标是：簇内相似度高，簇间相似度低。因此通过轮廓系数（Silhouette Coefficient）来评价聚类效果的好坏，适用于上述三种算法。1、将样本x与簇内的其他点之间的平均距离作为簇内的内聚度a2、将样本x与最近簇中所有点之间的平均距离看作是与最近簇的分离度b3、将簇的分离度与簇内聚度之差除以二者中比较大的数得到轮廓系数，计算公式如下\\(s^{(i)}=\\frac{b^{(i)}-a^{(i)}}{\\max \\left\\{b^{(i)}, a^{(i)}\\right\\}}\\)轮廓系数的取值在-1到1之间。当簇内聚度与分度离相等时，轮廓系数为0。当b»a时，轮廓系数近似取到1，此时模型的性能最佳。"
    } ,
  
    {
      "title"       : "Windows下安装C++ IDE（clion）和opencv环境",
      "category"    : "",
      "tags"        : "CV",
      "url"         : "./Windows%E4%B8%8B%E5%AE%89%E8%A3%85C++-IDE-clion-%E5%92%8Copencv%E7%8E%AF%E5%A2%83.html",
      "date"        : "2019-10-01 18:18:00 +0800",
      "description" : "讲述如何在Windows环境中安装clion和配置opencv环境",
      "content"     : "1. 下载软件 clion：C++的IDE cmake : 这里需要添加到环境变量中 D:\\Profile\\mingw64\\bin MinGW ：添加到环境变量 D:\\Profile\\mingw64\\bin opencv3.4.10：开源的计算机视觉库2. MinGW和OpenCV主要是如何用你的编译器来编译OpenCV。我们需要有include文件夹，这个在写代码时就用的到，还有lib和dll，这俩货我也不是很懂，dll的话没有是可以编译成功的，但运行是要失败的，所以我们是肯定要把dll加入到系统环境变量Path里的。lib是编译时就需要的，所以我们得把lib放在CLion的CMakeLists里面。下载完Windows的OpenCV，其实我们只有给Visual Studio用的dll和lib，可是我们想要g++来编译和运行，所以就得自己根据OpenCV的sources文件夹来自己编译OpenCV。 这里需要在cmake中加入OPENCV_ALLOCATOR_STATS_COUNTER_TYPE=int64_t，add Entry ==&gt; string，这里参考报错信息1 这里还需要再cmake中加入OPENCV_ENABLE_ALLOCATOR_STATS=OFF，参考报错信息2 需要2次Configure和1次Genrate即可编译完成。 cd opencv\\mingw-build目录下输入mingw32-make 等待完成，mingw32-make install 打开你的mingw-build文件夹，里面有个install目录就是你要的，可以复制一下这个文件夹，以后就不用重新编译了。我在C盘建立了OpenCV目录，并且把install文件夹下的文件复制进去了,C:\\OpenCV\\x64\\mingw\\bin加入系统环境变量Path中。3. 写CMakeList其实就是加入lib目录和include目录cmake_minimum_required(VERSION 3.16)project(opencv_test)set(CMAKE_CXX_STANDARD 14)add_executable(opencv_test main.cpp)## 添加的OpenCVConfig.cmake的路径set(OpenCV_DIR \"D:/Profile/opencv_builded\")## 搜索OpenCV目录find_package(OpenCV REQUIRED)## 添加OpenCV头文件目录include_directories(\"D:/Profile/opencv_builded/include\")## 链接OpenCV库文件target_link_libraries(opencv_test ${OpenCV_LIBS})4. 编译成可执行文件main.cpp文件中写完后，cd 项目目录，cmake .，即可看到项目中新加了文件夹cmake-build-debug中里面存在.exe可执行文件。"
    } ,
  
    {
      "title"       : "Linux下python安装和包管理",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./Linux%E4%B8%8Bpython%E5%AE%89%E8%A3%85%E5%92%8C%E5%8C%85%E7%AE%A1%E7%90%86.html",
      "date"        : "2019-06-20 04:20:00 +0800",
      "description" : "讲述在Linux环境下python包编译及安装过程，以及包管理工具virtualenv",
      "content"     : "1. 上传python文件并打包编译 下载python版本：https://www.python.org/ftp/python/ 解压：tar -xf Python-3..1.tgz 编译：sudo ./configure --prefix=/path/you/want/to/install/ --with-ssl &amp;&amp; make &amp;&amp; make install(这里需要加–prefix是因为可以直接在指定文件夹下删除软件即可，加入with ssl是由于pip需要ssl),在编译结束后，正常程序会装在 /usr/local/bin 下（注意这里如果不加–with-ssl默认安装的软件涉及到ssl的功能不可用） 创建软连接：ln -sf /usr/local/bin/python3.8 /usr/bin/python和ln -sf /usr/local/bin/python3.8-config /usr/bin/python-config2. venv管理和包安装 安装virtualenvs：pip3 install virtualenv 创建环境：sudo virtualenv --python=python3.6 环境名字 安装第三方包：进入环境下的bin目录，sudo ./pip3 install -r requirements.txt -i 指定的pip安装源 这里指定安装源较快。 3. 创建软连接ln -sf /usr/local/bin/python3.8 /usr/bin/pythonln -sf /usr/local/bin/python3.8-config /usr/bin/python-config4. 设置pip镜像源，下载提速之前利用pip进行安装的时候，要不是直接在pip下载的中途断掉，要不就是网速特别慢。这里推荐设置下国内的源进行pip下载。 临时使用的方式：pip install tensorflow -i 国内源国内源： 清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/这里最好不要一味的相信某一个源（比如清华源），吐槽下：下其他的包速度都很快，某些包的时候不仅慢，它还中途断掉！所以推荐最好每个都试试！ 永久配置某个源：这里就不需要再加-i 国内源linux：修改 ~/.pip/pip.confwindows：直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.inilinux和windows的具体内容都一致，如下：[global]index-url = 国内源[install]trusted-host=mirrors.aliyun.com"
    } ,
  
    {
      "title"       : "深度学习调参经验总结",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93.html",
      "date"        : "2019-03-09 18:20:00 +0800",
      "description" : "讲述在深度学习建模过程中遇到的问题及其解决思路",
      "content"     : "一. 网络中loss表现过于震荡1.1 模型拟合能力不够导致模型震荡model（层数：input(30,300,3) ==&gt; ful_collected_layer(30,300,64) ==&gt; lstm ==&gt; ful_collected_layer ==&gt; output）此时模型的loss由1到192震动太大，acc也是在一个epoch中时好时坏，由此考虑到是模型的分类能力的问题（可能处理不了非线性或者是异或的问题）\"\"\"增加了一层全连接层，之后效果显著，模型虽然也存在loss和acc会有极小幅度的震荡，但是趋向于收敛\"\"\"input(30,300,3) ==&gt; ful_collected_layer(30,300,64) ==&gt; lstm ==&gt; ful_collected_layer ==&gt; ful_collected_layer ==&gt; output1.2 batch size 设置过小导致模型震荡之前模型用的是batch_size = 30，经过增大batch_size之后，模型的震荡程度也减小。这里如果GPU显存小的情况下，只能将batch设置小。1.3 输入模型的数据没有shuffle导致模型震荡之前数据没有进行shuffle，导致在某一个batch_size中学习到的全是正样本，某一个batch_size里面又全是负样本，shuffle之后，振荡减小。np.random.seed(110) # 设定种子数，不然下面shuffle之后的y无法与X对应上np.random.shuffle(X)np.random.seed(110)np.random.shuffle(y)二. 网络经过多轮迭代依然无法上升了，acc始终在79%2.1 增大学习率开始学习率设置的是learning_rate = 0.001,之后增大10倍，设置为learning_rate = 0.01之后，acc在70多轮的时候就能提升到90% ，300轮之后能到97%。理由：当我们把学习率设置较小的时候，那么梯度下降的时候迈的步子就小，可能在遇到大的坑的时候就出去，然后就一致在坑里徘徊，最终只能达到局部最优，无法达到全局最优，调参的过程中应该首先实验大的学习率，然后再依次减小实验。learning_rate = 0.1 ==&gt; learning_rate = 0.01 ==&gt; learning_rate = 0.0012.2 优化器的选择实验下其他的梯度下降的优化器（optimizer），比如Adam，SGD，Adadelta，RMSProp，Momentum等，一般来说Adam较快，SGD最慢，但是却是最准确和稳定的，因此可以先用Adam进行实验，最后用SGD进行调参。tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=’Adadelta’)tf.train.MomentumOptimizer(learning_rate, momentum, use_locking=False, name=’Momentum’, use_nesterov=False)tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name=’Adam’)tf.train.GradientDescentOptimizer(learning_rate, use_locking=False,name=’GradientDescent’)三. 遇到loss和weights在训练中全是nan值的情况3.1 查看输入数据中是否存在nan值检查自己做完预处理的数据，看下是否存在nan值（比如需要计算0/0和log0的情况）\"\"\"检验下input_data中是否存在nan值\"\"\"input_data = np.array(input_data).reshape([-1,n_input])# 这里的input_data 是三维数组必须转成2dinput_data_pd = pd.DataFrame(input_data)if np.any(input_data_pd.isnull()) == True:print(\"input data has nan value!\")list_nan = list(map(tuple, np.argwhere(np.isnan(input_data_pd.values))))print(list_nan)3.2 梯度爆炸或者是梯度消失可能是梯度爆炸，有以下解决方式 （1）预训练+微调 （2）梯度剪切 + 权重正则 （3）使用不同的激活函数，比如之前用relu，可以换成tanh或者是elu （4）使用batchnorm （5）使用LSTM网络（如果之前用的是RNN结构） （6）使用残差结构以下是几种在不改变模型层数和结构的情况下解决梯度爆炸和梯度消失的方案。\"\"\"权重L2正则化\"\"\"cross_entropy = -tf.reduce_sum(ys * tf.log(tf.clip_by_value(tf.nn.softmax(prediction), 1e-10, 1.0)))weights_lossL2 = tf.add(tf.nn.l2_loss(weights_in),tf.nn.l2_loss(weights_out)) * 0.01regularzation_loss = cross_entropy + weights_lossL2cost = tf.reduce_mean(regularzation_loss)\"\"\"梯度剪裁\"\"\"opt = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.5)# Compute the gradients for a list of variables.grads_and_vars = opt.compute_gradients(cross_entropy, tf.trainable_variables())# grads_and_vars is a list of tuples (gradient, variable). Do whatever you# need to the 'gradient' part, for example cap them, etc.capped_grads_and_vars = [(tf.clip_by_value(gv[0], 0.1, 5.), gv[1]) for gv in grads_and_vars]# Ask the optimizer to apply the capped gradients.optimizer = opt.apply_gradients(capped_grads_and_vars)3.3 使用了梯度参见和L2正则之后出现Loss增大的情况在使用了梯度裁剪之后，其实只是人为的控制梯度的变化（将weights控制在小范围内(0.1,5)之间），此时权重依旧可以通过BP算法向负梯度的方向前进，但是由于人为的控制，导致weight的梯度极有可能朝着正梯度方向进行，这就会导致可以更新权重，但是loss反而增大的原因。3.4 这里必须要修改模型结构举一个例子：利用4层全连接层作为一个分类器，来训练。经历过以上所有的方式（包括调整激活函数等），依旧无法使得模型的loss减少，当我将层数降低为3层的时候，模型loss开始收敛，那么这就说明当无法使得模型收敛的时候，其实极有可能是模型的结构问题，需要重新设计模型的结构层数。四.训练中模型loss不收敛的几种情况总结： train loss 不断下降，val loss不断下降 ==&gt; 说明网络仍在学习 train loss 不断下降，val loss趋于不变 ==&gt; 说明网络过拟合 train loss 趋于不变，val loss不断下降 ==&gt; 说明数据集100%有问题 train loss 趋于不变，val loss趋于不变 ==&gt; 说明学习遇到瓶颈，需要减小学习率或批量数目 train loss 不断上升，val loss不断上升 ==&gt; 说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题 train loss 到稳定的时候反而比val loss还要高 ==&gt; 测试集数据量太小了，误差计算算法有问题 train loss 和 val loss趋于不变，但是val loss趋于0，而train loss却还很高 ==&gt; 说明使用dropout层后模型拟合能力变差，去掉dropout层。 train loss 和 val loss同时极缓的形式增大，这里可以考虑降低学习率或者是从这里进行截断，以loss最低点作为模型最优点。"
    } ,
  
    {
      "title"       : "机器学习常用函数",
      "category"    : "",
      "tags"        : "机器学习",
      "url"         : "./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0.html",
      "date"        : "2018-11-21 18:18:00 +0800",
      "description" : "介绍python的数据分析常用工具pandas、numpy和机器学习工具sklearn的总结",
      "content"     : "介绍python的数据分析常用工具pandas、numpy和机器学习工具sklearn的总结:一.数据处理工具pandas和numpy1.1 pandas读取数据data = pd.read_csv(\"data.csv\",index_col=False,encoding = \"utf-8\")data = pd.read_tabel(\"data.txt\",sep = \",\")1.2 numpy.random 生成数据的用法numpy.random.rand(4,2,3) ==&gt;生成4*2*3的矩阵，其中元素在[0,1)，floatnp.random.randn(4,2,3) ==&gt;生成4*2*3的矩阵，元素是标准正态分布（以0为均值、以1为标准差的正态分布，记为N（0，1）），floatnp.random.randint(-5,5,size=(2,2)) ==&gt;生成一个2*2的矩阵，元素值在[-5,5)随机整数，intnp.random.random_sample(size=(2,2)) ==&gt;生成一个2*2的矩阵，元素是[0,1),floatnp.random.seed(1676) ==&gt;设置种子数，每次生成的随机数相同1.3 pandas_profiling用于简单快速查看数据分布和得到数据报告。import pandas_profilingpfr = pandas_profiling.ProfileReport(data)pfr.to_file(\"./example.html\")1.4 pandas去掉重复项 df.drop_duplicates()df[df.duplicated()].shapedf = df.drop_duplicates()df.shape1.5 pandas中loc和iloc区别 loc利用index的名称（这里可以是index和行号不一致），来获取想要的行（或列）。（名称导向的这个特点，使得df[df.loc[‘col_name’] == ‘condition’, ‘col_name’] = value_1成立。具体的实际应用，可参考 代码案例 一步实现EXCEL的合并数据、数据筛选、数据透视功能。 iloc利用index的具体位置（所以它只能是整数型参数，行号），来获取想要的行（或列）。 # 这里loc就可以直接用自己写的索引来构成df.loc['C':'6', '3':, -1] # 利用iloc抽取指定位置（所在的行整数值）的索引所构成的新的dataframenew_dataframe = df.iloc[index_list,:]1.6 找到Nan值——np.isnan()nan_np_list = np.argwhere(np.isnan(np_data))1.7 哑变量生成——pd.get_dummies()dummy_device_type = pd.get_dummies(data_org['platform'],prefix='device_type')1.8 计算特征之间的相关系数自变量之间相关系数较大的话，需要考虑共线性的问题，共线性会导致模型出现开式解，降低模型的稳定性。常见方法有皮尔森相关系数和斯皮尔曼相关系数。两个系数都是介于-1和1之间，-1表示完全负相关，1表示完全正相关，0表示完全不相关。使用皮尔森相关系数有局限：要求数据是成对的从正态分布中取得的。而斯皮尔曼相关系数是一种秩相关系数，不受数据分布的影响，它通过对变量排序，以排序后两个变量的秩次差来计算相关系数。pearson = data.corr() # 适用于都是连续性变量spearman = data.corr('spearman') # 适用于离散和连续变量1.9 dataframe的拼接df_all_row = concat([df1,df2]) #等价于 df1.append(df2)，纵着拼接#等价于 merge(df1,df2,left_index=True,right_index=True,how='outer')df_all_col = concat([df1,df2],axis=1) # 横着拼接1.10 groupby的使用df = pd.DataFrame({'A': ['a', 'b', 'a', 'c', 'a', 'c', 'b', 'c'], 'B': [2, 8, 1, 4, 3, 2, 5, 9], 'C': [102, 98, 107, 104, 115, 87, 92, 123]})df.groupby('A').mean()==&gt; A B C a 2.0 108.000000b 6.5 95.000000c 5.0 104.6666671.11 Series.apply该函数用于对该series的所有元素进行处理生成一个新的series。new_series = pd.Series([i for i in range(10)]).apply(lambda x:x**2)\"\"\"这里是函数中带2个参数的\"\"\"def subtract_custom_value(x, custom_value): return x - custom_valuenew_series_2 = df.apply(subtract_custom_value, args=(5,))1.12 Series可以直接用于2个列相加减a = pd.Series([i for i in range(10)])b = pd.Series([i+1 for i in range(10)])diff_series = a - b1.13 sort_values将dataframe根据某一列顺序重新生成# 比如这里需要将整个dataframe根据时间戳的顺序（True）进行重新调整data = data.sort_values(by = ['timestamp']，ascending=True)1.14 concat 将Dataframe或者是Series进行合并data = pd.concat([x1,x2,x3],axis=1) # 这里是三个Series根据每个的index进行合并（按列）1.15 Series.reset_index 直接将Series转成Dataframedf = series.reset_index(name = 'index_name') # 这里给series的index设置名字为'index_name'，并变成一列1.16 df.groupby()的apply方式使用# 初始的dataframedf = pd.DataFrame({'A': 'a a b'.split(), 'B': [1,2,3], 'C': [4,6, 5]})g = df.groupby('A')print(g.apply(lambda x: x / x.sum())) B C0 0.333333 0.41 0.666667 0.62 1.000000 1.0print(g.apply(list)) B C0 [1,2] [3]1 [4,6] [5]1.17 df.groupby() 直接分成2个组的dataframedf = pd.DataFrame({'A': 'a a b'.split(), 'B': [1,2,3], 'C': [4,6, 5]})g = df.groupyby('A')for name,group in g: name_i = name df_i = group # 这里就是A只有a的dataframe二.绘图查看数据分布——seaborn和matplotlib2.1 频数分布直方图def plot_bar(x,y,color,title,width = 0.5): plt.figure() idx = np.arange(len(x)) plt.bar(idx,y,width,color = color) for xx, yy in zip(x, y): plt.text(xx, yy + 0.1, str('%.2f%%' % ((yy/np.array(y).sum())*100)), ha='center') plt.xticks(idx,x) plt.title(title) plt.xlabel('Hour') plt.ylabel('Trade-Frequence') plt.show()mu, sigma = 100, 15x = mu + sigma * np.random.randn(10000)# the histogram of the datan, bins, patches = plt.hist(x, 50, density=1, facecolor='g', alpha=0.75)plt.xlabel('Smarts')plt.ylabel('Probability')plt.title('Histogram of IQ')plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')plt.axis([40, 160, 0, 0.03])plt.grid(True)plt.show()2.2 散点图plt.plot(x,y)2.3 直线图plt.scatter(x,y)2.4 双变量分布图def two_dims_draw_relationship(df,xlabel,ylabel): # 双变量分布 x,ylabel是字符串，df必须是双维度的dataframe sns.set(color_codes=True) g = sns.jointplot(x=xlabel, y=ylabel, data=df, kind=\"kde\", color=\"y\") g.plot_joint(plt.scatter, c=\"m\", s=30, linewidth=1, marker=\"+\") g.ax_joint.collections[0].set_alpha(0) # 画背景网格线 g.set_axis_labels(\"${}$\".format(xlabel), \"${}$\".format(ylabel)) plt.show()2.5 多变量两两之间的分布图def all_two_feature_distribution(df = sns.load_dataset('iris')): sns.set(style=\"ticks\") sns.pairplot(df, hue=\"species\") plt.show()2.6 热度图scores_h = pd.DataFrame(np.array(scores_h).reshape(18, 3))scores_h.index = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]scores_h.columns = [1,2,3]plt.figure()ax = sns.heatmap(scores_h, annot=True,xticklabels=cols_all_features, yticklabels=cols_all_features,fmt='.2f')plt.ylabel(\"N_cluster\")plt.xlabel(\"linkages\")# 必须先savefig，之后再show，就不会出现保存时白色图片了plt.savefig(\"h.jpg\")plt.show()三.机器学习包sklearn3.1 划分数据集from sklearn.model_selection import train_test_split(trainX,testX,trainY,testY) = train_test_split(X,y,test_size=0.2,random_state=42)3.2 特征标准化 标准化：使用scale模块直接计算标准化，将标准化的array放在x_scale中，同时可以查看均值和标准差，但是该方式的一个不足是当存在新的样本到来时，无法利用已有的模块直接复用，需要利用mean和std自己计算。 x_scale = preprocessing.scale(DatMat) x_scale.mean(axis=0)x_scale.std(axis=0) 归一化：使用StandardScaler模块计算标准化，可以利用训练集数据建立一个转化的类，类似于实现将mean和std存储在该类中，将数据输入，就可以直接求出结果。 scaler = preprocessing.StandardScaler().fit(datingDatMat)datingDatMat = scaler.transform(datingDatMat)new_date = numpy.array([1, 2, 3])new_date_std = scaler.transform(new_date.reshape(1, -1)) 这里的scaler更象是扮演一个计算器的角色，本身并不存储数据。 3.3 交叉验证——查验模型稳定性from sklearn.model_selection import cross_val_score model_stability = cross_val_score(model, trainX,trainY,cv=10,scoring=\"accuracy\")mean_score_model = model_stability.mean()3.4 模型评估方式——混淆矩阵from sklearn.metrics import confusion_matrixprint(confusion_matrix(testY,y_pred))3.5 分类结果展示（准确性，召回率，f1-score）from sklearn.metrics import classification_reportprint(confusion_matrix(testY,y_pred))3.6 模型的保存和加载import pickle# 保存模型pickle.dump(rf_clf, open('model/model.model', 'wb'))# 导入模型model = pickle.load(open('model_save/model.model','wb'))3.7 数据样本不均衡——SMOTEENNfrom imblearn.combine import SMOTEENNsmote_enn = SMOTEENN(random_state=42)X_resampled,y_resampled = smote_enn.fit_sample(X,y)3.8 自动超参调节 自动调参——GridSearchCVfrom sklearn.model_selection import GridSearchCVrcl=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features='sqrt',random_state=10)# 现在不确定RandomForest其中一个参数n_estimators的个数param_test1 = {'n_estimators':range(10,71,10)}gsearch1= GridSearchCV(estimator =rcl,param_grid= param_test1,scoring='roc_auc',cv=5)gsearch1.fit(X,y)print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_) 自动调参神器——Hyperoptfrom hyperopt import fmin, tpe, hp, randimport numpy as npfrom sklearn.metrics import accuracy_scorefrom sklearn import svmfrom sklearn import datasets# SVM的三个超参数：C为惩罚因子，kernel为核函数类型，gamma为核函数的额外参数（对于不同类型的核函数有不同的含义）# 有别于传统的网格搜索（GridSearch），这里只需要给出最优参数的概率分布即可，而不需要按照步长把具体的值给一个个枚举出来parameter_space_svc ={ # loguniform表示该参数取对数后符合均匀分布 'C':hp.loguniform(\"C\", np.log(1), np.log(100)), 'kernel':hp.choice('kernel',['rbf','poly']), 'gamma': hp.loguniform(\"gamma\", np.log(0.001), np.log(0.1)),}# 鸢尾花卉数据集，是一类多重变量分析的数据集# 通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类iris = datasets.load_digits()#--------------------划分训练集和测试集--------------------train_data = iris.data[0:1300]train_target = iris.target[0:1300]test_data = iris.data[1300:-1]test_target = iris.target[1300:-1]#-----------------------------------------------------------# 计数器，每一次参数组合的枚举都会使它加1count = 0def function(params): c = params[\"C\"] kernel = params[\"kernel\"] gamma = params[\"gamma\"] # **可以把dict转换为关键字参数，可以大大简化复杂的函数调用 clf = svm.SVC(C=c,kernel = kernel,gamma = gamma) # 训练模型 clf.fit(train_data,train_target) # 预测测试集 prediction = clf.predict(test_data) global count count = count + 1 score = accuracy_score(test_target,prediction) print(\"第%s次，测试集正确率为：\" % str(count),score) # 由于hyperopt仅提供fmin接口，因此如果要求最大值，则需要取相反数 return -score# algo指定搜索算法，目前支持以下算法：# ①随机搜索(hyperopt.rand.suggest)# ②模拟退火(hyperopt.anneal.suggest)# ③TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）# max_evals指定枚举次数上限，即使第max_evals次枚举仍未能确定全局最优解，也要结束搜索，返回目前搜索到的最优解best = fmin(function, parameter_space_svc, algo=tpe.suggest, max_evals=100)# best[\"kernel\"]返回的是数组下标，因此需要把它还原回来kernel_list = ['rbf','poly']best[\"kernel\"] = kernel_list[best[\"kernel\"]]print(\"最佳参数为：\",best)clf = svm.SVC(**best)print(clf)3.9 to_categorical功能：将label转为one_hot形式，源于keras.utils包from keras.utils import to_categoricaly_onehot = to_categorical(y,num_classes(总类别数))3.10 绘制ROC曲线from sklearn.metrics import roc_curve, aucdef draw_ROC_curve(y_test, y_predict, savepath): '''画ROC曲线''' false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict) roc_auc = auc(false_positive_rate, true_positive_rate) plt.title('ROC') plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f' % roc_auc) plt.legend(loc='lower right') plt.plot([0, 1], [0, 1], 'r--') plt.ylabel('TPR') plt.xlabel('FPR') plt.savefig(savepath) plt.show() plt.close(0)四.分类模型4.1 随机森林from sklearn.ensemble import RandomForestClassifierrf_clf = RandomForestClassifier(n_estimators=700,n_jobs=-1,max_leaf_nodes=30)rf_clf.fit(trainX,trainY)y_pred = rf_clf.predict(testX)# 特征重要性feature_importance = rf_clf.feature_importances_4.2.Xgboost优势：表现快，训练时可以用所有的 CPU 内核来并行化建树；用分布式计算来训练非常大的模型；对于非常大的数据集还可以进行 Out-of-Core Computing)。参数：learning_rate ＝ 0.1 或更小，越小就需要多加入弱学习器；tree_depth ＝ 2～8；subsample ＝ 训练集的 30%～80%。from xgboost import XGBClassifiermodel = XGBClassifier()model.fit(X_train, y_train)y_pred = model.predict(X_test)# 可以在每加入一颗树后打印出 loglosseval_set = [(X_test, y_test)]model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)# 输出特征重要性from xgboost import plot_importancefrom matplotlib import pyplotplot_importance(model)pyplot.show()4.3 SVMfrom sklearn.svm import SVCclf = SVC(gamma='auto',C,kernel = \"RBF\")clf.fit(X, y)y_pred = clf.predict(testX)五.聚类模型5.1.轮廓系数评估聚类效果好坏——轮廓系数（Silhouette Coefficient）结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。该值处于-1~1之间，值越大，表示聚类效果越好。#聚类评估：轮廓系数from sklearn.metrics import silhouette_score# Kmeans的聚类结果来进行测试labels = KMeans(n_clusters=k).fit(data).labels_score = silhouette_score(data, labels)"
    } 
  
]
