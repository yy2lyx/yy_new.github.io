[
  
    {
      "title"       : "面试基础总结",
      "category"    : "",
      "tags"        : "面试",
      "url"         : "./%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93.html",
      "date"        : "2021-12-13 05:11:00 +0800",
      "description" : "记录算法面试中遇到的基础问题和过程记录",
      "content"     : "一. 操作系统1.1 并行和并发并发：在操作系统中，某一时间段，几个程序在同一个CPU上运行，但在任意一个时间点上，只有一个程序在CPU上运行。并行：当操作系统有多个CPU时，一个CPU处理A线程，另一个CPU处理B线程，两个线程互相不抢占CPU资源，可以同时进行，这种方式成为并行。知乎的例子： 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。1.2 进程和线程1.2.1 进程​ 进程是资源分配的基本单位，理解为一个程序。所以我们一般都要求进程池的进程数小于等于CPU的核心数。​ 如果问单核CPU能否运行多进程？答案又是肯定的。单核CPU也可以运行多进程，只不过不是同时的，而是极快地在进程间来回切换实现的多进程。进程拥有自己的地址空间，全局变量，文件描述符，各种硬件等等资源。1.2.2 线程​ 线程：线程是依赖于进程的。如果说进程和进程之间相当于程序与程序之间的关系，那么线程与线程之间就相当于程序内的任务和任务之间的关系。​ 一个程序内包含了多种任务。加上了线程之后，线程能够共享进程的大部分资源，并参与CPU的调度。意味着它能够在进程间进行切换，实现并发。1.2.3 为什么要用多进程，适用条件​ 总是在运行一个进程上的任务，就会出现一个现象。就是任务不一定总是在执行 ”计算型“ 的任务，会有很大可能是在执行网络调用，阻塞了，CPU 岂不就浪费了？因此，多进程适用于CPU密集型任务(各种循环处理、计算等等)。多线程适用于IO密集型任务(文件处理、网络爬虫等)。1.2.4 多进程通信管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。类似于python的multiprocessing.Pipe()消息队列MessageQueue：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。类似于python的multiprocessing.Queue()共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。1.2.5 python的多线程是真的多线程么​ Python在设计之初就考虑要在主循环中，同时只有一个线程在执行，就像单CPU的系统中运行多个进程那样，内存中可以存放多个程序，但任意时刻，只有一个程序在CPU中运行。同样地，虽然Python解释器可以运行多个线程，只有一个线程在解释器中运行。​ Python虚拟机的访问由全局解释器锁（GIL）来控制，正是这个锁能保证同时只有一个线程在运行。Python的多进程多线程测试： 在一个4核CPU上开4线程，发现电脑的CPU利用率没有占满，大致相当于单核水平。 在一个4核CPU上开4进程，发现CPU直接飙到了100%，说明进程是可以利用多核的！ Python多线程相当于单核多线程，多线程有两个好处：CPU并行，IO并行，单核多线程相当于自断一臂。所以，在Python中，可以使用多线程，但不要指望能有效利用多核。 1.2.6 多线程如何保证线程安全当多个线程同时操作同一个共享全局变量的时候，就容易出现线程安全问题，线程安全问题只会影响到线程对同一个共享的全局变量的写操作。利用线程锁来保证同一个时刻，有且仅有一个线程对共享的全局变量进行写操作。且开始写操作前，需要加锁，完成后，需要解锁，让其他线程再对其进行写操作。1.2.7 死锁问题​ 死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。死锁的4个必要条件： 互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放。 请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。 不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞如何避免死锁？只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件：一次性申请所有的资源。 破坏不剥夺条件：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。1.3 同步和异步1.3.1 阻塞和非阻塞 阻塞：指调用线程或者进程被操作系统挂起。 非阻塞：指调用线程或者进程不会被操作系统挂起。 1.3.2 同步和异步同步是阻塞模式，异步是非阻塞模式。 同步：指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去。 异步：指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回式系统会通知进程进行处理，这样可以提高执行的效率。1.4 协程协程是一种用户态的轻量级线程。子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。例子：def A(): print('1') print('2') print('3')def B(): print('x') print('y') print('z')假设由协程执行，在执行A的过程中，可以随时中断，去执行B，B也可能在执行过程中中断再去执行A，结果可能是：1 2 x y 3 z但是在A中是没有调用B的，看起来A、B的执行有点像多线程，但协程的特点在于是一个线程执行协程的优势： 极高的执行效率：因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 不需要多线程的锁机制：因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。1.4 内存分配管理1.4.1 分段和分页分段：在段式存储管理中，将程序的地址空间划分为若干段（segment），如代码段，数据段，堆栈段；这样每个进程有一个二维地址空间，相互独立，互不干扰。分页：在页式存储管理中，将程序的逻辑地址划分为固定大小的页（page），而物理内存划分为同样大小的帧，程序加载时，可以将任意一页放入内存中任意一个帧，这些帧不必连续，从而实现了离散分离。1.4.2 分段和分页不同点 段是信息的逻辑单位，它是根据用户的需要划分的，因此段对用户是可见的 ；页是信息的物理单位，是为了管理主存的方便而划分的，对用户是透明的。 段的大小不固定，有它所完成的功能决定；页大大小固定，由系统决定 段向用户提供二维地址空间；页向用户提供的是一维地址空间 段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制。 段式管理的优点是没有内碎片（因为段大小可变，改变段大小来消除内碎片）；页式存储管理的优点是没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）1.4.3 堆和栈对于一个由C/C++编译的程序，其所占用的内存可以划分为以下几个部分： 栈区（stack）—— 由操作系统自动分配和释放，主要用于存放函数参数值，局部变量等。其操作方式类似于数据结构中的栈。 堆区（heap）—— 一般由程序员动态分配和释放(malloc，free)，若程序员不主动释放，则程序结束后由操作系统回收。注意，它与数据结构中的堆是不同的，分配方式类似于链表。 BSS段——主要用于存放未初始化的静态变量和全局变量，可读写，它在程序结束后由操作系统进行释放。 数据段（data）——主要用于存放已初始化的静态变量和全局变量，可读写，它在程序结束后由操作系统释放。 代码段（text）——主要用于保存程序代码，包括CPU执行的机器指令，同时全局常量也是保存在代码段的，如字符串字面值。1.4.4 一个进程的内存结构/main.cppint a = 0; // 全局初始化区域char *p1; // 全局未初始化区域int main(){ int b; // 栈 char s[] = \"adoryn\"; // 栈 char *p2; // 栈 char *p3 = \"zhaobryant\"; // 字符串字面量存放在常量区，p3存放在栈上 static int c = 0; // 全局（静态）初始化区域 p1 = (char *)malloc(10); p2 = (char *)malloc(20); // 分配获得的10和20字节的内存区放在堆区 strcpy(p1, \"zhaobryant\"); // 字符串字面量存放在常量区，编译器可能会将它与p3所指向的\"zhaobryant\"优化为同一个地址 return 0;}二. linux相关2.1 查看CPU和内存占用情况 查看物理内存使用情况：free 查看目前正在运行的进程所占的内存百分比和CPU百分比：top。假如某个进程显示400%，说明该程序利用了4核CPU。 查看磁盘使用情况：df 查看当前进程：ps，ps -aux则是当前所有的正在内存当中的程序 2.2 查找匹配 在某个路径下查找是否有某个文件：find 路径 -name 文件名 根据字符串查找相应匹配的文件：grep。它是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 查找后缀有 py 字样的文件中包含 print 字符串的文件（当前路径下）：grep print *.py 递归查找符合条件的文件，这里是查找指定目录（git_repo）下及其子目录所有文件中包含helloworld字符串的文件 联合ps检查当前进程中python3进程是否运行：ps -aux | grep python3。其中|是管道命令 是指ps命令与grep同时执行。 2.3 网络相关 显示各种网络相关信息：netstat 列出所有处于监听状态的Sockets netstat -l # 只显示监听端口netstat -lt #只列出所有监听 tcp 端口netstat -lu #只列出所有监听 udp 端口netstat -lx #只列出所有监听 UNIX 端口 配合grep查找tcp下的指定端口号信息 netstat -plnt | grep :53# -p 输出中显示 PID 和进程名称# -l 仅列出有在 Listen (监听) 的服务状态# -n 不想让主机，端口和用户名显示，而是用ip显示# -t 只列出所有监听 tcp 端口 2.4 select、poll和epoll用户空间和内核空间操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间（供内核使用），一部分为用户空间（供进程使用）。进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。进程的切换需要经过以下变换： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。进程阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。文件描述符fd文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。缓存IO在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。Socketsocket是一种”打开—读/写—关闭”模式的实现，服务器和客户端各自维护一个”文件”，在建立连接打开后，可以向自己文件写入内容供对方读取或者读取对方内容，通讯结束时关闭文件。IO多路复用IO multiplexing就是我们说的select，poll，epoll。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。select调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024poll不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。epoll相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。epoll对文件描述符的操作有两种模式，LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：　　 LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。　　 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。epoll的优点主要是以下个方面： 监视的描述符数量不受限制 IO的效率不会随着监视fd的数量的增长而下降2.5 TCP三握四挥三次握手如图所示，双方之间的三个蓝色箭头就表示了三次握手过程中所发生的数据交换： 第一次握手：客户端向服务器发送报文段1，其中的 SYN 标志位 (前文已经介绍过各种标志位的作用)的值为 1，表示这是一个用于请求发起连接的报文段，其中的序号字段 (Sequence Number，图中简写为seq)被设置为初始序号x (Initial Sequence Number，ISN)，TCP 连接双方均可随机选择初始序号。发送完报文段1之后，客户端进入 SYN-SENT 状态，等待服务器的确认。 第二次握手：服务器在收到客户端的连接请求后，向客户端发送报文段2作为应答，其中 ACK 标志位设置为 1，表示对客户端做出应答，其确认序号字段 (Acknowledgment Number，图中简写为小写 ack) 生效，该字段值为 x + 1，也就是从客户端收到的报文段的序号加一，代表服务器期望下次收到客户端的数据的序号。此外，报文段2的 SYN 标志位也设置为1，代表这同时也是一个用于发起连接的报文段，序号 seq 设置为服务器初始序号y。发送完报文段2后，服务器进入 SYN-RECEIVED 状态。 第三次握手：客户端在收到报文段2后，向服务器发送报文段3，其 ACK 标志位为1，代表对服务器做出应答，确认序号字段 ack 为 y + 1，序号字段 seq 为 x + 1。此报文段发送完毕后，双方都进入 ESTABLISHED 状态，表示连接已建立。常见面试题 1： TCP 建立连接为什么要三次握手而不是两次？ 防止已过期的连接请求报文突然又传送到服务器，因而产生错误 三次握手才能让双方均确认自己和对方的发送和接收能力都正常 告知对方自己的初始序号值，并确认收到对方的初始序号值常见面试题2： TCP 建立连接为什么要三次握手而不是四次？ 相比上个问题而言，这个问题就简单多了。因为三次握手已经可以确认双方的发送接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值得确认，也就无需再第四次握手了。四次挥手建立一个连接需要三次握手，而终止一个连接要经过 4次握手。这由 TCP 的半关闭( half-close) 造成的。既然一个 TCP 连接是全双工 (即数据在两个方向上能同时传递)， 因此每个方向必须单独地进行关闭。四次挥手详细过程如下： 客户端发送关闭连接的报文段，FIN 标志位1，请求关闭连接，并停止发送数据。序号字段 seq = x (等于之前发送的所有数据的最后一个字节的序号加一)，然后客户端会进入 FIN-WAIT-1 状态，等待来自服务器的确认报文。 服务器收到 FIN 报文后，发回确认报文，ACK = 1， ack = x + 1，并带上自己的序号 seq = y，然后服务器就进入 CLOSE-WAIT 状态。服务器还会通知上层的应用程序对方已经释放连接，此时 TCP 处于半关闭状态，也就是说客户端已经没有数据要发送了，但是服务器还可以发送数据，客户端也还能够接收。 客户端收到服务器的 ACK 报文段后随即进入 FIN-WAIT-2 状态，此时还能收到来自服务器的数据，直到收到 FIN 报文段。 服务器发送完所有数据后，会向客户端发送 FIN 报文段，各字段值如图所示，随后服务器进入 LAST-ACK 状态，等待来自客户端的确认报文段。 客户端收到来自服务器的 FIN 报文段后，向服务器发送 ACK 报文，随后进入 TIME-WAIT 状态，等待 2MSL(2 * Maximum Segment Lifetime，两倍的报文段最大存活时间) ，这是任何报文段在被丢弃前能在网络中存在的最长时间，常用值有30秒、1分钟和2分钟。如无特殊情况，客户端会进入 CLOSED 状态。 服务器在接收到客户端的 ACK 报文后会随即进入 CLOSED 状态，由于没有等待时间，一般而言，服务器比客户端更早进入 CLOSED 状态。三. C++3.1 STL中list,vector,map,set区别vector vector封装了数组，拥有一段连续的内存空间，并且起始地址不变。 存取复杂度：能实现高效的随机存取，其时间复杂度为O(1)。 增删复杂度：由于内存空间是连续的，因此插入和删除时，会造成内存块的拷贝，时间复杂度为O(n)。 特别是当数组空间不足时，会重新申请一块内存空间（2倍）并进行拷贝。 vector支持根据下标随机存取元素。其原因是“循秩访问”这种向量特有的元素访问方式。list list 封装了链表，且是双向链表。因此内存空间是不连续的。 每一个结点都维护一个信息块、一个前向指针和一个后向指针，因此支持前向/后向遍历。 存取复杂度：只能通过指针访问数据，所以list的随机存取非常没有效率，时间复杂度为o(n)。原因是存储的对象是离散的，随机访问需要遍历整个链表 增删复杂度：在已经定位到要增删元素的位置的情况下，增删元素能在常数时间内完成，即O(1) 。可以不分配必须的内存大小方便的进行添加和删除操作。 当要存储的是大型负责类对象时，list要优于vector。 list 容器不支持根据下标随机存取元素。不支持[ ]操作符和vector.at()。 map map的本质其实就是映射，键值（key-value）一一对应。 map的实现是一颗红黑树，因此，map的内部键的数据都是排好序的，查找和删除、插入的效率都是n(logn)。 multimap 允许一个键对应多个值；而map则是一个键对应一个值。set 所得元素的只有key没有value，value就是key。不允许键值的重复，即在set中每个元素的值都唯一。 set的实现是一颗红黑树，因此，set的内部键的数据都是排好序的，查找和删除、插入的效率都是n(logn)。 set不能通过迭代器修改set元素值，其原因是set的值就是键。3.2 虚函数虚函数使用条件​ 当基类指针指向一个子类对象，通过这个指针调用子类和基类同名成员函数的时候，基类声明为虚函数「子类不写也可以」就会调子类的这个函数，不声明就会调用基类的。关键字：virtual构造和析构可以是虚函数么​ 构造不能为虚函数：如果构造函数是虚函数，那么就需要通过vtable（虚函数指针表） 来调用，但此时面对一块 raw memeory，到哪里去找 vtable 呢？毕竟，vtable 是在构造函数中才初始化的啊，而不是在其之前。因此构造函数不能为虚函数。​ 析构为虚函数：此时 vtable 已经初始化了；况且我们通常通过基类的指针来销毁对象，如果析构函数不为虚的话，就不能正确识别对象类型，从而不能正确销毁对象。虚函数可以是内联函数(inline)么 内联函数：由于函数调用耗时长，因此在编译时将所调用的函数的代码直接嵌入到主调函数中，可以理解为内联函数体代码直接替换了调用函数这行代码。 虚函数可以是内联函数，内联是可以修饰虚函数的，但是当虚函数表现多态性的时候不能内联。 内联是在编译器建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时（运行期）不可以内联。3.3 继承和多态C++有其三大特性：封装、继承和多态。继承​ 以让某个类型的对象，获得另一种类型对象属性的方法。实际上就是类与类之间可以共用代码，实现代码重用。可以理解为子类继承父类的方法。​ 例子：父类为Animal，其仅有2个成员函数，分别为bark()和eat()。子类为Dog，继承了父类的bark()和eat()，同时它还有自己的另一个方法run()。多态​ 多种形态，指一个类实例的相同方法在不同情况下有不同表现形式。​ 即子类可以重写父类的某个函数，从而为这个函数提供不同于父类的行为。一个父类的多个子类可以为同一个函数提供不同的实现，从而在父类这个公共的接口下，表现出多种行为。​ 例子：父类为Animal，其仅有2个成员函数，分别为bark()和eat()。子类为Dog，Cat，Snake，对于同一个方法bark()而言，不同的子类存在不同的喊叫方式，即为多态。区别 多态的实现要求必须是共有继承。 继承关系中，并不要求基类方法一定是虚函数。而多态时，要求基类方法必须是虚函数。 多态：子类重写父类的方法，使得子类具有不同的实现。且运行时，根据实际创建的对象动态决定使用哪个方法。四. 数据结构和算法4.1 排序算法排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。这里比如冒泡、插入、快排、归并、堆排序等。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 这里比如计数排序和位图排序。算法复杂度 排序方法 时间复杂度（平均） 时间复杂度（最坏） 时间复杂度（最好） 空间复杂度 稳定性 冒泡排序 稳定 插入排序 稳定 快速排序 不稳定 归并排序 稳定 堆排序 不稳定 计数排序 稳定 位图排序 稳定 冒泡排序 Bubble sort​ 它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。算法步骤： 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。插入排序 Insertion Sort​ 它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。算法步骤： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。快速排序 Quick Sort​ 通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。算法步骤： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。归并排序 Merge Sort​ 该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。算法步骤： 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。堆排序 Heap Sort​ 利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点，大顶堆和小顶堆。算法步骤： 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。计数排序 Counting Sort​ 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。算法步骤： 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。位图排序 Bitmap Sort​ 位图排序是一种效率极高(复杂度可达O(n))并且很节省空间的一种排序方法，但是这种排序方法对输入的数据是有比较严格的要求(数据不能重复，大致知道数据的范围)。 位图排序即利用位图或者位向量来表示集合。参考的面试题：40亿个QQ号码，如果使用O(1)的时间复杂度去查找一个QQ号是否存在。算法步骤： 根据待排序集合中最大的数，开辟一个位数组，用来表示待排序集合中的整数。 待排序集合中的数字在位数组中的对应位置置1，其他的置0。 将对应序号的整数放置到位对应的index上，并在每个对应的位上置位1. 检验每一位，如果该位位1，输出对应的整数。4.2 红黑树二叉查找树(BST)特性： 左子树上所有结点的值均小于或等于它的根结点的值。 右子树上所有结点的值均大于或等于它的根结点的值。 左、右子树也分别为二叉排序树。 根据二分查找的思想，查找的最大次数等同于树的高度。但是二叉查找树有其自身的缺陷：在多次出入新的节点后可能会出现不平衡现象，如下图。红黑树（RBT）红黑树是一种自平衡的二叉查找树。其除了具有二查找树的特性外，还具有以下特性： 节点是红色或黑色。 根节点是黑色。 每个叶子节点都是黑色的空节点（NIL节点）。 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。如下图插入一个14的新节点。红黑树实现自平衡的三种操作： 左旋：逆时针旋转红黑树的两个节点，使得父节点被自己的右孩子取代，而自己成为自己的左孩子。 右旋：顺时针旋转红黑树的两个节点，使得父节点被自己的左孩子取代，而自己成为自己的右孩子。 变色 4.3 回溯算法递归实现-伪代码算法 ReBack(k)if k&gt;n: then &lt;x1,x2...xn&gt;是解else: while S_k != 空集 do 找到S_k中的最小值x_k S_k = S_k - {x_k} 计算S_k+1 ReBack(k+1)迭代实现-伪代码迭代算法 Backtrack输入：n输出：所有的解1. 对于i=1,2,...n 确定x_i # 确定初始值2. k = 13. 计算S_k4. while S_k != 空集 do # 满足约束分支搜索 找到S_k中的最小值x_k S_k = S_k - {x_k} if k &lt; n then k = k+1 计算S_k else &lt;x1,x2...xn&gt;是解5. if k&gt;1 then k = k-1 ; goto4 # 回溯四后问题 问题描述：在4 * 4 的方格棋盘中放置4个皇后，使得没有2个皇后在同一行、同一列、也不在45°的斜线上。问有多少种可能的布局？解是4维向量&lt;x_1,x_2,x_3,x_4&gt;，解为：&lt;2,4,1,3&gt;，&lt;3,1,4,2&gt;其搜索空间是一个4叉树，如下图。 每个结点有4个儿子，分别代表1，2，3，4列位置。 第 i 层选择解向量中第 i 个分量的值 最深层的树叶是解。 按深度优先次序便利树，找到所有的解。0-1背包问题 问题描述：有n种物品，每种物品只有1个。第 i 种物品价值为v_i，重量位w_i，i = 1,2,…,n。问如何选择放入背包的物品，使得总重量不超过B，而价值达到最大？解：n维 0-1 向量&lt;x_1,x_2,…x_n&gt;节点：&lt;x_1,x_2,…x_k&gt;搜索空间： 0-1 取值的二叉树，称为子集树，有2^n 片树叶。可行解：满足约束条件\\(\\sum_{i=1}^{n} w_{i} x_{i} \\leq B\\)最优解：可行解中价值达到最大的解实例：V = {12,11,9,8}，W = {8,6,4,3},B = 13货郎问题 问题描述：有n个城市，已知任两个城市之间的距离，求一条每个城市恰好经过一次的回路，使得总长度最小。实例：City = {1,2,3,4},d(1,2) = 5,d(1,3) = 9,d(1,4) = 4,d(2,3) = 13,d(2,4) = 2,d(3,4) = 7。解：&lt;1,2,4,3&gt; 长度 = 5 + 2 + 7 + 9 = 23搜索空间：排列树，每层有 (n-1)! 片树叶"
    } ,
  
    {
      "title"       : "Jetson Xavier NX 的使用记录",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./JetsonXavierNX%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95.html",
      "date"        : "2021-08-25 05:11:00 +0800",
      "description" : "记录使用Jetson Xavier NX的使用体验中遇到的问题和记录",
      "content"     : "一. 远程桌面 在windows10远程上操作jetson Xavier，远程的前提：jetson xavier和Windows的PC在同一个局域网内（我这里是直接在windows10上开启热点）。 安装xrdp：sudo apt-get install xrdp vnc4server xbase-clients1.1：桌面共享没反应 桌面共享其实就是一个vnc-server（因此没有必要再在linux上安装vnc-server了），如果要远程，必须要先开启共享，允许其他人控制自己电脑。这里发现双击了桌面共享，没反应。解决方案： 安装dconf-editor：sudo apt-get install dconf-editor 运行dconf-editor，更改系统配置，org ==&gt; gnome ==&gt; desktop ==&gt; remote-access，关闭以下两个：promotion-enabled和requre-encryption 开启桌面共享：/usr/lib/vino/vino-server1.2 开启远程 在Windows10上安装vnc-client，官网地址：https://www.realvnc.com/en/connect/download/viewer/windows/ 输入linux的ip后，直接连接即可。 1.3 建立双击可执行文件.desktop 由于每次我开启远程桌面的时候，都需要在命令行输入相关指令，很麻烦，就想说有没有可以直接在像windows一样快捷方式 写一个shell脚本来开启桌面共享：vim ~/vnc-server.sh #!/bin/sh/usr/lib/vino/vino-server 在桌面上新建一个.desktop文件：vim ~/Desktop/vnc-server.desktop [Desktop Entry]Encoding=UTF-8Type=ApplicationCategories=trueVersion=1.0Name=vnc-serverExec=sh /home/yy/vnc-server.sh #注意这里是绝对路径Path=/home/yyTerminal=false # 是否保留终端StartupNotify=true # 开机自启动 给.desktop文件加上可执行权限：sudo chmod +x ~/Desktop/vnc-server.desktop 1.4 开机自动开启 创建开机自启动文件夹：mkdir ~/.config/autostart 复制.desktop文件到该文件夹下：cp ~/Desktop/vnc-server.desktop ~/.config/autostart/ 给.desktop文件加上可执行权限：sudo chmod +x ~/.config/autostart/vnc-server.desktop二. 配置cuda和cuDNN 用JetPack刷机（本人选用的是Jetpack4.4.1），jetpack地址：https://developer.nvidia.com/embedded/jetpack-archive 安装完成后，我们可以查看jetpack的版本：cat /etc/nv_tegra_release，同时已经给我们安装好了CUDA 10.2 、cudnn 8.0 、opencv、python3.6。2.1 设置cuda环境变量​ 查询cuda版本：nvcc -V基本就能看到cuda信息了，但是这里却报错没有nvcc指令。解决方案：将cuda添加到环境变量中 sudo vim /etc/profile ，这里说下其实在/usr/local下有2个cuda相关文件夹，分别是cuda和cuda-10.2，你会发现其实是一致的。 export PATH=/usr/local/cuda-10.2/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64$LD_LIBRARY_PATHexport CUDA_HOME=/usr/local/cuda-10.2 source /etc/profile，再去命令行输入nvcc -V就可以看到cuda10.2的信息了 2.2 设置cuDNN安装好的cuDNN的头文件：/usr/include/cudnn.h安装好的cuDNN的库文件：/usr/lib/aarch64-linux-gnu/libcudnn*(1) 而这些头文件和库文件都不在cuda目录下，因此要复制到cuda目录下： 复制头文件：sudo cp /usr/include/cudnn.h /usr/local/cuda/include 复制库文件：sudo cp /usr/lib/aarch64-linux-gnu/libcudnn* /usr/local/cuda/lib64/(2) 修改文件权限：sudo chmod 777 /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*(3) 重新软连接： cd /usr/local/cuda/lib64 sudo ln -sf libcudnn.so.8.0.0 libcudnn.so.8 sudo ln -sf libcudnn_ops_train.so.8.0.0 libcudnn_ops_train.so.8 sudo ln -sf libcudnn_ops_infer.so.8.0.0 libcudnn_ops_infer.so.8 sudo ln -sf libcudnn_adv_infer.so.8.0.0 libcudnn_adv_infer.so.8 sudo ln -sf libcudnn_cnn_infer.so.8.0.0 libcudnn_cnn_infer.so.8 sudo ln -sf libcudnn_cnn_train.so.8.0.0 libcudnn_cnn_train.so.8 sudo ln -sf libcudnn_adv_train.so.8.0.0 libcudnn_adv_train.so.8 sudo ln -sf libcudnn_ops_train.so.7.3.1 libcudnn_ops_train.so.7 sudo ln -sf libcudnn_ops_infer.so.7.3.1libcudnn_ops_infer.so.7 sudo ln -sf libcudnn_adv_infer.so.7.3.1 libcudnn_adv_infer.so.7 sudo ln -sf libcudnn_cnn_infer.so.7.3.1 libcudnn_cnn_infer.so.7 sudo ln -sf libcudnn_cnn_train.so.7.3.1 libcudnn_cnn_train.so.7 sudo ln -sf libcudnn_adv_train.so.7.3.1 libcudnn_adv_train.so.7(4) 编译与验证：sudo ldconfigsudo cp -r /usr/src/cudnn_samples_v8/ ~/cd ~/cudnn_samples_v8/mnistCUDNNsudo chmod 777 ~/cudnn_samples_v8sudo make cleansudo make./mnistCUDNN # 验证cuDNN(5) 查询cuDNN版本：cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2三. 配置torch和torchvision3.1 配置torch 这里安装torch的方式和普通的windows和linux下安装不一样，pytorch有专门的jetson版本Pytorch的jetson版本下载地址：https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-9-0-now-available/72048 下载完成后直接使用pip进行安装：sudo pip3 install numpy torch-1.9.0-cp36-cp36m-linux_aarch64.whl 安装必须的依赖：sudo apt-get install libopenblas-base libopenmpi-dev 3.2 配置torchvision由于安装的是1.9.0版本的pytorch，直接安装于其对应的torchvision(0.10.0)：sudo pip3 install torchvision==0.10.03.3 验证GPU可用 命令行打开python，导入torch包：import torch 查看pytorch可调用的cuda版本：torch.version.cuda 查看cuda是否可用：torch.cuda.is_available()3.4 使用yolo遇到的问题问题：RuntimeError: No such operator torchvision::nms实验过程：在网上搜到的都是torch和torchvision的版本不对，要升级torchvision之类的，实验将torchvision升级到0.10.0，依旧不行。解决方案：其实在使用yolo时的非极大值抑制的时候，可以将utils.general.py中nms相关代码替换 原始代码：i = torch.ops.torchvision.nms(boxes, scores, iou_thres) 替换代码： import torchvisioni = torchvision.ops.nms(boxes, scores, iou_thres)"
    } ,
  
    {
      "title"       : "ZED2相机api使用心得",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./ZED2%E7%9B%B8%E6%9C%BAapi%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97.html",
      "date"        : "2021-08-25 05:11:00 +0800",
      "description" : "记录ZED双目相机的python API使用过程",
      "content"     : "一. ZED相机的选型 STEREOLABS（ZED相机厂家）的官网：https://www.stereolabs.com/zed/ZED双目相机有以下四种型号：ZED 2i：ZED相机最新款（ZED相机二代的进阶版，防尘防水）ZED 2 ：ZED相机二代（相较于1代多了支持IMU）ZED mini：功能上和二代基本一致，尺寸更小，性能上要差，比如这里支持的景深在15m以内，而二代的景深最大支持20m。ZED：ZED相机一代，支持2K视频，景深范围在(0.3m,25m)，无IMU,所以对于需要玩SLAM的这款就不推荐了。二. 安装ZED的SDK2.1 安装SDK ZED相机SDK官网：https://www.stereolabs.com/developers/release/我们可以看到是，所有的SDK基本都需要你安装cuda，因此我选择了cuda11.0进行安装，具体的cuda安装过程可参考我之前的一篇博客：Windows10环境下搭建CUDA10.1和pytorch1.6。直接安装即可，将下载下来的exe双击运行ZED_SDK_Windows10_cuda11.0_v3.5.2_4.exe即可，如下图所示一直往下走即可。2.2 安装python API安装完成之后，可以在 C:\\Program Files (x86)\\ZED SDK\\下看到get_python_api.py，如下图直接进行安装python的api：python get_python_api.py三. python API的使用 导入zed的python包 import pyzed.sl as sl 查看zed相机版本，这里说下，如果检测到是ZED一代的化，是不支持拿IMU数据的。 zed = sl.Camera()info = zed.get_camera_information()print(\"Camera Model: \" + str(info.camera_model)) 拿到相机的温度信息（本人亲测可用，讲真别用官方教程里的例子，得到的温度全是0，坑） sensors_data = sl.SensorsData() if zed.get_sensors_data(sensors_data, sl.TIME_REFERENCE.CURRENT) == sl.ERROR_CODE.SUCCESS: # 这里分别拿到左、右相机，imu，气压计4者的温度 temperature_left = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.ONBOARD_LEFT) temperature_right = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.ONBOARD_RIGHT) temperature_imu = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.IMU) temperature_barometer = sensors_data.get_temperature_data().get(sl.SENSOR_LOCATION.BAROMETER) print(\"Left: {:.2f}, Right: {:.2f}, IMU: {:.2f}, Barometer: {:.2f}\\r\\n\".format(temperature_left, temperature_right, temperature_imu, temperature_barometer)) 拿到相机的惯导(IMU)信息，注意这里使用的是Y轴向上的右手定律，这里即Z轴方向是车头/飞机头的方向。 # 初始化zed = sl.Camera() # Create a ZED camera objectinput_type = sl.InputType() # Set configuration parameters init = sl.InitParameters(input_t=input_type) # 初始化 self.init.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP # 右手定律Y轴向上 while True: runtime_parameters = sl.RuntimeParameters() if zed.grab(runtime_parameters) == sl.ERROR_CODE.SUCCESS: zed_imu = zed_sensors.get_imu_data() zed_imu_pose = sl.Transform() ox = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[0], 3) oy = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[1], 3) oz = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[2], 3) ow = round(zed_imu.get_pose(zed_imu_pose).get_orientation().get()[3], 3) print(\"IMU Orientation: Ox: {0}, Oy: {1}, Oz {2}, Ow: {3}\\n\".format(ox, oy, oz, ow)) 这里拿到的数据是四元数，如果我们想转成欧拉角，这里推推荐使用scipy的转换函数Rotation.from_quat。 from scipy.spatial.transform import Rotation# 将上面拿到的四元数转成欧拉角rot = Rotation.from_quat([ox,oy,oz,ow])eular_angle = rot.as_euler('xyz', degrees=True)print(f'翻滚角：{eular_angle[2]},俯仰角：{eular_angle[0]},方位角：eular_angle[1]') 拿取当前时间（毫秒级） # 当前时间戳(毫秒)time_zed = zed_pose.timestamp.get_milliseconds() 拿到前景图 # 定义图像数据image_size = zed.get_camera_information().camera_resolutionimage_zed = sl.Mat(image_size.width, image_size.height, sl.MAT_TYPE.U8_C4) while True： # 从zed相机中拿到前景图 zed.grab() # 拿取图像 zed.retrieve_image(image_zed, sl.VIEW.LEFT, sl.MEM.CPU, image_size) # 转成numpy image_np = image_zed.get_data() 拿到景深图（当然也可以拿到3D点云数据） # 这里需要在初始化中加入深度模式init.depth_mode = sl.DEPTH_MODE.ULTRA # 深度模式 (默认-PERFORMANCE)init.coordinate_units = sl.UNIT.MILLIMETER # 毫米级 (默认-MILLIMETER) # 定义测量数据depth_zed = sl.Mat(image_size.width / 2, image_size.height / 2) # 16位进行保存point_cloud_zed = sl.Mat(image_size.width / 2,image_size.height / 2) while True： # 从zed相机中拿到景深图 zed.grab() # 拿取景深图像 zed.retrieve_measure(depth_zed, sl.MEASURE.DEPTH,sl.MEM.CPU,image_size) # 拿取3D点云图像 zed.retrieve_measure(point_cloud_zed, sl.MEASURE.XYZRGBA) # 转成numpy depth_np = depth_zed.get_data() point_cloud_value = point_cloud_zed.get_value(x, y)[1]"
    } ,
  
    {
      "title"       : "python的打包神器——Nuitka",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./python%E7%9A%84%E6%89%93%E5%8C%85%E7%A5%9E%E5%99%A8Nuitka.html",
      "date"        : "2021-08-10 23:11:00 +0800",
      "description" : "对比pyinstaller和nuitka打包工具及nuitka的使用过程",
      "content"     : "一. pyinstaller和Nuitka使用感受1.1 使用需求 这次也是由于项目需要，要将python的代码转成exe的程序，在找了许久后，发现了2个都能对python项目打包的工具——pyintaller和nuitka。这2个工具同时都能满足项目的需要： 隐藏源码。这里的pyinstaller是通过设置key来对源码进行加密的；而nuitka则是将python源码转成C++（这里得到的是二进制的pyd文件，防止了反编译），然后再编译成可执行文件。 方便移植。用户使用方便，不用再安装什么python啊，第三方包之类的。1.2 使用感受2个工具使用后的最大的感受就是： pyinstaller体验很差！ 一个深度学习的项目最后转成的exe竟然有近3个G的大小（pyinstaller是将整个运行环境进行打包），对，你没听错，一个EXE有3个G！ 打包超级慢，启动超级慢。 nuitka真香！ 同一个项目，生成的exe只有7M！ 打包超级快（1min以内），启动超级快。 二. Nuitka的安装及使用2.1 nuitka的安装 直接利用pip即可安装：pip install Nuitka 下载vs2019(MSVS)或者MinGW64，反正都是C++的编译器，随便下。2.2 使用过程对于第三方依赖包较多的项目（比如需要import torch,tensorflow,cv2,numpy,pandas,geopy等等）而言，这里最好打包的方式是只将属于自己的代码转成C++，不管这些大型的第三方包！以下是我demo的一个目录结构（这里使用了pytq5框架写的界面）：├─utils //源码1文件夹├─src // 源码2文件夹├─logo.ico // demo的图标└─demo.py // main文件使用以下命令（调试）直接生成exe文件：nuitka --standalone --show-memory --show-progress --nofollow-imports --plugin-enable=qt-plugins --follow-import-to=utils,src --output-dir=out --windows-icon-from-ico=./logo.ico demo.py这里简单介绍下我上面的nuitka的命令： --standalone：方便移植到其他机器，不用再安装python --show-memory --show-progress：展示整个安装的进度过程 --nofollow-imports：不编译代码中所有的import，比如keras，numpy之类的。 --plugin-enable=qt-plugins：我这里用到pyqt5来做界面的，这里nuitka有其对应的插件。 --follow-import-to=utils,src：需要编译成C++代码的指定的2个包含源码的文件夹，这里用,来进行分隔。 --output-dir=out：指定输出的结果路径为out。 --windows-icon-from-ico=./logo.ico：指定生成的exe的图标为logo.ico这个图标，这里推荐一个将图片转成ico格式文件的网站（比特虫）。 --windows-disable-console：运行exe取消弹框。这里没有放上去是因为我们还需要调试，可能哪里还有问题之类的。经过1min的编译之后，你就能在你的目录下看到：├─utils //源码1文件夹├─src // 源码2文件夹├─out // 生成的exe文件夹 ├─demo.build └─demo.dist └─demo.exe // 生成的exe文件├─logo.ico // demo的图标└─demo.py // main文件当然这里你会发现真正运行exe的时候，会报错：no module named torch,cv2,tensorflow等等这些没有转成C++的第三方包。这里需要找到这些包（我的是在software\\python3.7\\Lib\\site-packages下）复制（比如numpy,cv2这个文件夹）到demo.dist路径下。至此，exe能完美运行啦！"
    } ,
  
    {
      "title"       : "Mask_RCNN在TF2下跑通自己的数据集",
      "category"    : "",
      "tags"        : "机器视觉, 深度学习",
      "url"         : "./Mask_RCNN%E5%9C%A8TF2%E4%B8%8B%E8%B7%91%E9%80%9A%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86.html",
      "date"        : "2021-07-26 20:11:00 +0800",
      "description" : "讲述Mask RCNN在tensorflow2.x下如何跑通自己的数据集",
      "content"     : "论文原文地址：https://arxiv.org/abs/1703.06870 MaskRCNN官方的git地址：https://github.com/matterport/Mask_RCNN一. 构建数据集这里参考官方推荐的气球语义分割的例子，这里选用的是和他一致的打标工具 VIA (VGG Image Annotator)。个人感觉这个比labeme好用太多。 直接在VIA官网下载即可，下载完成后如下图所示，直接用浏览器打开via.html即可开箱使用。 选择Add Files添加图片后，选择Attributes设置打标的label，然后用多边形工具进行打标，如下图。 输出json格式的打标结果 将图片和json结果保存在同一个目录下，构建自己的数据集时可参考我的目录结构。├─dataset│ ├─train│ │ ├─1.jpg│ │ ├─2.jpg│ │ ├─3.jpg│ │ └─annotations.json│ └─val│ │ ├─1.jpg│ │ ├─2.jpg│ │ ├─3.jpg│ │ └─annotations.json二. 准备工作 由于官方的代码只支持tensorflow的版本都是1.x的版本，对于tensorflow 2.x的版本官方代码中很多地方不能调用，这里需要大量修改。2.1 环境搭建本人是Windows系统，安装的是11.0版本的cuda，以下是我安装的项目必须的python包。 scikit-image==0.16.2 这里说一句，如果安装的是更高版本的，最好降低成0.16的，不然可能后面训练的时候会报Input image dtype is bool. Interpolation is not defined with bool data type，这里参考。 tensorflow==2.2.0 就是因为tensorflow的1.x版本不支持cuda11.0，才折腾好久…. keras==2.4.0 numpy==2.1.02.2 源码的替换 之前本人找到一个TF2.0版本的Mask RCNN，哎，但是发现没卵用！ 下载官方源码：git clone https://github.com/matterport/Mask_RCNN.git 替换官方的核心代码mrcnn下的所有代码。PS：由于这里改的官方代码中mrcnn/model.py,mrcnn/config.py,mrcnn/utils.py,parallel_model.py需要修改的文件太多，直接给出git地址，大家可以直接下在mrcnn这个文件夹及其下面的py文件对官方的mrcnn这个文件夹进行替换（不然太折腾人了）。2.3 训练代码修改 这里训练的代码参考的是官方的samples/ballon/ballon.py这个文件，直接复制到主目录Mask_RCNN下，修改成自己想要的名字，比如train.py。 修改根地址，这里由于从samples/ballon/目录到主目录下，因此修改ROOT_DIR = '.' 修改config类成自己的类，注意我这里是改成了动物类，然后一共是识别2个种类，然后最好这里的NAME和利用VIA构建数据集中的Attribute一致。 class AnimalConfig(Config): # Give the configuration a recognizable name NAME = \"animal\" # Adjust down if you use a smaller GPU. IMAGES_PER_GPU = 1 # Number of classes (including background) NUM_CLASSES = 1 + 2 # Background + balloon # Number of training steps per epoch STEPS_PER_EPOCH = 100 # Skip detections with &lt; 90% confidence DETECTION_MIN_CONFIDENCE = 0.9 修改Dataset类成自己的Dataset类，同时修改load函数class AnimalDataset(utils.Dataset): def load_animal(self, dataset_dir, subset): # Add classes. We have only one class to add. self.add_class(\"animal\", 1, \"pig\") self.add_class(\"animal\", 2, \"cow\") ..... self.add_image( \"animal\", image_id=a['filename'], path=image_path, width=width, height=height, polygons=polygons) 修改load_mask函数 def load_mask(self, image_id): image_info = self.image_info[image_id] if image_info[\"source\"] != \"animal\": return super(self.__class__, self).load_mask(image_id) 修改image_reference函数 def image_reference(self, image_id): \"\"\"Return the path of the image.\"\"\" info = self.image_info[image_id] if info[\"source\"] == \"animal\": return info[\"path\"] else: super(self.__class__, self).image_reference(image_id) 修改训练函数def train(model): \"\"\"Train the model.\"\"\" # Training dataset. dataset_train = AnimalDataset() dataset_train.load_animal(args.dataset, \"train\") dataset_train.prepare() # Validation dataset dataset_val = AnimalDataset() dataset_val.load_animal(args.dataset, \"val\") dataset_val.prepare()三. 训练直接在命令行中python train.py train --dataset=./dataset --weights=coco即可看到模型开始训练了。这里注意2点： 用VIA打标好的数据集注意是放到Mask RCNN主目录下，所以这里的命令参数--dataset=./dataset 权重用的是coco形式，即--weights=coco，如果不是的话，你会遇到这类问题mrcnn_bbox_fc/kernel:0' shape=(1024, 8) dtype=float32_ref&gt; has shape (1024, 8), but the saved weight has shape (1024, 324) ，具体可参考"
    } ,
  
    {
      "title"       : "ORB-SLAM3在windows下的编译使用",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./orb_slam3.html",
      "date"        : "2021-05-12 10:11:00 +0800",
      "description" : "讲述ORB-SLAM3在windows下的编译及其使用",
      "content"     : "一. 数据集1.1 数据集介绍Euroc：基于室内的MAV(Micro Aerial Vehicle，微型飞行器)，一共两个场景（Machine Hall + Vicon Room）,其中每个数据集包含2个下载连接： ROS(Robot Operating System) bag：机器人操作库，适用于嵌入式，这里推荐一个很好的双目+IMU应用在jetson nano上的git。 ASL Dataset Format：数据集结构，包含传感器文件和双目相机的图片。1.2 数据使用介绍可用的数据包含： Visual-Inertial Sensor Unit（视觉惯性传感器单元） Stereo Images（双目图片） IMU（惯导数据） Ground-Truth位姿轨迹 Vicon 6轴运动姿态捕捉系统 Leica MS50 3维姿态镭射追踪 Leica MS50 3维重构 1.3 IMU数据介绍 w_RS_S_x [rad s^-1] ：MAV在R坐标系下的x轴角速度信息，单位rad/s w_RS_S_y [rad s^-1] ：MAV在R坐标系下的y轴角速度信息，单位rad/s w_RS_S_z [rad s^-1] ：MAV在R坐标系下的z轴角速度信息，单位rad/s a_RS_S_x [m s^-2]：MAV在R坐标系下x轴的线加速度信息，单位m/s^2 a_RS_S_y [m s^-2]：MAV在R坐标系下y轴的线加速度信息，单位m/s^2 a_RS_S_z [m s^-2]：MAV在R坐标系下z轴的线加速度信息，单位m/s^2二. 第三方包编译 ORB_SLAM3论文地址：https://arxiv.org/pdf/2007.11898.pdf 使用ORB_SLAM3官方git，推荐使用的系统为ubuntu 18.04，本人用win 10下进行测试的，这里推荐一个在win 10下编译ORB_SLAM3的git，目前在该仓库下编译运行没毛病！2.1 前期依赖的第三方包 eigen：线性算术的C++模板库（属于g2o的依赖），这里直接用vcpkg安装vcpkg install eigen boost：后面编译ORB_SLAM3库需要，这里也是直接用vcpkg安装vcpkg install boost。 opencv3.4.11：编译DBoW2和ORB_SLAM3需要。直接上官网下载exe即可，当然也可以利用vcpkg进行安装。2.2 DBoW2用于SLAM回环检测，这里需要配置opencv环境。具体过程如下： 给Thirdparty/DBoW2/CMakeLists.txt配置opencv3.4.11的路径 set(OpenCV_DIR \"D:/software/opencv/opencv/build\") 在Thirdparty/DBoW2路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/DBoW2.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT) 右键项目 ==&gt; 生成，即可看到生成好的lib文件Thirdparty/DBoW2/lib/Release/DBoW2.lib 2.3 g2o用于图优化的框架。具体过程如下： 在Thirdparty/g2o路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/g2o.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT)；C/C++ ==&gt; 预处理器 ==&gt;最上面加入WINDOWS 右键项目 ==&gt; 生成，即可看到生成好的lib文件Thirdparty/g2o/build/Release/g2o.lib2.4 Pangolin用于3D视觉和3D导航的视觉图和用户之间的交互。这里其实和编译ORB_SLAM3没有关系，但是我们使用ORB_SLAM3库的时候应用的例子上是需要这个库的。具体过程如下： 在Thirdparty/g2o路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/Pangolin.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT) 这里是需要下载它依赖的其他库的，最好对git设置代理 git config --global http.proxy http://127.0.0.1:1080git config --global https.proxy http://127.0.0.1:1080 右键ALL_BUILD ==&gt; 生成，即可看到生成好的lib文件ThirdParty/Pangolin/lib/Release/pangolin.lib 三.编译ORB_SLAM3 给orbslam3-windows/CMakeLists.txt配置opencv3.4.11的路径 set(OpenCV_DIR \"D:/software/opencv/opencv/build\") 在orbslam3-windows的路径下新建一个build文件夹，cmake生成cmake .. 看到configuration done的时候，用vs2019打开build/ORB_SLAM3.sln 将配置改成release，同时右键项目==&gt; 属性 ==&gt; 常规 ==&gt; 配置类型 ==&gt; 静态库(.lib)；属性 ==&gt; 高级 ==&gt; 目标文件扩展名 ==&gt; .lib；C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT)；C/C++ ==&gt; 预处理器 ，添加以下预编译器定义 WINDOWSCOMPILEDWITHC11 右键项目 ==&gt; 生成，即可看到生成好的lib文件ORB_SLAM3/build/Release/ORB-SLAM3.lib 四. 编译测试案例及展示4.1 编译stereo_inertial_euroc 用vs2019打开build/ORB_SLAM3.sln 将配置改成release，同时右键项目stereo_inertial_tum_vi ==&gt; 属性 ==&gt; C/C++ ==&gt; 代码生成 ==&gt; 运行库 ==&gt; 多线程 (/MT)；C/C++ ==&gt; 预处理器 ，添加以下预编译器定义COMPILEDWITHC11；链接器 ==&gt; 高级 ==&gt; 导入库，改为空；链接器 ==&gt; 输入 ==&gt; 去掉..\\Thirdparty\\boost_1_67_0\\lib64-msvc-14.1\\libboost_serialization-vc141-mt-s-x64-1_67.lib（由于这里是vcpkg安装的boost，因此该路径下根本没有这个lib）。 右键项目 ==&gt; 生成，即可看到生成好的exe文件ORB_SLAM3/Examples/Stereo-Inertial/Release/stereo_inertial_tum_vi.exe4.2 使用展示案例stereo_inertial_euroc这个案例是双目 + 惯导的Euroc数据集的应用。 将下载好的数据集文件夹名字改成MH01(这里是由于本人下载是MH_01_easy.zip) 进入到生成好的exe文件夹下cd orbslam3-windows\\Examples\\Stereo-Inertial\\Release，可以看到生成好的stereo_inertial_euroc.exe 开启程序： .\\stereo_inertial_euroc.exe ..\\..\\..\\Vocabulary\\ORBvoc.txt ..\\EuRoC.yaml ..\\MH01\\ ..\\EuRoC_TimeStamps\\MH01.txt dataset-MH01_stereoi 结果展示如下图所示："
    } ,
  
    {
      "title"       : "目标检测(one stage)-从FPN到DSSD",
      "category"    : "",
      "tags"        : "机器视觉, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_4.html",
      "date"        : "2021-04-28 05:11:00 +0800",
      "description" : "讲述目标检测(one stage)-从FPN到DSSD",
      "content"     : "一. FPN特征金字塔网络 论文地址：https://arxiv.org/pdf/1612.03144.pdf这篇论文发布的时间是2017年4月19号，可以说在此之后，对于目标检测（小物体）而言，提升巨大，基本之后的模型比如DSSD，yolov3等都参考过该模型架构。1.1 解决的问题 目标检测的基本挑战（问题）：识别多尺度变化的目标能力不足。这里解决了一下两个方面的难点： 相机距离目标远近不同导致拍摄的图片中目标尺寸不同而导致识别效率低下。 小目标物体的识别较难。 1.2 图像特征金字塔特征金字塔是在不同大小尺寸的目标检测中的一个基础组件。 如下图所示，经过多次特征抽取后，越到高层的feature map所囊括的细节信息就越少，对于底层信息（比如小的目标）预测就越难。 那么图像特征金字塔做了什么呢？既对同一张图片进行多次下采样，从而得到多张不同尺寸的图片，进而生成不同尺寸的feature map，从而使模型拥有对不同尺度大小的物体进行检测的能力。但是论文中也提出它的问题：消耗太大的内存和计算量。 看到上面这张图的时候，你是不是觉得很熟悉？和某个网络的推理层很像？是的，就是SSD。SSD和YOLOv1最大的差别（之前的文章中有讲，有兴趣可以查看本人之前的文章）其实是推理层的不同，SSD用的就是多尺度的特征图综合来预测目标，从而达到对于小物体也能够检测的目的。 如下图所示，是不是感觉和上面的图像特征金字塔很像？差别还是有的，SSD是对同一张图片进行卷积抽取其不同尺度的feature map进行分别做预测，而特征金字塔是对不同尺度的图片分别做特征抽取得到不同尺度的feature map在分别做预测。这里论文中也提到SSD的缺点：失去了高层语义信息重用的机会，导致低层语义信息不足。 1.3 特征金字塔网络（FPN） 这里推荐一篇个人认为不错的git仓库：easy-fpn.pytorch，是由pytorch复现的fpn网络。FPN的网络结构如下图所示：我们可以看到，其实说白了FPN相较于SSD和特征金字塔厉害的地方： 强于SSD：重用了高层语义信息，每次预测都是结合了当前层和上一层的语义信息，使每一层不同尺度的特征图都具有较强的语义信息。 强于特征金字塔：同样通过上下采样（这里不同的是采样的feature map），使模型拥有对不同尺度大小的物体进行检测的能力，却又构建了一个端到端的网络。下面是来源于我上面推荐的git仓库的FPN网络的细节架构图。方便我们很好的理解FPN中到底做了什么。如下图所示，从底层的feature map（1067 * 800）到高层的feature map(34 * 25)，每层feature map经过1 * 1的卷积之后和经过up sample之后的上层feature map做一个add操作，在进行推理。下面是细节复现的pytorch代码：# Bottom-up pathwayc1 = self.conv1(image)c2 = self.conv2(c1)c3 = self.conv3(c2)c4 = self.conv4(c3)c5 = self.conv5(c4)# Top-down pathway and lateral connectionsp5 = self.lateral_c5(c5)p4 = self.lateral_c4(c4) + F.interpolate(input=p5, size=(c4.shape[2], c4.shape[3]), mode='nearest')p3 = self.lateral_c3(c3) + F.interpolate(input=p4, size=(c3.shape[2], c3.shape[3]), mode='nearest')p2 = self.lateral_c2(c2) + F.interpolate(input=p3, size=(c2.shape[2], c2.shape[3]), mode='nearest')# Reduce the aliasing effectp4 = self.dealiasing_p4(p4)p3 = self.dealiasing_p3(p3)p2 = self.dealiasing_p2(p2)p6 = F.max_pool2d(input=p5, kernel_size=2)二. DSSD 论文地址：https://arxiv.org/abs/1701.06659.pdf这篇论文发布的时间是2017年1月23号。DSSD(Deconvolutional Single Shot Detector)，听名字就知道了，是SSD的升级版本，而且其实就是SSD + FPN的结合体，但是很奇怪，为啥你还在别人FPN后面发布呢？2.1 和SSD的差别下图是SSD和DSSD的网络架构示意图。SSD和DSSD在前面特征抽取层的backbone都是一样的，差别在于后面推理的过程： SSD的推理过程经过多个卷积得到的不同尺寸的特征图来进行预测。 DSSD的推理过程是FPN网络架构的复现：由底层特征结合经up sampling之后的上层特征，做一个结合的操作，论文中提到2中结合方式： Eltw-sum：也叫broadcast add，将浅层和深层的特征图在对应的通道上做加法运算。 Eltw-prod：也叫broadcast mul，将浅层和深层的特征图在对应的信道上做乘法运算。 2.2 DSSD在模型效果上的提升下图是SSD（左1和右1）和DSSD（左2和右2）的模型在同一张图片上的检测效果。可以明显的发现： DSSD能检测到更多的目标。 DSSD能检测到更小的目标。"
    } ,
  
    {
      "title"       : "C++下消息队列（多消费者模式）的实现",
      "category"    : "",
      "tags"        : "代码",
      "url"         : "./%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84C++%E5%AE%9E%E7%8E%B0.html",
      "date"        : "2021-04-24 07:21:00 +0800",
      "description" : "讲述在C++下实现的消息队列（多消费者模式）",
      "content"     : "不允许用现成的消息队列比如rabbitmq等，非要造轮子！一. 生产者-消费者模式 生产者：这里由于是通过txt文件来进行交互，相当于txt文件的内容就是生产者，同时还需要实时监控txt文件，将其新消息放入队列。 消费者：从队列中取消息，并需要告诉队列该条消息已经被消费。 断点续传：需要考虑到程序崩溃之后，知道从哪开始消费。1.1 多消费者——多线程这里如果如果是IO操作较多的话，推荐使用多线程来创建消费者。具体创建和消费的过程如下： 我们可以看到其实这里的多消费者其实是串联的形式来进行消费，因此如果是CPU资源低，IO操作多的话，推荐这种形式。 多线程之间的内存变量交互很友善，不像多进程（哎，难受啊）。 1.2 多消费者——多进程 考虑到这里是需要像CPU请求较多资源，甚至是需要使用GPU的资源和利用CUDA加速（深度学习模型占用资源较多），因此我这里使用的是多进程来构建消费者。具体创建和消费过程如下:二. 过程的实现 具体代码可以参考本人的git 定时器：每隔一段时间扫描txt文件，并将文件中新加入的消息存放入队列中。 // timer扫描int wait_sec = 1;// 单独启动一个线程持续扫描文件（每5秒）string path = \"video.txt\";Timer timer1;timer1.start(2000, std::bind(getVideoFromTxt, path, wait_sec, &amp;output)); 中间件：需要存放已经消费的消息，这样知道消费的具体位置，且支持程序崩溃/断掉之后，重启后知道在哪开始消费，同样用txt进行保存到本地。 多消费者：使用多进程创建消费者，这里考虑到进程中间的共享内存不好交互，直接使用txt来交互数据（反正进程之间的内存交互其实也是通过一个共享文件映射来完成的）。这里直接使用调用命令行的方式来构建消费者。 void gen_multiProcess(int id, string input_txt,string output_txt) { STARTUPINFO si; si.cb = sizeof(si); PROCESS_INFORMATION pi; ZeroMemory(&amp;si, sizeof(si)); ZeroMemory(&amp;pi, sizeof(pi)); string cmdLine = \"D:/vs_project/setTimer/x64/Debug/setTimer.exe \" + input_txt + \" \" + output_txt; cout &lt;&lt; cmdLine &lt;&lt; endl; wstring str = StringToWString(cmdLine); BOOL bSuccess = CreateProcess(NULL, const_cast&lt;LPWSTR&gt;(str.c_str()), NULL, NULL, FALSE, 0, NULL, NULL, &amp;si, &amp;pi); if (bSuccess) { //handleOfProcess[id] = pi.hProcess; cout &lt;&lt; \"Process-\" &lt;&lt; id &lt;&lt; \"completed!\" &lt;&lt; endl; } else { cout &lt;&lt; \"Error:\" &lt;&lt; id &lt;&lt; endl; }} 三. 收获其实手写消息队列，帮助自己更多的了解消息消费机制，以及多进程和多线程的使用，当然了也更了解了C++的标准库。所以推荐大家还是亲手动手手写一哈。哎，作为一个python调包侠突然写这种偏底层，有点难受好吧。"
    } ,
  
    {
      "title"       : "目标检测(one stage)-YOLOv2",
      "category"    : "",
      "tags"        : "机器视觉, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_3.html",
      "date"        : "2021-04-21 04:21:00 +0800",
      "description" : "目标检测（one stage）——YOLOv2",
      "content"     : "一. 与V1的不同之处YOLOv2相较于YOLOv1在VOC2007数据集上表现从63.4%提升到78.6%。YOLOv2与YOLOv1的不同之处一共体现在下面几个方面： Batch Normalization（批归一化层）：在模型不过拟合的前提下，可以拿掉Dropout层；同时加速模型训练。在VOC2007数据集上效果mAP提升2.4%。 High Resolution Classifier（提高分辨率）:原本YOLOv1中模型训练的使用的是224 * 224分辨率的图像，现在resize成448 * 448的图片，最后经过10个epoch的微调。在VOC2007数据集上效果mAP提升3.7%。 Convolutional With Anchor Boxes（提高检测目标数量）：原本yolov1中将其中一个pool层拿掉后， feature map的大小由7 * 7 变成了13 * 13，然后每个1 * 1的grid里面增加了K个anchor boxes，因此从yolov1只能检测7 * 7 = 49个目标，增加到了13 * 13 * K个目标。在VOC2007数据集上效果虽然mAP下降了0.3%，但是在Recall上提升了7%。 New Network（DarkNet-19）：提出了一个新的网络架构DarkNet-19（19个Conv 和 5个MaxPool），能够在YOLOv1的基础上减少33%的计算量。在VOC2007数据集上效果mAP提升0.4%。 Dimension Clusters（尺度聚类）：如果Anchor boxes一开始就能和实际的物体的宽高比很接近，那么对于模型的收敛是否有帮助呢？YOLOv2这里就是使用了KMeans对所有的图片的宽高比（利用IOU计算之间的距离）进行聚类，结果是聚类个数K取5效果最好，从而每个grid中找到5个最好的预选框。 Direct Location Prediction（绝对位置预测）：模型原本的anchor box在预测物体的x,y坐标时候会发生数值不稳定的现象（而R-CNN网络的boundingbox并非随机，而是由RPN网络生成），毕竟随机初始化的anchor box的位置，肯定需要花费大量时间才能学习到合适的位置。那么yolov2是如何完成？这里红色为anchor box，蓝色为模型的预测框，这里引入sigmoid函数来缩小到(0,1)之间，同时对计算后的结果进行归一化的处理， 使用尺度聚类和直接位置预测两种方式后模型在VOC2007数据集上效果mAP提升4.8%。 Fine-Grained Features（颗粒度特征）：将feature map拆解成更小的feature map。借由前面DarkNet得到高解析度的特征拆解成小解析度的特征，因此来检测较小的物体。VOC2007数据集上效果mAP提升1%。 Muti-Scale Training（多尺度训练）：每10个batch去做一个resize的动作，从一开始的320 * 320到最终的608 * 608，最后得到的feature map从开始的10 * 10到19 * 19。VOC2007数据集上效果mAP提升1.2%。 High Resolution Detector（输入大size的input）：通过输入图像尺寸更大，使得模型检测到更小物体。下图是从288到544尺寸之间mAP提升效果，当然fps会相应的减少。VOC2007数据集上效果mAP提升2%。 二. 其他模型的比较下图是YOLOv2在COCO2015数据集上的表现。可以看到YOLOv2相较于SSD还有一些的差距的。"
    } ,
  
    {
      "title"       : "windows下安装python-pcl",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./Windows%E4%B8%8B%E5%AE%89%E8%A3%85PCL.html",
      "date"        : "2021-04-10 18:21:00 +0800",
      "description" : "介绍如何在win10下安装python版本的PCL点云库",
      "content"     : "一. 准备工作 python 版本：3.7.9 cython numpy python-pcl:1.9.1 python-pcl源码：后面需要进行编译 PCL1.9.1的All-In-One Installer ：目前安装仅支持1.6到1.9的版本 visual studio 2019 Windows Gtk二. 安装 将下载好的ALL-In-One Installer进行安装，这里会要求你添加到环境变量（必须添加啊），并且会安装OpenNI这个工具。 解压下载好的windows Gtk，将bin目录下所有文件复制到python-pcl源码目录下的pkg-config目录下。 在pkg-config目录下，运行脚本InstallWindowsGTKPlus.bat，该脚本会下载必须的内容，下载完成后会多出这些文件夹，如下图所示 安装python的pcl包： cd 你安装python-pcl源码目录 python setup.py build_ext -i python setup.py install 三. 安装遇到的坑3.1 坑一：cannot find PCL 问题：当你运行python setup.py build_ext -i的时候报出：setup.py: error: cannot find PCL, tried pkg-config pcl_common-1.7 pkg-config pcl_common-1.6 pkg-config pcl_common 解决方案：这里就是上面说的，别下除了1.6到1.9版本的pcl的All-In-One Installer啊。3.2 坑二：DLL load failed 问题：全部安装完成之后，一切没有问题了，当你打开python，运行import pcl的时候报出：DLL load failed。 解决方案：重启电脑！四. python版本的使用4.1 点云数据的展示（python）构建点云–Point_XYZRGBA格式(需要点云数据是N*4，分别表示x,y,z,RGB ,其中RGB 用一个整数表示颜色)，下面是python版本的点云数据展示import pclimport pcl.pcl_visualization as viewer #可视化库import numpy as np# cloud = pcl.load(\"cloud.pcd\")cloud_np = np.load(\"cloud.npy\")cloud = pcl.PointCloud_PointXYZRGBA(cloud_np)visual = pcl.pcl_visualization.CloudViewing()visual.ShowColorACloud(cloud)v = Truewhile v: v = not (visual.WasStopped())4.2 命令行展示由于上面已经下载了PCL1.9.1了，可以直接在命令行中进行展示：pcl_viewer_release H cloud.PCD，下面的是来自Middlebury 2014数据集中经过立体匹配后的3D点云图。"
    } ,
  
    {
      "title"       : "目标检测(one stage)-SSD",
      "category"    : "",
      "tags"        : "机器视觉, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_2.html",
      "date"        : "2021-04-04 04:21:00 +0800",
      "description" : "目标检测（one stage）——SSD",
      "content"     : "一. YOLO和SSD的对比yolo和ssd两个模型结构如下图所示：两个模型之间最主要的差别： 在特征抽取层其实相差不大：YOLO用的是器自己的conv架构；SSD用的是VGG-16 主要差别在结果预测上：YOLO用的是全连接层后得到7*7的grid，利用每个grid的boundingbox来做目标检测；SSD利用不同大小的feature map来做目标检测。二. 模型结构2.1 特征抽取层那么如何从VGG-16的结构变成SSD的结构呢?下图是一个VGG-16的示意图。将VGG-16的最后一层pooling层变成3*3 的卷积层，再接一个atrous conv（空洞卷积）拿到不同大小的feature map。如下所示。2.2 空洞卷积这里运用atrous conv layer而不是普通的conv layer的目的： 在相同的感受野的同时，能获得更快的运算速度如下图所示，是5 * 5 的卷积的kernel和3 * 3的atrous conv的kernel的感受野。可以看到，如果是3 * 3的conv层接5 * 5的conv层，那么feature map中单一点的感受野其实是7个像素点；而如果是3 * 3的conv层接3 * 3的atrous conv层，能达到相同的感受野，且计算速度更快。2.2 推理层下图是SSD的推理层的示意图。可以看到，图片经过vgg16之后，首先会得到较浅的feature map,随后经过几层卷积之后，得到较为深层的feature map（所以在上图中仅有较深层的能检测到车这种大物体），同时每层的feature map都会经过一个检测器和分类器得到检测结果，最后经过NMS得到最终的检测结果。那么整个SSD的anchor box的数量是：\\(38*38*3+19*19*6+10*10*6+5*5*6+3*3*6+1*1*6 = 7308\\)三. 模型训练3.1训练lossSSD和YOLO的loss中的检测类别值有所不同：假定检测目标一共A个类别，那么YOLO的预测类别数位A个，而SSD的预测类别则是A+1个（包含了背景类）。如下图所示。3.2 难负例挖掘对于正负样本不均衡的情况，SSD采用了hard negative mining(难负例挖掘)技巧来解决。hard negative是指在图片中容易将负样本（背景）看成是正样本（前景）的样本。而mining的操作就是将这类样本放入模型进行学习，从而减少模型的false positive。那么SSD是如何引用hard negative mining技巧呢？如下图，其中蓝色的box的我们希望它的confidence较低，而绿色的confidence较高。 对于一张图而言，选出其中anchor box中negative置信度较高的box。 正负比例的anchor box = 1：33.2 数据增强SSD模型在论文中也使用了很多不同的data augmentation(数据增强)的操作。方式一： 针对原始输入图片和ground truth进行IOU的操作 对其中iou = 0.1，0.3，0.5，0.7和0.9来进行采样。 对采样后的图片进行resize成相同大小的图片，然后进行水平翻转的操作。方式二（Random Expansion-得到的小目标训练样本）： 对原始图像做不同比例的缩小。 然后放在相同大小图片中不同的地方。四. 结果比较可以看到，SSD相较于YOLO在准确性上有很大的提升，同时预测速度上也能达到很高的fps。"
    } ,
  
    {
      "title"       : "目标检测(one stage)-YOLOv1",
      "category"    : "",
      "tags"        : "机器视觉, 深度学习",
      "url"         : "./%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_1.html",
      "date"        : "2021-03-11 04:21:00 +0800",
      "description" : "目标检测（one stage）的开始——YOLOv1",
      "content"     : "一. 目标检测算法的分类及历史1.1 目标检测算法的分类目标检测算法主要分为2大类： one-stage(one-shot object detectors) ：直接预测目标的bounding box及其类别。特点是一步到位，速度很快。比如：YOLO，SSD等系列模型。 two-stage：需要先使用启发式方法(selective search)或者CNN网络(RPN)产生Region Proposal，然后再在Region Proposal上做分类与回归。特点是：慢，但是准确率高。比如：RCNN系列模型。由于在工业应用中，往往对模型预测速度有要求，而two-stage目标检测模型由于先天的不足，因此本文仅考虑one-stage目标检测模型。1.2 目标检测发展流程目标检测（one-stage）的总体发展流程： 2015.06 — YOLOv1：第一个one-stage目标检测器。 2015.12 — SSD：结合anchor box和多尺度特征的one-stage目标检测器。 2016.12 — YOLOv2：YOLO的第二版。 2016.12 — FPN：特征金字塔（结合不同尺寸的特征图像） 2017.01 — DSSD：SSD结合FPN。 2017.08 — RetinaNet：Focal Loss解决正负样本不均衡 2018.04 — YOLOv3：YOLO的第三版。 2018.07 — CBAM：Attention机制的目标检测。 2019.11 — EfficientDet：Google提出的目标检测器。 2020.04 — YOLOv4：YOLO的第四版。 2020.06 — YOLOv5：YOLO第五版。二. YOLO当我最初学习图像分类的时候，就一直疑惑：如果我利用卷积层抽取目标特征后直接把分类任务做成回归任务（包含目标的位置和类别信息）可以作为目标检测器么？答案来了——YOLO（You Look Only Once）。2.1 模型结构YOLO模型的结构如上所示： 输入为一个448*448的一个图片输入。 一共是经过24层的卷积层抽取特征，使用relu作为每一层的激活函数。 最后通过全连接层，且output形式为[7,7,30]的输出。模型输出的理解： 将448 * 448的图像分为7 * 7的grid（网格），每个grid都会进行判断：是否为前景，且会构建2个boundingbox来框出物体。因此，一共是有7 * 7 * 2个框。而每个grid都会输出x,y,w,h,c；这里的confidence的计算就是前景目标的概率 * iou的值。 除了boundingbox的计算外，当然还需要输出目标是哪个类别，即输出检测到的目标是某个类别的概率。这样就可以计算每个grid属于某个类别下的iou情况了。 最后利用NMS（非极大值抑制：顾名思义就是不是最大的置信度就不要了）找到每个目标的最合适的框。具体NMS的算法步骤如下： （1）首先拿到的是YOLO模型输出的结果，即7 * 7 * 2个框，每个框都是由5个元素（x,y,w,h,c）。这里需要知道一张图片中有多少个目标且目标confidence最高的结果。 （2）通过计算两两框之间的IOU（交并比），用来划分一张图片中有多少个目标（如果IOU&gt;0说明属于同一目标下的框）。 （3）对同一目标下的所有框的confidence进行排序，找到最大的的confidence对应的框。 2.2 模型训练这里主要讲述模型训练过程中loss的定义过程。2.2.1 Location Loss定义如下所示：如上所示，假定是将图片划分为3 * 3个grid，每个grid有且仅有一个预测框，由于只计算和前景目标匹配的框，因此只会计算grid5和grid7的location loss。 grid5的loss： grid7的loss： 但是这里看大大猫和小猫的loss竟然是一样的，大猫的loss应该明显要小一些，而小猫的loss明显要大一些。因此这种loss的计算还需要提升。这里就将w,h的分别先进行开根号处理。 grid5的loss： grid7的loss：2.2.2 Object Loss定义如下：那么上图的每个grid的confidence的值如下：object loss的值为：但是这个是只划分了3 * 3个grid的，那么如果是原论文中的7 * 7的情况下呢，此时的object loss的值为：我们可以看到，0.96这个检测的背景的loss就过大了，那么在反向传播的过程中，梯度的变化很大程度就着重在背景的部分，以至于学习前景的能力较差。因此，重新定义object loss（其实就是在背景loss引入一个系数，比如0.5）：2.2.3 classification loss定义如下：2.4 YOLO存在问题2.4.1 同一个grid却是多个目标的中心点如上图所示，人和车的中心点基本都落在中心的grid中，对于yolo而言，就无法分辨到底是人还是车？一个grid下只能预测1个目标。2.4.2 同一个grid中存在多个小目标如上图所示，同一个grid下有多个鸟（小目标），而对于yolo而言，一个grid下只能预测1个目标。"
    } ,
  
    {
      "title"       : "Scoop软件推荐",
      "category"    : "",
      "tags"        : "coding",
      "url"         : "./Scoop%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90.html",
      "date"        : "2021-02-04 04:21:00 +0800",
      "description" : "介绍Windows平台软件管理工具",
      "content"     : "一. scoop包管理器 scoop官网：https://scoop.sh/一直以来觉得linux的apt-get和macos的brew来安装软件特别方便，而windows下总是要去一个个找软件，然后一个个安装，一个个设置环境变量，很麻烦。这里就推荐一款类似的windows的包管理器——scoop。1.1 安装scoop 确保是powershell 5（及其以上）进行安装：Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') iwr -useb get.scoop.sh | iex 如果这里报错，需要需要改变执行规则：Set-ExecutionPolicy RemoteSigned -scope CurrentUser1.2 scoop必备软件 aria2：多线程下载软件（scoop下载软件的时候也用这个来加速，scoop会默认设置） sudo：感觉和linux的sodu一样，windows不是有个“以管理员身份运行”么。 git：下载git的时候，需要你下载7zip。1.3 scoop常用命令 寻找软件：scoop search 软件名 安装软件：scoop install 软件名 删除软件：scoop uninstall 软件名 查看已安装的软件：scoop list 清理缓存：scoop cache rm 软件名 或者scoop cache rm * 查看可添加仓库：scoop bucket known 添加额外仓库：scoop bucket add 软件名 二. 美化命令行是不是总是觉着无论是windows的cmd.exe还是powershell都特别丑？那就来美化他吧！2.1 美化终端这里推荐的是微软自带的windows-terminal，反正个人感觉挺好用也挺好看的。 直接用scoop进行安装即可：scoop install windows-terminal，但是注意一点：这个终端需要你的windows系统最少为Windows 10 183622.2 美化命令行（主题）不知道有多少人是喜欢linux的oh my zsh这个主题的，我反正是大爱，因此一直在windows上找它的替代品，没想到还真被我找到了——oh-my-posh主题框架。 scoop进行安装：scoop install oh-my-posh 安装oh-my-posh模块：Install-Module oh-my-posh 设置robbyrussell主题：Set-Theme robbyrussell，这里由于本人比较喜欢linux的oh my zsh这个主题，因此选择这个和他接近的主题，大家也可以根据自己喜欢来选择自己的主题。2.3 设定每次开启都启用主题选择好了自己喜欢的主题，但是发现每次都需要打这一行Set-Theme robbyrussell代码才能启动主题，是不是觉着老蛋疼了，所以下面是介绍一劳永逸的方案。 建立一个profile： if (!(Test-Path -Path $PROFILE )) { New-Item -Type File -Path $PROFILE -Force } 修改下这个profile：vim $profile 添加自己喜欢的主题风格到profile中：Set-Theme robbyrussell接下来，你每次开启终端后发现自动加载自己喜欢的主题哦！三. 必备的一些软件推荐这里推荐的软件主要是利用scoop可以直接完成下载的。 语言：java、python、ruby等等你没听错，是的，可以直接用scoop来安装，环境变量都给你配好咯。 编辑器：vscode、pycharm、eclipse等等你能想到的免费的它都有！ 大数据：spark、hadoop、rabbitmq、kafka、flume等等 数据库：redis、mongodb、mysql 压测：jmeter、postman 其他需要的软件：cmake、tar、typora、vim、touch、youtube-dl、chrome、OpenSSL"
    } ,
  
    {
      "title"       : "Libtorch的GPU使用问题记录",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Libtorch%E7%9A%84GPU%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95.html",
      "date"        : "2021-01-31 04:21:00 +0800",
      "description" : "介绍pytorch的C++版本的gpu使用的解决问题的过程记录",
      "content"     : "这里得吹逼下自己领导，10min解决了困扰我2天的问题（好吧，也许是我太蠢）。一. 问题描述由于项目需要使用libtorch（pytorch的C++版本）的GPU版本，但是发现无法使用GPU，因此将问题和解决过程记录下来，方便日后观看和反思。二. 解决问题的过程2.1 使用的torch版本这里需要说下pytorch和libtorch的版本一定要一致，且和cuda的版本一致。这里都是通过pytorch官网上进行安装即可。 pytorch1.6.0（GPU）：使用pip安装 # CUDA 10.1pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html pytorch1.6.0（CPU）：使用pip安装 # CPU onlypip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html libtorch1.6.0（GPU）：选择使用release版本即可（据说debug有问题） https://download.pytorch.org/libtorch/cu101/libtorch-win-shared-with-deps-1.6.0%2Bcu101.zip libtorch1.6.0（CPU）：选择使用release版本即可（据说debug有问题） https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-1.6.0%2Bcpu.zip 2.2 使用cmakelist的搭建工程cmake_minimum_required(VERSION 3.12 FATAL_ERROR)project(torch_gpu_test)# add CMAKE_PREFIX_PATHlist(APPEND CMAKE_PREFIX_PATH \"D:/software/opencv/opencv/build/x64/vc15/lib\")list(APPEND CMAKE_PREFIX_PATH \"D:/software/libtorch_gpu\")list(APPEND CUDA_TOOLKIT_ROOT_DIR \"D:/software/cuda/development\")find_package(Torch REQUIRED)find_package(OpenCV REQUIRED)if(NOT Torch_FOUND) message(FATAL_ERROR \"Pytorch Not Found!\")endif(NOT Torch_FOUND)message(STATUS \"Pytorch status:\")message(STATUS \" libraries: ${TORCH_LIBRARIES}\")message(STATUS \"Find Torch VERSION: ${Torch_VERSION}\")message(STATUS \"OpenCV library status:\")message(STATUS \" version: ${OpenCV_VERSION}\")message(STATUS \" libraries: ${OpenCV_LIBS}\")message(STATUS \" include path: ${OpenCV_INCLUDE_DIRS}\")add_executable(torch_gpu_test torch_gpu_test.cpp)target_link_libraries(torch_gpu_test ${TORCH_LIBRARIES} ${OpenCV_LIBS})set_property(TARGET torch_gpu_test PROPERTY CXX_STANDARD 11) 这里利用vs2019生成项目之后，编写以下代码进行测试：#include &lt;torch/torch.h&gt;#include &lt;torch/script.h&gt;using namespace torch;int main(){ torch::DeviceType device_type = at::kCPU; if (torch::cuda::is_available()) { cout &lt;&lt; \"cuda!\" &lt;&lt; endl; torch::DeviceType device_type = at::kCUDA; } else { cout &lt;&lt; \"cpu\" &lt;&lt; endl; } } 到了这里我开始了我的问题之旅：由于是Release版本，不能debug，只能主观的认为这里应该是cuda的环境没配好导致torch无法使用gpu的，因此一直在找cmake的cuda环境配置问题。2.3 Release with Debug改变了我的想法 这里得说下本人第一次知道release版本也可以debug！（本人也算一C++小白哈，别计较）这里顺带记录下如何使用vs2019的Release with debug的过程： 直接在项目中将Release版本选择为RelWithDebInfo 禁用代码优化功能：这里是防止出现“变量已被优化掉 因而不可用”这种问题在这里debug的时候发现，device这个我定义的变量是可以加载cuda的！因此可以推翻我之前想的（cuda环境的问题）。2.4 libtorch1.6GPU版本问题这里就可以肯定是libtorch的GPU问题了。为啥torch::cuda::is_available()会是false呢？ 网上的思路是：在“属性 –&gt; 链接器 –&gt; 命令行 –&gt; 其他选项”中添加： /INCLUDE:?warp_size@cuda@at@@YAHXZ 本人实验了下，按照网上的添加会报错，因此以下是本人实验可行的结果：在“链接器 –&gt; 输入 –&gt; 附加依赖项”中进行添加：D:\\software\\libtorch_gpu\\lib\\torch_cuda.libD:\\software\\libtorch_gpu\\lib\\torch_cpu.lib-INCLUDE:?warp_size@cuda@at@@YAHXZ 这里很奇怪，cmakelist明明已经配置好了libtorch的gpu，但是这里却没有torch_cuda.lib至此，问题解决了！"
    } ,
  
    {
      "title"       : "GCN在多标签分类中的应用",
      "category"    : "",
      "tags"        : "深度学习, 图算法",
      "url"         : "./GCN%E5%9C%A8%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html",
      "date"        : "2021-01-11 04:21:00 +0800",
      "description" : "介绍在图卷积网络在多标签分类任务中的应用",
      "content"     : "一. Torch的图神经网络库pyG torch_geometric 官方文档：https://pytorch-geometric.readthedocs.io/en/latest/index.html1.1 安装及使用这里参考官网的安装过程。 确定自己安装的pytorch版本：pip list进行查看，例如本人的torch版本为1.6.0+cu101（这里的cu101是指cuda10.1） 安装相关的第三方包，这里注意要匹配上面的torch版本，因此：${TORCH} = 1.6.0，${CUDA} = cu101 pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.htmlpip install torch-geometric 安装完成之后，测试下import torch_geometric导包没有报错说明安装完成了。 1.2 图数据导入torch_geometric.data这个模块包含了一个叫Data的类，而这个类可以很方便的构建属于自己的数据集。data实例有以下属性： x：节点的特征矩阵，shape = [节点个数，节点的特征数]。 edge_index：这里可以理解为图的邻接矩阵，但是要注意这里要将邻接矩阵转换成COO格式，shape = [2, 边的数量]，type = torch.long。 edge_attr：边的特征矩阵，shape = [边的个数，边的特征数] y：标签，如果任务是图分类，shape = [1, 图的标签数]；如果是节点分类，shape = [节点个数，节点的标签数]。（这里注意一哈：在torch中如果是多分类任务，不用转成onehot形式哦，因此标签数为1） is_directed()：是否是有向图(1) 下面是edge_index的具体从邻接矩阵生成COO模式的代码。from scipy.sparse import coo_matrix # 转化成COO格式coo_A = coo_matrix(adj_arr)edge_index = [coo_A.row, coo_A.col](2) 构建自己的数据集，只需要用list来封装这些Data即可。具体代码如下：dataset = [Data(x,edge_index,y) for _ in range(10)]1.3 图数据的转换及展示我们可以利用networkx来对Data这个图进行展示和转换成networkx的图结构。from torch_geometric.utils.convert import to_networkximport networkx as nxdef draw(Data): G = to_networkx(Data) nx.draw(G) nx.write_gexf(G, \"test.gexf\") plt.savefig(\"path.png\") plt.show()同时，还可以将gexf格式的图数据文件经过Gephi这个开源的图数据展示软件来进行节点的渲染。二. 图卷积网络GCN在多标签分类中的应用 论文参考：Semi-Supervised Classification with Graph Convolutional Networks2.1 GCN在模型应用上的优缺点。本次探究的是图卷积网络在图分类（多标签）上的应用，因此不涉及到节点的分类任务。GCN的优点：可以捕捉图的全局信息，很好的表征节点的特征，边的特征。GCN的缺点：若是新增节点，整个图发生变化， 那么GCN的结构就会发生变化。因此对于节点不固定的图结构来说，不适用。GCN的主要作用：抽取图中节点的拓扑信息（节点的邻接信息）。这里学到的是每个节点的一个唯一确定的embedding。如下图所示，多层的GCN抽取的是每个节点的唯一确定的embedding。GCN的特性： 局部参数共享，算子是适用于每个节点（圆圈代表算子），处处共享。 感受域正比于层数，最开始的时候，每个节点包含了直接邻居的信息，再计算第二层时就能把邻居的邻居的信息包含进来，这样参与运算的信息就更多更充分。层数越多，感受域就更广，参与运算的信息就更多。2.2 GCN在图分类的模型搭建图分类任务下的模型搭建过程如下：因此，利用pytorch_geometric来搭建图分类任务（多标签）的模型。这里代码中引入了两次图卷积和池化。在输入的数据中，除了包含节点的特征，还包含了边的特征。import torchimport torch.nn.functional as Ffrom torch_geometric.nn import GraphConv, TopKPoolingfrom torch_geometric.nn import global_mean_pool as gapfrom torch_geometric.nn import global_max_pool as gmpclass Net(torch.nn.Module): def __init__(self, num_features,multi_label): super(Net, self).__init__() self.conv1 = GraphConv(num_features, 8) # self.conv1.weight.data.normal_() self.pool1 = TopKPooling(8, ratio=0.5) self.conv2 = GraphConv(8, 8) self.pool2 = TopKPooling(8, ratio=0.5) self.lin1 = torch.nn.Linear(16, 64) self.lin2 = torch.nn.Linear(64, 128) self.lin3 = torch.nn.Linear(128, multi_label) def forward(self, data): x, edge_index,edge_attr, batch = data.x, data.edge_index, data.edge_attr,data.batch x = F.relu(self.conv1(x,edge_index,edge_attr)) x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, edge_attr, batch) x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1) x = F.relu(self.conv2(x, edge_index,edge_attr)) x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, edge_attr, batch) x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1) x = x1 + x2 x = F.relu(self.lin1(x)) x = F.dropout(x, p=0.3, training=self.training) x = F.relu(self.lin2(x)) x = F.dropout(x, p=0.3, training=self.training) x = torch.sigmoid(self.lin3(x)) return x2.3 多标签（Multi-Label）分类任务在多标签分类任务中： 输入的y：shape = [batch_size, multi-label的个数]，其中multi-label的形式都是[0,1,0,0,1….]，即每个类别之间都互不影响，且结果只有0和1。这里在torch_geometric.data.y的shape = [1,multi-label的个数]。 分类模型的最后一层激活函数：torch.sigmoid()函数（即二分类常用的激活函数）,这里对于多标签分类任务同样适用。 损失函数的定义： torch.nn.BCELoss() 准确率的定义：在训练的时候，一般除了看训练集和验证集的loss以外，acc其实也可以当作模型好坏的指标。但是对于多标签分类而言，这里和一般的多分类，二分类任务定义的准确率不太一样。个人的理解（可能不对蛤）：对于一个样本（多标签）而言，有且仅有每个标签都预测对了，这个样本才能算预测正确了，因此，定义了以下acc。pred = torch.where(pred&gt;acc_thread ,torch.ones_like(pred),torch.zeros_like(pred))acc = 0for i in range(pred.shape[0]): if pred[i].int().equal(data.y[i]): acc +=1 epoch_accuracy += acc"
    } ,
  
    {
      "title"       : "windows下搭建libtorch和paddle的C++环境搭建",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./windows-%E4%B8%8B%E6%90%AD%E5%BB%BAlibtorch%E5%92%8Cpaddle%E7%9A%84C++%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html",
      "date"        : "2020-12-26 04:21:00 +0800",
      "description" : "介绍在C++平台下搭建torch和paddle的环境",
      "content"     : "参考文章：NSTALLING C++ DISTRIBUTIONS OF PYTORCH，安装与编译 Windows 预测库，在C++中加载PYTORCH模型一. 必要软件 vs2019：paddle和torch这里的编译都是由Visual Studio 2019完成的 libtorch：直接在官网上进行下载压缩包，这里说明下分为release和debug版本，直接下载release版本即可。 paddle：这里选择2.0-rc1的cpu版本的直接进行解压安装。 opencv：windows下直接安装exe到本地即可。 cmake：直接用scoop安装scoop install cmake二. 安装libtorch环境2.1 构建一个C++项目目录层级如下：├─example-app ├─build // 新建一个空目录 ├─CMakeLists.txt // 构建一个cmakelist └─example-app.cpp // 构建一个cpp文件用于测试其中，CMakeList.txt具体设置如下：cmake_minimum_required(VERSION 3.12 FATAL_ERROR)project(example-app)# add CMAKE_PREFIX_PATH#增加opencv和libtorch的路径list(APPEND CMAKE_PREFIX_PATH \"D:/software/opencv/opencv/build/x64/vc15/lib\") # 注意这里如果是vs2015的版本，需要改成 /build/x64/vc14/liblist(APPEND CMAKE_PREFIX_PATH \"D:/software/libtorch\")find_package(Torch REQUIRED)find_package(OpenCV REQUIRED)if(NOT Torch_FOUND) message(FATAL_ERROR \"Pytorch Not Found!\")endif(NOT Torch_FOUND)message(STATUS \"Pytorch status:\")message(STATUS \" libraries: ${TORCH_LIBRARIES}\")message(STATUS \"OpenCV library status:\")message(STATUS \" version: ${OpenCV_VERSION}\")message(STATUS \" libraries: ${OpenCV_LIBS}\")message(STATUS \" include path: ${OpenCV_INCLUDE_DIRS}\")add_executable(example-app example-app.cpp)target_link_libraries(example-app ${TORCH_LIBRARIES} ${OpenCV_LIBS})set_property(TARGET example-app PROPERTY CXX_STANDARD 11)C++测试代码（example-app.cpp）如下（测试opencv和libtorch）：#include &lt;torch/torch.h&gt;#include &lt;iostream&gt;#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;using namespace std;using namespace cv;int main() { torch::Tensor tensor = torch::rand({2, 3}); std::cout &lt;&lt; tensor &lt;&lt; std::endl; std::cout &lt;&lt; \"ok!\" &lt;&lt; std::endl; Mat img = imread(\"1.jpg\"); imshow(\"1\",img); waitKey(0); return 0;}2.2 编译和生成项目 进入到build目录：cd build 利用cmake进行编译： cmake .. 编译顺利的话，就可以看到build目录下生成了如下所示： 利用vs2019打开项目example-app.sln 点击example-app 右键选择设为启动项，并且将版本选择release版本，点击本地Windows调试器2.3 调试问题的解决 报错信息：由于找不到c10.dll，torch.dll这种找不到dll文件的，直接将dll文件(这些dll文件都在libtorch/lib路径下)复制到build/release文件夹下 opencv_world3411.dll和opencv_ffmpeg3411_64.dll等都在opencv的opencv\\opencv\\build\\x64\\vc15\\lib路径下。 这里注意测试opencv的时候，需要将图片放置到和example-app.vcxproj同级目录下2.4 exe生成文件的平台移植 如果需要将生成的exe文件移植到其他PC上面，只需要将release文件夹下所有文件（包括dll文件和exe文件）复制到其他PC即可。 生成的exe文件在找图片的时候也是同级目录下找，因此需要将图片放置到exe文件的同级目录下。2.5 pytorch模型在C++平台的使用PyTorch模型从Python到C++的转换由Torch Script实现。Torch Script是PyTorch模型的一种表示，可由Torch Script编译器理解，编译和序列化。一般利用trace将PyTorch模型转换为Torch脚本,必须将模型的实例以及样本输入传递给torch.jit.trace函数。这将生成一个 torch.jit.ScriptModule对象，并在模块的forward方法中嵌入模型评估的跟踪。三. 安装paddle的C++环境3.1 下载安装paddle这里官网有2种方式在windows上安装paddle环境：一个是通过git下载paddle源码进行编译安装，另一种直接从官网下载zip编译好的文件（本文使用该种方式）。3.2 结合paddleOCR测试并使用paddle预测库 paddleOCR的git地址：https://github.com/PaddlePaddle/PaddleOCR 下载到本地之后，cd PaddleOCR\\deploy\\cpp_infer，修改CMakeList.txt文件SET(PADDLE_LIB \"D:/software/paddle_inference_install_dir\") # 这里是下载的paddle预测库的路径SET(OPENCV_DIR \"D:/software/opencv/opencv\") # 这里是下载的opencv的路径find_package(OpenCV REQUIRED) 新建一个build文件夹：mkdir build 进入build：cd build ， 编译：cmake .. 同样的利用vs2019打开项目ocr_system.sln，生成即可。 这里注意需要将paddle_fluid.dll放入到Release目录下。"
    } ,
  
    {
      "title"       : "C++的包管理工具——VCPKG",
      "category"    : "",
      "tags"        : "coding",
      "url"         : "./C++%E7%9A%84%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7-VCPKG.html",
      "date"        : "2020-12-19 04:21:00 +0800",
      "description" : "介绍如何利用vcpkg来管理C++的库",
      "content"     : "在我最初学C/C++的时候，一直为要下各种第三方库而烦恼，为啥C++没有像python一样简单的包管理工具呢？于是，它来了——VCPKG。一. windows 下安装和配置VCPKG的官方git地址：https://github.com/microsoft/vcpkg1.1 软件及平台要求 windows7及其以上（本人的是Windows10） git VS2015及其以上（本人的是VS2019）1.2 windows下安装 git克隆下官方的git仓库：git clone https://github.com/microsoft/vcpkg 进入到仓库中：cd vcpkg，注意下这里官方建议把vcpkg目录放到C:\\src\\下。 安装vcpkg：boostrap-vcpkg.bat1.3 安装库的示例 比如需要安装opencv，可以先搜索下vcpkg是否支持：vcpkg.exe search opencv 进行库的编译安装：vcpkg.exe install opencv 需要在visual studio中直接使用opencv：vcpkg.exe integrate install二. 环境变量配置这里需要注意两点： 把vcpkg的路径添加到环境变量中：path = C:\\src\\vcpkg ，这样就可以随时随地使用vcpkg.exe咯，而不用每次到C:\\src\\vcpkg下执行命令。 vcpkg默认是安装32位的库，我是需要安装x64的库，因此需要添加一个系统变量：VCPKG_DEFAULT_TRIPLET=x64-windows三. VCPKG下载过慢解决方案3.1 先下载后编译vcpkg直接执行命令vcpkg.exe install opencv 的时候，会先下载需要的第三方包（下一个编译一个），那么有时候网速不好的时候，就会下不了，从而断掉，因此可以先把所有需要的库全部下载下来，再进行编译。 下载包，并编译：vcpkg.exe install opencv --only-downloads 对下载好的包继续编译：vcpkg.exe install opencv3.2 直接手动下载不好下载的包在VCPKG下载包的时候，总是会碰到下载突然卡住，其中一个包下载不下来的情况，直接手动去下载(如下图框出的链接地址)下来，然后放在C:\\src\\vcpkg\\downloads 下面，重新再次执行下载命令即可。"
    } ,
  
    {
      "title"       : "利用Jasper2主题和Netlify完善个人博客",
      "category"    : "",
      "tags"        : "博客",
      "url"         : "./Jasper2%E5%8D%9A%E5%AE%A2%E8%B7%9F%E6%96%B0.html",
      "date"        : "2020-11-27 04:21:00 +0800",
      "description" : "介绍如何利用Jasper2主题（jekyll）完成个人博客设置和利用netlify加速访问速度",
      "content"     : "一. 更换Jekyll主题 之前用的是jekyll主题：Flexible-Jekyll，如下图所示： 觉得有点过于单调和简单了，于是找到了命中注定：Jasper2,如下图所示： 如果也想使用这个Jasper2主题的话，最好多读读其作者的readme哦！1.1 在git仓库中推送生成好的html文件​ 克隆Jasper2项目（master分支）到自己本地，然后利用jekyll开启本地预览模式：bundle exec jekyll server，这里你会发现在本地的上级目录下生成一个jasper2-pages文件夹。 如果是和我一样是利用username.github.io建立的仓库，那么就直接在项目中新建一个_site文件夹，并将jasper2-pages内容复制到该目录下即可。 如果是利用github pages来展示自己的项目的话，那就建立一个gh-pages 分支，同时将jasper2-pages内容复制到该目录下即可。注意：每次跟新仓库代码后，都需要重新生成一次，并替换掉_site文件夹内容！1.2 内容替换 _config.yml：用来修改主页上的一些个人信息 about/index.md：修改关于的个人说明 _post：将其中的文章替换掉自己文章1.3 前端页面的修改 ps:本人也是个前端新手，也是根据李小肥的指导才知道怎么修改前端的，给你们撒狗粮，哈哈哈！这里就不讲修改细节部分了，主要讲述如何快速找到想要修改的代码，毕竟授人以鱼不如授人以渔嘛！ 进入到jasper2的展示主页，右键页面中的facebook图标点击检查,就可以看到以下界面： 在这个check界面上可以看到有个social-link social-link-fb类，那么如果我们想要修改这些内容，可以直接在项目代码中全文搜索，就可以在author.html中找到咯，但是这和我们想修改图标或者是内容不符啊，那么就往这个类的上面继续找site-nav-right,这里就指向了我们想要修改的图标。 如果还想修改css文件呢，可以看到上图中Styles下面中就是，还可以直接在这里修改，就可以同时在网页中预览哦！如何找这个css文件呢，旁边的screen.css:279就是，而且连行号都给你了！二. Netlify加速访问目前有一些提供域名解析、CDN加速的免费网站，其还可以在GitHub中挂载触发器，一旦发现GitHub Pages仓库变化了，立即同步编译发布，减少人工操作。而Netlify就是一个，当然还有Vecel这个。 经过本人的亲自实验，Jasper2这个无法在Vecel应用上（但是之前的flexible主题可以哦），毕竟Jasper2的作者都推荐你用Netlify，你还不用么！2.1 拥有一个专属于自己的域名如果你已经有自己的域名了，或者你不想要啥域名，直接用netlify提供的或者gitpages就挺好的，那就跳过这段吧！ 去阿某云买一个新鲜热乎的域名，有好多后缀可以选的，反正我是选了个便宜的，当然还有比我这个.website还low的.xyz和.top，反正看个人喜好和收入吧，毕竟是要花钱的。 注意这里一定要实名注册，买完域名才能使用！2.2 利用netlify进行加速 注册：这里无论是vercel还是netlify都是可以直接关联github账户的，因此直接用你的github账户进行注册即可。 添加github仓库：这里添加你的username.github.io，然后等待他发布即可。 添加自己的域名，然后按照要求，去你买域名的网站设置CNAME进行关联即可。三. 搜索引擎的个人站点入口除了能够不翻墙被人访问自己的博客，但是别人用搜索引擎搜不到你咋办呢？这就需要让各个搜索引擎收录自己的站点咯。3.1 搜索引擎提交个人站点的入口 百度 ： https://ziyuan.baidu.com/site/index 谷歌 ： https://www.google.com/webmasters/tools/home?hl=en 搜狗 ： http://zhanzhang.sogou.com/index.php/dashboard/index 360　： http://info.so.360.cn/site_submit.html Bing ： https://www.bing.com/toolbox/webmaster/3.2 验证站点的所有权这里几乎每个搜索引擎都会要你验证下自己是不是提交的站点的所有者，因此需要你验证下身份。我这里是通过添加主页的tag来进行验证的（可以添加好几个搜索引擎的哦）&lt;head&gt; &lt;meta name=\"baidu-site-verification\" content=\"自己的百度验证码\" /&gt; &lt;meta name=\"sogou_site_verification\" content=\"自己的搜狗验证码\"/&gt;&lt;/head&gt;"
    } ,
  
    {
      "title"       : "Keras训练模型部署到C++环境并生成DLL",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Keras%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0C++%E7%8E%AF%E5%A2%83%E7%94%9F%E6%88%90DLL.html",
      "date"        : "2020-11-03 22:21:00 +0800",
      "description" : "主要讲述将Keras训练好的.h5模型转换成.pt格式，并将其转换后的模型部署到C++环境中，并生成DLL",
      "content"     : "一. 准备工作 在Windows上搭建完成Tensorflow的C++环境，这里参考本人的上一篇博客 在windows上部署opencv3的C++环境 python3的keras和tf2.X的环境：pip3 install keras==2.4.3和pip3 install tensorflow==2.3.1 python3的opencv环境：pip3 install opencv-python 二. python模型训练 python模型构建：Kaggle猫狗分类 python完整代码放在本人的git仓库上面2.1 模型训练，保存.h5格式的模型文件在keras的model.save()模型保存的函数中，只支持2种保存方式： .h5格式的文件进行保存模型整个结构及其权重。 以文件夹（包含assets saved_model.pb variables）来保存，模型架构和训练配置（包括优化器、损失和指标）存储在 saved_model.pb 中。权重保存在 variables/ 目录下。 因此使用.h5格式来存储模型结构及权重。2.2 h5文件转pt文件将训练好的模型以.h5格式进行存储，再通过转成.pt格式的模型文件，提供C++的tensorflow调用模型。from keras.models import load_modelimport tensorflow as tfimport tensorflow.python.keras.backend as Kfrom tensorflow.python.framework import graph_io# 针对tf2.x来说不支持freezegraph的，这里需要使用tf1的方式tf.compat.v1.disable_eager_execution()def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True): from tensorflow.python.framework.graph_util import convert_variables_to_constants graph = session.graph with graph.as_default(): freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or [])) output_names = output_names or [] output_names += [v.op.name for v in tf.compat.v1.global_variables()] input_graph_def = graph.as_graph_def() if clear_devices: for node in input_graph_def.node: node.device = \"\" frozen_graph = convert_variables_to_constants(session, input_graph_def, output_names, freeze_var_names) return frozen_graph\"\"\"----------------------------------配置路径-----------------------------------\"\"\"h5_model_path = 'model/model.h5'pb_model_name = 'model.pt'output_path = '.'\"\"\"----------------------------------导入keras模型------------------------------\"\"\"K.set_learning_phase(0)net_model = load_model(h5_model_path)print('input is :', net_model.input.name)print('output is:', net_model.output.name)\"\"\"----------------------------------保存为.pb格式------------------------------\"\"\"sess = K.get_session()frozen_graph = freeze_session(K.get_session(),output_names=[out.op.name for out in net_model.outputs])graph_io.write_graph(frozen_graph, output_path, pb_model_name, as_text=False)2.3 测试pt模型文件对同一个图片分别利用.h5模型文件和.pt模型文件进行预测。.pt格式的模型预测代码如下：def pred_with_pt(img, pb_file_path): with tf.Graph().as_default(): output_graph_def = tf.compat.v1.GraphDef() # 打开.pb模型 with open(pb_file_path, \"rb\") as f: output_graph_def.ParseFromString(f.read()) tensors = tf.import_graph_def(output_graph_def, name=\"\") print(\"tensors:\", tensors) # 在一个session中去run一个前向 with tf.compat.v1.Session() as sess: init = tf.compat.v1.global_variables_initializer() sess.run(init) op = sess.graph.get_operations() input_x = sess.graph.get_tensor_by_name(\"conv2d_input:0\") # 具体名称看上一段代码的input.name print(\"input_X:\", input_x) out_softmax = sess.graph.get_tensor_by_name(\"dense/Softmax:0\") # 具体名称看上一段代码的output.name print(\"Output:\", out_softmax) img_out_softmax = sess.run(out_softmax, feed_dict={input_x: img}) return img_out_softmax三. C++ 模型预测3.1 将模型预测类进行封装并生成动态链接库DLL VS2019部署（参考本人的上一篇博客）：新建一个动态链接库(DLL)，新建一个头文件tf_clf.h和源文件tf_clf.cpp，这里是生成DLL包（点击生成 ==&gt; 重新生成DLLTF） 这里要把tensorflow_cc.dll放到生成的x64/release里面 这里还需要在Release属性页里配置C/C++的预处理器(防止后面编译时出现这种错误tstring.h(350,40): error C2589: “(”:“::”)： _XKEYCHECK_HNOMINMAX 头文件tf_clf.h中声明类的导出 class __declspec(dllexport) TFClf;class TFClf {private:vector&lt;float&gt; mean = { 103.939,116.779,123.68 };int resize_col = 224;int resize_row = 224;string input_tensor_name = \"conv2d_input\";string output_tensor_name = \"dense/Softmax\";Point draw_point = Point(50, 50);public:string image_path, model_path;TFClf(string img, string model) :image_path(img), model_path(model) {}void mat_to_tensor(Mat img, Tensor* output_tensor);Mat preprocess_img(Mat img);void model_pred();void show_result_pic(Mat img, int output_class_id, double output_prob);}; 源文件tf_clf.cpp完成类的具体实现，注意这里要#include \"pch.h\"3.2 新建一个项目测试DLL 新建一个控制台的空项目，并将打包好的DllTF.dll和DllTF.lib复制到工程中 配置属性管理器：这里需要在Release | x64添加之前配置好的opencv_release.props和tf_release.props。 这里要把tensorflow_cc.dll放到生成的x64/release里面 新建一个头文件tf_clf.h #pragma once#ifndef TF_CLF_H#endif // !TF_CLF_H#pragma comment(lib,\"DllTF.lib\")class __declspec(dllexport) TFClf;class TFClf {private: vector&lt;float&gt; mean = { 103.939,116.779,123.68 }; int resize_col = 224; int resize_row = 224; string input_tensor_name = \"conv2d_input\"; string output_tensor_name = \"dense/Softmax\"; Point draw_point = Point(50, 50);public: string image_path, model_path; TFClf(string img, string model) :image_path(img), model_path(model) {} void mat_to_tensor(Mat img, Tensor* output_tensor); Mat preprocess_img(Mat img); void model_pred(); void show_result_pic(Mat img, int output_class_id, double output_prob);}; 新建一个源文件main.cpp# include \"tf_clf.h\"int main() {string model_path = \"D:/yeyan/pycharm_project/dogcat/model/model.pt\";string img_path = \"D:/yeyan/pycharm_project/dogcat/data/test_set/test_set/cats/cat.4001.jpg\";TFClf clf = TFClf(img_path, model_path);clf.model_pred();}"
    } ,
  
    {
      "title"       : "Window下搭建Tensorflow的C++环境",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Window%E4%B8%8B%E6%90%AD%E5%BB%BATensorflow%E7%9A%84C++%E7%8E%AF%E5%A2%83.html",
      "date"        : "2020-11-02 03:21:00 +0800",
      "description" : "介绍如何在windows下搭建tensorflow的C++环境",
      "content"     : "参考Tensorflow官网安装文章：https://www.tensorflow.org/install/source_windows?hl=zh-cn一. 下载需要的软件 bazel：Google 的一款可再生的代码构建工具，类似于Cmake。使用scoop进行安装：scoop install bazel python3.7：这里最好用pip 安装下必要的第三方包，比如tensorflow,kears,numpy等。 下载官方源码：git clone https://github.com/tensorflow/tensorflow.git二. 进行bazel源码编译2.1 配置build cd到源码目录：cd tensorflow-master 通过在 TensorFlow 源代码树的根目录下运行以下命令来配置系统构建：python3 ./configure.py 这里选择的是cpu版本的，每个配置的选择如下：You have bazel 3.7.0 installed.Please specify the location of python. [Default is C:\\soft\\python3.7.9\\python3.exe]:Found possible Python library paths: C:\\soft\\python3.7.9\\lib\\site-packagesPlease input the desired Python library path to use. Default is [C:\\soft\\python3.7.9\\lib\\site-packages]Do you wish to build TensorFlow with ROCm support? [y/N]: nDo you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: yEigen strong inline overridden.Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Preconfigured Bazel build configs. You can use any of the below by adding \"--config=&lt;&gt;\" to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=mkl_aarch64 # Build with oneDNN support for Aarch64. --config=monolithic # Config for mostly static monolithic build. --config=numa # Build with NUMA support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects. --config=v2 # Build TensorFlow 2.x instead of 1.x.Preconfigured Bazel build configs to DISABLE default on features: --config=noaws # Disable AWS S3 filesystem support. --config=nogcp # Disable GCP support. --config=nohdfs # Disable HDFS support. --config=nonccl # Disable NVIDIA NCCL support.2.2 bazel编译 修改bazel中间文件存储的路径（磁盘可用空间 Release 版本 &gt;= 16G , Debug版本 &gt;= 40G 编译的中间文件默认会放到 C:\\用户\\你的账号名\\ _bazel_你的账号名 下. C 盘可能没有那么大的空间, 所以要改一下输出文件的路径），打开tensorflow文件夹，vim .bazelrc，在最后一行加上startup --output_user_root=D:/tf，如果不修改路径，可能会编译到一半就卡死。 bazel编译动态链接库命令（这里加上使用的最大内存）： bazel build --config=opt //tensorflow:tensorflow_cc.dll --local_ram_resources=1024 编译的过程可能会很长，千万不要以为有问题就Ctrl C了（分2个过程：下中间资源+编译），编译完成后会出现 Build completed successfully 编译好的库文件在tensorflow-master\\bazel-bin\\tensorflow目录下，分别是tensorflow_cc.dll和tensorflow_cc.dll.if.lib。 bazel编译头文件命令： bazel build --config=opt //tensorflow:install_headers --local_ram_resources=1024 编译好的头文件在tensorflow-master\\bazel-bin\\tensorflow\\include目录下。三. 新建项目测试 注意： ​ 1. 这里编译的是tensorflow的release版本，因此构建项目的时候把环境从debug变成release ​ 2. 在新建项目属性表（这里无论是opencv还是tensorflow）中，要选择release版本的x64（64位） 新建一个项目 在项目中新建一个文件夹存放之前编译好的头文件，库文件，具体结构如下所示├── tf_test// 整个项目 ├── x64 // 这里是生成解决方案得到的 ├── tf // 这里存放所有编译好的文件 ├──bin // 存放dll动态库文件 ├──tensorflow_cc.dll ├──lib // 存放静态库文件 ├──tensorflow_cc.lib ├──include // 直接是tensorflow编译好的include目录 ├──main.cpp 属性管理器 —— Release X64 —— 添加新项目属性表（如果代码中还需要添加opencv库的可以参考本人另一篇博客） VC++目录中的包含目录中添加：D:tf_test\\tf\\include VC++目录中的库目录中添加：D:tf_test\\tf\\lib 链接器——输入——附加依赖项中添加：tensorflow_cc.lib 选择项目为release和x64平台。 使用以下代码进行测试 #include &lt;iostream&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/opencv.hpp&gt;#include\"tensorflow/core/public/session.h\"#include\"tensorflow/core/platform/env.h\"using namespace std;using namespace tensorflow;using namespace cv;int main(){ const string model_path = \"D:\\\\code\\\\yinbao_face\\\\live.pb\"; const string image_path = \"0.jpg\"; Mat img = imread(image_path); cvtColor(img, img, COLOR_BGR2RGB); resize(img, img, Size(112, 112), 0, 0, INTER_NEAREST); int height = img.rows; int width = img.cols; int depth = img.channels(); // 图像预处理 img = (img - 0) / 255.0; // img.convertTo(img, CV_32FC3, 1.0 / 255, 0); // 取图像数据，赋给tensorflow支持的Tensor变量中 const float* source_data = (float*)img.data; Tensor input_tensor(DT_FLOAT, TensorShape({ 1, height, width, 3 })); auto input_tensor_mapped = input_tensor.tensor&lt;float, 4&gt;(); for (int i = 0; i &lt; height; i++) { const float* source_row = source_data + (i * width * depth); for (int j = 0; j &lt; width; j++) { const float* source_pixel = source_row + (j * depth); for (int c = 0; c &lt; depth; c++) { const float* source_value = source_pixel + c; input_tensor_mapped(0, i, j, c) = *source_value; //printf(\"%d\"); } } } Session* session; Status status = NewSession(SessionOptions(), &amp;session); if (!status.ok()) { cerr &lt;&lt; status.ToString() &lt;&lt; endl; return -1; } else { cout &lt;&lt; \"Session created successfully\" &lt;&lt; endl; } GraphDef graph_def; Status status_load = ReadBinaryProto(Env::Default(), model_path, &amp;graph_def); if (!status_load.ok()) { cerr &lt;&lt; status_load.ToString() &lt;&lt; endl; return -1; } else { cout &lt;&lt; \"Load graph protobuf successfully\" &lt;&lt; endl; } // 将graph加载到session Status status_create = session-&gt;Create(graph_def); if (!status_create.ok()) { cerr &lt;&lt; status_create.ToString() &lt;&lt; endl; return -1; } else { cout &lt;&lt; \"Add graph to session successfully\" &lt;&lt; endl; } cout &lt;&lt; input_tensor.DebugString() &lt;&lt; endl; //打印输入 vector&lt;pair&lt;string, Tensor&gt;&gt; inputs = { { \"input_1:0\", input_tensor }, //input_1:0为输入节点名 }; // 输出outputs vector&lt;Tensor&gt; outputs; vector&lt;string&gt; output_nodes; output_nodes.push_back(\"output_1:0\"); //输出有多个节点的话就继续push_back double start = clock(); // 运行会话，最终结果保存在outputs中 Status status_run = session-&gt;Run({ inputs }, { output_nodes }, {}, &amp;outputs); Tensor boxes = move(outputs.at(0)); cout &lt;&lt; boxes.DebugString() &lt;&lt; endl; //打印输出 double end = clock(); cout &lt;&lt; \"time = \" &lt;&lt; (end - start) &lt;&lt; \"\\n\"; if (!status_run.ok()) { cerr &lt;&lt; status_run.ToString() &lt;&lt; endl; return -1; } else { //cout &lt;&lt; \"Run session successfully\" &lt;&lt; endl; }}四. 测试中出现的问题4.1 生成解决方案的时候报错无法打开包括文件：解决方式：在本地的通过python pip安装后的tensorflow文件夹中（C:\\soft\\python3.7.9\\Lib\\site-packages\\tensorflow\\include）将google文件夹复制到D:tf_test\\tf\\include下面，即可解决4.2 生成解决方案的时候报错Link1120:解决方式：将vs2019上报错信息复制，cd到tensorflow-master\\tensorflow\\tools\\def_file_filter(这里的tensorflow-master是自己下载tensorflow源码的地方），编辑def_file_filter.py.tpl文件：# Header for the def file. (找到这一行代码) if args.target: def_fp.write(\"LIBRARY \" + args.target + \"\\n\") def_fp.write(\"EXPORTS\\n\") def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\") # 下面两个就是复制的错误信息 def_fp.write(\"\\t ?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z\\n\") def_fp.write(\"\\t ??0SessionOptions@tensorflow@@QEAA@XZ\\n\")重新编译DLL，头文件（虽然很麻烦，但是还是得做啊）4.3 有太多的错误导致IntelliSense引擎无法正常工作,其中有些错误无法在编辑器解决方式：在项目-&gt;属性-&gt;配置属性-&gt;C/C++-&gt;预处理器-&gt;预处理器定义中加入_XKEYCHECK_H就消失了4.4 找不到tensorflow_cc.dll文件解决方式：将tensorflow_cc.dll文件复制到x64/release文件夹下。"
    } ,
  
    {
      "title"       : "pytorch使用YOLOv5训练数据",
      "category"    : "",
      "tags"        : "深度学习, 机器视觉",
      "url"         : "./pytorch%E4%BD%BF%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE.html",
      "date"        : "2020-10-27 03:21:00 +0800",
      "description" : "介绍如何利用官方开源的Yolov5训练一个属于自己的模型",
      "content"     : "一. 需要下载的资源 Fork 下官方的开源项目：https://github.com/ultralytics/yolov5 git clone 下Fork之后的项目到自己本地仓库中。 采用的训练集（简单的，仅有一个类）：源自Kaggle的小麦数据集 如果有gpu的话，最好安装cuda进行训练加速，这里可以参考本人的另一篇文章：Windows10环境下搭建CUDA10.1和pytorch1.6二 . 构建属于自己的目标检测模型2.1 在官网的开源yolo5项目的基础上进行构建 git 克隆到本地仓库：git clone https://github.com/xx/yolov5.git 进入项目中，并安装需要的第三方依赖：pip install -r requirements.txt 新建一个原始数据的目录：：mkdir ori_data，将下载好的小麦数据集解压后放到项目。 创建输出一个文件输出目录：mkdir wheat_data，并在此目录下新建以下目录，如下图所示 ├─images│ ├─train│ └─val└─labels ├─train └─val 构建数据集，新建一个munge_data.py文件import osimport pandas as pdimport numpy as npimport ast from sklearn.model_selection import train_test_splitfrom tqdm import tqdmimport shutilDATA_PATH = 'ori_data'OUTPUT_PATH = 'wheat_data'def process_data(data,data_type = 'train'): for _,row in tqdm(data.iterrows(),total=len(data)): image_name = row['image_id'] bounding_boxes = row['bbox'] yolo_data = [] for bbox in bounding_boxes: x = bbox[0] y = bbox[1] w = bbox[2] h = bbox[3] x_center = x+w/2 y_center = y+h/2 # 这里需要将图像数据归一化处理（yolo需要的输入为归一化后的数据,且为浮点数） x_center /= 1024.0 y_center /= 1024.0 w /= 1024.0 h /= 1024.0 yolo_data.append([0,x_center,y_center,w,h]) yolo_data = np.array(yolo_data) # 保存bbox的图片信息 np.savetxt( os.path.join(OUTPUT_PATH,f'labels/{data_type}/{image_name}.txt'), yolo_data, fmt=['%d','%f','%f','%f','%f'] ) # 将目标图片文件保存到指定文件中 shutil.copyfile( os.path.join(DATA_PATH,f'train/{image_name}.jpg'), os.path.join(OUTPUT_PATH,f'images/{data_type}/{image_name}.jpg'), )if __name__ == \"__main__\": df = pd.read_csv(os.path.join(DATA_PATH,'train.csv')) # 将string of list 转成list数据 df.bbox = df.bbox.apply(ast.literal_eval) # 利用groupby 将同一个image_id的数据进行聚合，方式为list进行，并且用reset_index直接转变成dataframe df = df.groupby(['image_id'])['bbox'].apply(list).reset_index(name = 'bbox') # 划分数据集 df_train,df_val = train_test_split(df,test_size=0.2,random_state=42,shuffle=True) # 重设 index （这里数据被打乱，index改变混乱） df_train = df_train.reset_index(drop=True) df_val = df_val.reset_index(drop=True) process_data(df_train,data_type='train') process_data(df_val,data_type='val') 运行构建数据的py文件：python munge_data.py 这里可以看到在输出结果目录中，放入了需要的整理后的数据集 新建一个wheat.yamlyaml文件，指定模型训练时候的输入及其类别（注意这里冒号后面要加空格，yamal格式问题） train: wheat_data/images/train // 指定训练目录val: wheat_data/images/val // 指定验证目录nc: 1 // 指定类别names: [\"wheat\"] // 指定类别名字 进行模型训练：python3 train.py --img 1024 --batch 8 --epoch 100 --data wheat.yaml --cfg .\\models\\yolov5s.yaml --name wm 这里可能会报错（Dataloader中设置了多进程导致的），报错信息如下所示：File \"C:\\soft\\python3.7.9\\lib\\multiprocessing\\reduction.py\", line 60, in dump ForkingPickler(file, protocol).dump(obj) BrokenPipeError: [Errno 32] Broken pipe这里可以参考文章：https://github.com/pytorch/pytorch/issues/2341。解决方案：将utils\\datasets.py文件中num_workers改成0即可（代码第68行）。训练完成后如下图： 可以查看本地tensorboard训练过程：tensorboard --logdir=runs，如果这里出现问题，可以换成一下命令：python -m tensorboard.main --logdir logs 这里还可以使用coco数据集的预训练模型进行训练，可能效果会更好python3 train.py --img 1024 --batch 8 --epoch 100 --data wheat.yaml --cfg .\\models\\yolov5s.yaml --name wm --weights 将训练好的模型放到当前文件夹下：cp runs/exp0_wm/weights/best.pt . 选择测试图片的文件夹进行生成测试：python detect.py --source ./test_data --weights best.pt ，这里可以看到新生成一个文件夹inference/output中就是测试后标记bbox后的图片。"
    } ,
  
    {
      "title"       : "Windows10环境下搭建CUDA10.1和pytorch1.6",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%AD%E5%BB%BACUDA10.1%E5%92%8Cpytorch1.6.html",
      "date"        : "2020-10-20 18:18:00 +0800",
      "description" : "介绍如何在Windows10环境中搭建GPU使用环境和pytorch",
      "content"     : "一. 安装CUDA10.1 这里需要安装鲁大师，查看自己电脑的显卡型号，这里是gtx 1060 ，6g1.1 安装Nvidia驱动 首先在Nvidia官网上安装显卡驱动，连接地址：https://www.nvidia.com/Download/index.aspx 这里需要去nVidia官网 https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html 查看cuda和显卡驱动对应表上cuda10.1对应驱动的版本号。 查看自己安装驱动的版本号：NVIDIA控制面板-帮助-系统信息中查看，这里可以看到我的驱动的版本号为456，这里是大于官网安装cuda10.1需要的418的驱动的。1.2 安装CUDA10.1 安装CUDA：适合哪个版本的CUDA，就可以去官网下载对应的CUDA了，但是官网首页的CUDA一般是最新版，我们可能需要下载旧版本比如CUDA10.1，可以cuu点击下面连接进行下载：https://developer.nvidia.com/cuda-toolkit-archiv，这里直接选择CUDA Toolkit 10.1 (Feb 2019)即可。这里一般选择自定义安装，可以通过命令行查看是否安装成功：nvcc -V，这里如果提升没有整个命令，说明还没有将cuda的路径添加到环境变量中，需要设置环境变量，添加CUDA安装目录下的bin和libnvvp目录，如下所示： 安装cuDNN：选择对应的版本号和系统，这里注意官网最前面的几个连接中都是windows10 的x86（32位）的，这里需要选择老一点的cuDNN的版本，如下所示 下载后，将压缩包解压得到cuda文件夹，文件夹下有三个文件夹，复制这三个文件夹到CUDA安装的目录’D:\\soft\\cuda’下，会自动将cudnn的三个文件夹的文件合并到其三个同名文件夹bin、include和lib中。 查看自己电脑中Nvidia的GPU信息：nvidia-smi二. 安装pytorch pytorch官网安装地址：https://pytorch.org/get-started/locally/2.1 修改pip源，提升下载包速度如果本地pip下载很慢，修改pip源：Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)内容如下：[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.comwindows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini。内容同上。2.2 通过pip安装pytorch这里可以直接访问pytorch官网来选择适合自己的版本：pip安装：pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html2.3 查看是否安装成功，并查看能否驱动cudaimport torchprint(torch.cuda.is_available())"
    } ,
  
    {
      "title"       : "jekyll和github pages搭建个人博客",
      "category"    : "",
      "tags"        : "博客",
      "url"         : "./Mac%E4%B8%8B%E5%88%A9%E7%94%A8jekyll%E5%92%8Cgithub-pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html",
      "date"        : "2020-10-18 03:21:00 +0800",
      "description" : "在Windows环境中利用jekyll来本地测试jekyll主题，并结合github pages来搭建个人的博客",
      "content"     : "一. 搭建环境1.1 下载软件 jekyll：这个是将纯文本转化为静态网站和博客，使用gem安装下载：gem install bundler jekyll。 github pages：免费开源，并且可以自动生成域名，自己去构建一个属于自己的github账号和新建一个仓库（名字为：XX.github.io，这里XX就是你自己的账号名称） mac或者linux或者Windows平台。1.2 选择一个适合自己的博客主题 jekyll主题：http://jekyllthemes.org/ jekyll插件：http://www.jekyll-plugins.com/ 本人选择的jekyll主题：Flexible-Jekyll，如下图所示： 选择一个地方存放下载的主题（直接git下载）：git clone https://github.com/artemsheludko/flexible-jekyll 这里直接在本地利用jekyll生成一个网页进行测试和调试：bundle exec jekyll server或者是bundle exec jekyll s，这里会生成一个本地的博客地址：http://127.0.0.1:4000/，可以直接查看1.3 将主题放入自己的仓库中 git下载创建好的xx.github.io该仓库到自己本地：git clone https://github.com/xx/xx.github.io.git 将之前下载的主题放到自己创建的github.io这个仓库中 git上传修改的地方： git添加修改的地方：git add . git提交：git commit -m \"修改\" git推送：git push 这里可以直接在网页上打开https://xx.github.io/查看自己的博客，当然这里还是别人的内容，还需要自己修改成属于自己专属的博客。 这里注意：如果无法访问github pages，这里需要修改dns服务器就可以了，修改为114.114.114.114，具体如下所示：二. 创建一个专属于自己的个人博客2.1 了解下载主题的文件和文件夹的作用和内容整个下载jekyll主题的代码结构如下图所示： _drafts文件夹：主要是存放一些自己还没有写好的markdown文档，这里是不会在网页上展示的，但是没有写完的却可以通过git来保存 _posts文件夹：保存已经写好的markdown文章 assets文件夹：保存一些图片和css的一些文件 _config.yml：用来修改主页上的一些个人信息 Gemfile：这里是该主题下需要的一些gem依赖，这里直接在当前目录下bundle install即可下载安装依赖，注意这里如果gem下载很慢，可以设定source源，直接在Gemfile文件开头写source 'https://gems.ruby-china.com/'2.2 如何在博客中展示公式如果自己的博客中需要展示数学公式的，那么需要在_layouts/default.html文件中进行添加：&lt;/script&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"&gt;&lt;/script&gt;2.3 打开自己博客网页响应很慢这里一般是自己博客中展示的图片资源过大导致的，这里需要将图片进行无损压缩，重新上传即可。这里推荐几个图片无损压缩的网站。 TinyPNG：https://tinypng.com/ Compressor：https://compressor.io/三. Windows下安装jekyll这里由于jekyll一般在mac和linux上安装较为方便，官方也有指导，但是windows下官方写的不详细，因此这里主要介绍下Windows下如何安装jekyll。3.1 软件安装 Ruby+Devkit 2.7.2(x64)：安装的时候注意添加到Path中，其需要安装MSYS2 and MINGW development toolchain RubyGems：这里直接选择zip进行安装即可。3.2 Jekyll及gem依赖的安装 cd到解压后的RubyGems的文件中：ruby setup.rb 安装jekyll：gem install jekyll 安装jekyll-paginate：gem install jekyll-paginate 安装bundler（注意这里在后面安装gem依赖需要）：gem install bundler 验证下jekyll和bundler：jekyll -v和bundler -v3.3 在本地运行静态博客 cd到自己博客中，安装gem依赖：bundle install 本地运行服务：bundle exec jekyll s"
    } ,
  
    {
      "title"       : "Windows下安装mysql和导入sql文件",
      "category"    : "",
      "tags"        : "数据库",
      "url"         : "./Windows%E4%B8%8B%E5%AE%89%E8%A3%85mysql%E5%92%8C%E5%AF%BC%E5%85%A5sql%E6%96%87%E4%BB%B6.html",
      "date"        : "2020-10-17 03:21:00 +0800",
      "description" : "介绍在Windows环境下本地安装mysql和导入sql文件的使用",
      "content"     : "一. 下载软件 mysql：这里使用的是scoop来进行安装：scoop install mysql，这里的优势是自动帮你配好环境了 安装Navicat Premium 破解Navicat Premium二. 初始化mysql 初始化数据库：mysqld --initialize --console,并记录红色标注的字符，这是随机生成的密码 输入mysqld -install将mysql安装为Windows的服务： 启动mysql：net start mysql 首次进入mysql：mysql -u root -p，输入第一次的系统生成的密码 输入ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'mysql的密码';回车 别漏了后面有个分号 mysql的密码是安装mysql时设置的密码 输入FLUSH PRIVILEGES;，这里一定要输入，不然用navicat链接的时候会报1251连接不成功 修改my.ini文件：首先进入scoop安装的mysql文件夹中（C:\\Users\\Administrator\\scoop\\apps\\mysql\\8.0.21），修改my.ini文件，如果不加secure_file_priv=''，会导致无法导入导出数据。[mysqld]datadir=D:/yeyan/mysql_data/datasecure_file_priv='' [client]user=root三. 导入.sql文件 启动mysql：net start mysql 首次进入mysql：mysql -u root -p，输入自己的密码 查看数据库：show databases; 使用某个数据库：use test 查看该数据库下的表：show tables; 导入sql文件：source D:/git_repo/Trace/data.sql;"
    } ,
  
    {
      "title"       : "Visual Studio 2019 下搭建opencv3.4.11的C++环境",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./Visual-Studio-2019-%E4%B8%8B%E6%90%AD%E5%BB%BAopencv3.4.11%E7%9A%84C++%E7%8E%AF%E5%A2%83.html",
      "date"        : "2020-10-16 03:21:00 +0800",
      "description" : "在Windows下利用Visual Studio2019 来搭建opencv3.4.11的C++环境",
      "content"     : "一. 下载需要的软件 visual studio 2019 社区版 opencv3.4.11二. 基于C++的环境搭建2.1 创建系统环境变量 解压opencv，到D:\\software 配置系统变量：Path下添加Opencv的路径D:\\software\\opencv\\opencv\\build\\x64\\vc15\\bin（这里选择vc15更适合vs2019，如果是vs2015就选择vc14） 2.2 在Visual Studio2019中配置Opencv 选择视图-属性管理器- 选择Debugx64-添加新项目属性表-这里选择保存的名称和位置 选择VC++目录-包含目录中添加以下 D:\\software\\opencv\\opencv\\build\\includeD:\\software\\opencv\\opencv\\build\\include\\opencvD:\\software\\opencv\\opencv\\build\\include\\opencv2 选择VC++目录-库目录中添加D:\\software\\opencv\\opencv\\build\\x64\\vc15\\lib 选择链接器-输入-附加依赖项中添加opencv_world3411d.lib 保存即可，注意这里构建的新建项目属性表可以保存下来，直接其他的项目直接导入用即可（视图-属性管理器- 选择Debugx64-添加现有属性表） 回到解决方案资源管理器-项目-属性-配置管理器-活动解决方案平台-选择x64-Debug三. 构建代码测试 构建cpp源码：解决方案-源文件-添加-新建项-cpp文件用以下代码进行测试#include&lt;iostream&gt;#include&lt;opencv2/core/core.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/opencv.hpp&gt;#include&lt;math.h&gt;using namespace cv;using namespace std;int main(){ //Mat img = imread(\"D:\\\\vs_project\\\\opencvtest\\\\1.jpg\"); Mat img = imread(\"D:/vs_project/opencvtest/1.jpg\"); if (img.empty()) { cout &lt;&lt; \"Could not load img...\" &lt;&lt; endl; return -1; } namedWindow(\"ori_img\", WINDOW_AUTOSIZE); imshow(\"ori_img\", img); // 图像转成灰度图像 Mat gray_img; cvtColor(img, gray_img, CV_RGB2GRAY); namedWindow(\"gray_img\", WINDOW_AUTOSIZE); imshow(\"gray_img\", gray_img); waitKey(0); return 0;}"
    } ,
  
    {
      "title"       : "Xcode搭建Opencv3环境",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./Xcode%E6%90%AD%E5%BB%BAOpencv3%E7%8E%AF%E5%A2%83.html",
      "date"        : "2020-10-14 03:21:00 +0800",
      "description" : "在Mac中利用Xcode神器搭建opencv3的C++环境",
      "content"     : "1. 下载opencv 使用简单粗暴的方式——brew进行安装：brew install opencv@3，注意这里通过brew下载的opencv3的地址为：/usr/local/Cellar/opencv@3/3.4.9_1（后面配置include和lib有用）。 这里存在很大的问题：brew除了下载opencv以外还需要下载opencv的依赖包（很多），这里强力推荐换brew的镜像源（本人用的清华的，当然也可以用中科大的）。具体配置方式如下： 第一步：替换brew.git： cd \"$(brew --repo)\"git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git 第二步：替换 homebrew-core.git： powershell cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git 2. 在Xcode上搭建opencv的环境 新建项目：macOS - Command Line Tool - 这里选择语言为C++ 点击项目，选择Build Settings- 在搜索框中搜索search。 在头文件路径Header Search Paths中debug中添加一下/usr/local/Cellar/opencv@3/3.4.9_1/include/usr/local/Cellar/opencv@3/3.4.9_1/include/opencv/usr/local/Cellar/opencv@3/3.4.9_1/include/opencv2 在Library Search Paths中添加/usr/local/Cellar/opencv@3/3.4.9_1/lib 在项目中添加动态链接库文件：选择项目- 右键New Group - 新建一个名字（比如lib）- 右键lib - Add files to - 按下/会直接提示到那个目录下找dylib，这里是/usr/local/Cellar/opencv@3/3.4.9_1/lib，把当前目录下的所有dylib都添加进去即可，如下图。 以上就是整个opencv3在Xcode的环境了。3. 测试案例#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;using namespace std;using namespace cv;int main(int argc, const char * argv[]) { // insert code here... cout &lt;&lt; \"This is my first try C++ in xcode!\\n\"; Mat img = imread(\"/Users/xcode_project/C++_project/opencvTutorial/test.jpeg\"); if (img.empty()){ cout &lt;&lt; \"Could not open image ...\"&lt;&lt; endl; return -1; } namedWindow(\"test\",CV_WINDOW_AUTOSIZE); imshow(\"test\", img); waitKey(0); return 0;}"
    } ,
  
    {
      "title"       : "深度学习算法面试总结",
      "category"    : "",
      "tags"        : "机器学习, 深度学习, 面试",
      "url"         : "./%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93.html",
      "date"        : "2020-06-12 23:21:00 +0800",
      "description" : "面试中机器学习，深度学习中常问的算法总结",
      "content"     : "面试官会根据自己简历中提到的一些点进行提问，这里先自己对某些点进行深挖。一.数据处理海量数据： （1）数据量太大，无法短时间内处理完成 （2）无法一次性将数据放入内存中。1.1 缺失值处理 填充固定值：选取某个固定值/默认值填充缺失值。 填充均值：对每一列的缺失值，填充当列的均值。 填充中位数：对每一列的缺失值，填充当列的中位数。 填充众数：对每一列的缺失值，填充当列的众数。由于存在某列缺失值过多，众数为nan的情况，因此这里取的是每列删除掉nan值后的众数。 填充上下条的数据：对每一条数据的缺失值，填充其上下条数据的值。 填充插值得到的数据：用插值法拟合出缺失的数据，然后进行填充。插值是离散函数逼近的重要方法，利用它可通过函数在有限个点处的取值状况，估算出函数在其他点处的近似值。二.机器学习2.1 SVM和LR的区别与联系？SVM 和 LR 都是属于分类算法，不过 SVM 是通过划分超平面的方法来进行分类，而 LR 则是通过计算样本属于哪个类别的概率，从而达到分类效果2.2 交叉熵函数系列问题？与最大似然函数的关系和区别？在二分类中，交叉熵函数和负最大似然函数的表达式是相同的，但是交叉熵函数是从信息论角度得到的，而最大似然函数则是从概率论角度得到的交叉熵涉及到2点： 信息量：假设X是一个离散型随机变量，其取值集合为X，概率分布函数为p(x)=Pr(X=x),x∈X，我们定义事件X=x0的信息量为：I(x0)=−log(p(x0))，可以理解为，一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。 熵：假设小明的考试结果是一个0-1分布XA只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。 相对熵：称为KL散度，是两个随机分布间距离的度量。越小说明分布越一致。 交叉熵：交叉熵与KL距离在行为上是等价的，都反映了分布p，q的相似程度。特别的，在logistic regression中，p:真实样本分布，服从参数为p的0-1分布，即X∼B(1,p)X∼B(1,p)q:待估计的模型，服从参数为q的0-1分布，即X∼B(1,q)两者的交叉熵为2.3 SVM的核函数使用非线性核的支持向量机可以处理线性不可分的问题。通过核函数，支持向量机可以将特征向量映射到更高维的空间中，使得原本线性不可分的数据在映射之后的空间中变得线性可分，如下图所示，原本二维空间的线性不可分（异或问题）转成三维空间，就可以线性可分了。常用的核函数：线性核、多项式核、高斯核（RBF）、拉普拉斯核等等。核函数的选择其实才是SVM模型的最大变数。2.4 L1和L2范数范数的定义：\\(\\|\\mathbf{x}\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}\\)L1范数就是p=1,即：\\(\\|\\boldsymbol{x}\\|_{1}:=\\sum_{i=1}^{n}\\left|x_{i}\\right|\\)L2范数就是p = 2，即:\\(\\|\\boldsymbol{x}\\|_{2}:=\\sqrt{x_{1}^{2}+\\cdots+x_{n}^{2}}\\)这里如果需要求解如何使得上述式子最小，无可避免三步走：求导，置零，解方程。因此L2范数计算就比L1范数计算更容易，因此L2范数应用较多。L1 和 L2 范数在机器学习上最主要的应用大概分下面两类： 作为损失函数使用(计算回归问题中需要计算拟合的线和点之间的距离)，这里L1是LAD（最小绝对偏差），L2是最小二乘法 作为正则项使用（防止过拟合）也即所谓 L1-regularization 和 L2-regularization：这里就是将x替换成权重w，这两个正则项最主要的不同，包括两点：如上面提到的，L2 计算起来更方便，而 L1 在特别是非稀疏向量上的计算效率就很低；还有就是 L1 最重要的一个特点，输出稀疏，会把不重要的特征直接置零，而 L2 则不会；最后，如之前多次提过，L2 有唯一解，而 L1 不是。2.5 决策树决策树属于典型的“白盒模型”，如下图所示，我是否应该接收一个新的offer？这里可以通过构建一个个节点，来判断我是否应该接收offer。比较常用的决策树有ID3，C4.5和CART（Classification And Regression Tree）。熵：熵是随机变量的不确定程度。越混乱熵值越高，说明越混乱，分类越混乱。当熵值为0时，说明是纯物质。以下是熵的公式。\\(H(X)=-\\sum_{i=1}^{n} p_{i} \\log p_{i}\\)​ 当Entropy最大为1的时候，是分类效果最差的状态，当它最小为0的时候，是完全分类的状态。因为熵等于零是理想状态，一般实际情况下，熵介于0和1之间。熵的不断最小化，实际上就是提高分类正确率的过程。例子：A = [1,1,1,1,2,2,1,1,1,1] , B = [1,2,3,4,5,6,6,7,8,0] ，显然A集合的熵值小很多。信息增益 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 C4.5决策树学习应用信息增益准则选择特征。信息增益的定义：给定训练数据集D和特征A，经验熵H（D）表示对数据集D进行分类的不确定性。而经验条件熵H（D|A）表示在特征A 给定的条件下对数据集D进行分类的不确定性，那么它们的差，即信息增益。如下公式：\\(g(D, A)=H(D)-H(D \\mid A)\\)表示由于特征A而使得对数据集D的分类的不确定性减少的程度。不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。基尼指数 CART决策树采用基尼指数(Gini index)来选择划分特征。基尼指数的定义：在数据集中随机抽取2个样本，其类别不一样的概率。因此Gini越小，数据集D纯度越高。\\(\\operatorname{Gini}(D)=\\sum_{k=1}^{Y} \\sum_{k \\neq j} p_{k} p_{j}=1-\\sum_{k=1}^{Y} p_{k}^{2}\\)决策树的过拟合决策树很容易发生过拟合的现象。原因是由于可以通过不断的分枝使得信息熵为0.如何解决该现象？进行剪枝： 预剪枝：在决策树生成过程中，对每个节点划分前估计出验证集的精度决定是否划分。 后剪枝：先训练完成一个完整的决策树，再自底而上进行剪枝。2.6 随机森林随机森林属于集成学习的Bagging方法，同时也是由多个弱分类器构建的强分类器。森林： 随机森林是由很多决策树并行构成的，决策树之间没有关联。 当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。随机： 样本的随机：在训练过程中，输入到每个决策树的样本是从总体样本中随机抽样的。 决策树节点的随机：对每个决策树而言，其节点属性都是从总的属性中随机抽取的。特点： 由于是并行模型，训练快。 得到不同特征的对模型的重要程度。 不容易过拟合。 如果有很大一部分的特征遗失，仍可以维持准确度。三. NLP3.1 什么是TF-IDF?词频-逆文档频率TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。\\(\\text { 词频(TF) }=\\frac{\\text { 某个词在文章中的出现次数 }}{\\text { 文章的总词数 }}\\)\\(\\text { 逆文档频率(IDF) }=\\log \\left(\\frac{\\text { 语料库的文档总数 }}{\\text { 包含该词的文档数 } x+1}\\right)\\)当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。3.2 什么是word2vec判断一个词的词性（动词，名词）这里可以用word2vec，嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec是词嵌入的一种。 Skip-gram 模型：用一个词语作为输入，来预测它周围的上下文 CBOW 模型：拿一个词语的上下文作为输入，来预测这个词语本身3.3 fastText word2vec的CBOW模型架构和fastText模型非常相似 fastText 和CBOW差别：CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。3.4 NER named-entity-recognition（命名实体识别，又叫“专名识别”）。指识别文本中具有特定意义的实体，主要包括人名，地名，机构名，专有名词。NER系统就是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体，比如产品名称、型号、价格等。学术上NER所涉及的命名实体一般包括3大类（实体类，时间类，数字类）和7小类（人名、地名、组织机构名、时间、日期、货币、百分比）。货币、百分比等数字类实体可通过正则搞定。 NER是NLP中一项基础性关键任务。从自然语言处理的流程来看，NER可以看作词法分析中未登录词识别的一种，是未登录词中数量最多、识别难度最大、对分词效果影响最大问题。同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。 原句：姚明在NBA打篮球 如下标签：姚/B-PER 明/I-PER 在/O NBA/B_ORG 打/O 篮/O 球/O其中常见的方法是对字或者词打上标签。B-type, I-type, O， 其中B-type表示组成该类型实体的第一个字或词。I-type表示组成该类型实体的中间或最后字或词，O表示该字或词不组成命名实体，当然有的地方也采用B-type, I-type, E-type，O形式。整体结构如下： 字（词嵌入）==&gt; BiLSTM（拿到字的每一个标签的所有得分）==&gt; CRF（输出预测标签值） 这里问什么要用到CRF(直接用全连接分类即可)？==&gt; CRF层能从训练数据中获得约束性的规则：CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。在训练数据训练过程中，这些约束可以通过CRF层自动学习到。 这些约束可以是： I：句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” II：标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列. III：标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。 有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。 CRF（条件随机场）：属于判别式模型，条件随机场对多个变量在给定观测值后的条件概率进行建模。概率图模型是以某些可观测的变量为条件分布进行推断。假设某个字的前后（x_1,x_2,x_3）,推断问题的目标就是计算2在1的条件下发生的概率，然后所有条件概率相加。3.5 文本增强技术 词汇和短语进行替换：选择同义词进行替换；空间中找到相邻的词汇进行替换；利用TF-IDF对哪些非核心词汇（分值很低的）进行替换 随机噪音：随机插入一些词汇，占位符；交换词汇或者shuffle句子；随机删除词汇或者句子 混合增强：起源于图像的mixup（猫和狗的混合）。提出了wordMixup和sentMixup将词向量和句向量进行Mixup。 回译：中文翻译成英文表达，然后再由英文翻译回中文。 GAN对抗生成网络：GAN 主要分为两部分：生成模型和判别模型。生成模型的作用是模拟真实数据的分布，判别模型的作用是判断一个样本是真实的样本还是生成的样本。GAN 的目标是训练一个生成模型完美的拟合真实数据分布使得判别模型无法区分。四. 图像算法 参考文章：图像总常用的变换边缘检测图像增强技术4.1 图像特征提取的方法有哪些？ SIFT（尺度不变特征变换）—— 图像拼接： HOG（方向梯度直方图（Histogram of Oriented Gradient, HOG））—— 行人检测：特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。）本质：梯度的统计信息，而梯度主要存在于边缘的地方。 LBP(Local Binary Pattern局部二值模式)：种描述图像局部纹理的特征算子，具有旋转不变性与灰度不变性等显著优点。LBP特征将窗口中心点与邻域点的关系进行比较，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了复杂场景下（光照变换）特征描述问题（局部纹理特征提取）。4.2 为什么要图像的灰度化? 图像识别中要识别物体：找到edge ==&gt; 计算梯度 ==&gt; 需要用到灰度图 有利于图像特征提取：RGB采用的是三通道，而灰度图用的是单通道，能加快特征抽取。4.3 为什么预处理中要归一化和标准化 取值范围从0～255已经转化为0～1之间了，这个对于后续的神经网络或者卷积神经网络处理有很大的好处，加快梯度下降求解的速度 减小了几何变换和仿射变化的影响。4.4 为什么要中值滤波和均值滤波? 目的：消除图像中的噪声成分叫作图像的平滑化或滤波操作。图像的能量大部分集中在幅度谱的低频和中频段是很常见的，而在较高频段，感兴趣的信息经常被噪声淹没。 中值滤波：一连串数字｛1，4，6，8，9｝中，数字6就是这串数字的中值.椒盐噪声很好的被平滑了，而且也没均值那样模糊化太过于严重。 均值滤波：图片中一个方块区域（一般为3*3）内，中心点的像素为全部点像素值的平均值。一般均值滤波过于模糊化了。4.5 边缘检测算子 Roberts算子：基于x轴和y轴的\\(s_{x}=\\left[\\begin{array}{cc}1 &amp; 0 \\\\0 &amp; -1\\end{array}\\right]\\)\\(s_{y}=\\left[\\begin{array}{cc}0 &amp; -1 \\\\1 &amp; 0\\end{array}\\right]\\) Prewitt算子：\\(s_{x}=\\left[\\begin{array}{ccc}-1 &amp; 0 &amp; 1 \\\\-1 &amp; 0 &amp; 1 \\\\-1 &amp; 0 &amp; 1\\end{array}\\right]\\)\\(s_{y}=\\left[\\begin{array}{ccc}1 &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 0 \\\\-1 &amp; -1 &amp; -1\\end{array}\\right]\\) Sobel算子：\\(s_{x}=\\left[\\begin{array}{ccc}-1 &amp; 0 &amp; 1 \\\\-2 &amp; 0 &amp; 2 \\\\-1 &amp; 0 &amp; 1\\end{array}\\right]\\)\\(s_{y}=\\left[\\begin{array}{ccc}1 &amp; 2 &amp; 1 \\\\0 &amp; 0 &amp; 0 \\\\-1 &amp; -2 &amp; -1\\end{array}\\right]\\) 基本的边缘算子如Sobel求得的边缘图存在很多问题，如噪声污染没有被排除、边缘线太过于粗宽等 Canny算子：目标是找到一个最优的边缘。具有以下优势 低错误率：标识尽可能多的实际边缘，剑豪噪声产生的误报。 高定位性：标识出的边缘要与图像的实际边缘尽可能的接近。 最小响应：图像中的边缘只能标识一次。 canny检测的步骤： 使用高斯滤波器降噪。 利用Sobel算子进行卷积（x和y反向） 将像素点上x和y卷积之后的平方求根，并计算x，y方向上的角度， \\(G=\\sqrt{G_{x}^{2}+G_{y}^{2}}\\)，\\(\\theta=\\arctan \\left(\\frac{G_{y}}{G_{x}}\\right)\\) 非极大值抑制，进一步排除非边缘的像素，仅保留一些细线条。 滞后阈值：高于某阈值，保留为边缘像素，反之排除。 4.6 常用的插值方法在图像几何变换时，无法给有些像素点直接赋值，例如，将图像放大两倍，必然会多出一些无法被直接映射的像素点，对于这些像素点，通过插值决定它们的值。于是，产生了图像插值算法。 线性插值：最近邻插值，双线性插值以及双三次插值等，\\(f(x)=a_{1} x+a_{0}\\)4.7 深度学习和传统目标检测方法的优缺点传统的目标检测算法对光照，明暗，数据传输，物体遮挡等上模型的鲁棒性不强。4.8 图像增强技术增强技术也可以有多种分类，如，可以分为平滑（抑制高频成分）与锐化（增强高频成分），空间域与频域。 空间域增强就是指增强构成图像的像素，是直接对这些像素进行操作的过程。 频域则是修改图像的傅立叶变换。4.9 SSD和Yolo SSD：将物体检测这个问题的解空间，抽象为一组预先设定好（尺度，长宽比）的bounding box。在每个bounding box，预测分类label，以及box offset来更好的框出物体。对一张图片，结合多个大小不同的feature map的预测结果，以期能够处理大小不同的物体。 （优点）相比Fast RNN系列，删除了bounding box proposal这一步，及后续的重采样步骤，因而速度较快，达到59FPS。 （优点） YOLO：将物体检测这个问题定义为bounding box和分类置信度的回归问题。将整张图像作为输入，划分成SxS grid，每个cell预测B个bounding box（x, y, w, h）及对应的分类置信度（class-specific confidence score）。分类置信度是bounding box是物体的概率及其与真实值IOU相乘的结果。 （优点）速度快，45FPS （优点）YOLO使用图像的全局信息做预测，因而对背景的误识别率低。 （缺点） 每个cell只能拥有一个label和两个bounding box，这个空间局限性，使得对小物体检测效果不好 （缺点）对于物体长宽比的泛化能力较弱，当一类物体新的长宽比出现时，检测准确率减低。 二者之间的差别：YOLO在卷积层后接全连接层，即检测时只利用了最高层Feature maps（包括Faster RCNN也是如此）而SSD采用金字塔结构，即利用了conv4-3/fc7/conv6-2/conv7-2/conv8_2/conv9_2这些大小不同的feature maps，在多个feature maps上同时进行softmax分类和位置回归。SSD还加入了Prior box4.10 零样本学习（Zero-shot Learning）和单样本学习（One-shot Learning） 零样本学习：基于可见标注数据集&amp;可见标签集合（seen），学习并预测不可见（unseen，无标注）数据集结果。4.11 前景背景分割4.12 工业相机CCD和CMOS CCD（电荷耦合元件）：输出节点统一输出数据，信号一致性好；CCD采用逐个光敏输出，速度较慢 CMOS（金属氧化物半导体元件）：CMOS芯片中每个像素都有自己的信号放大器，各自进行电荷到电压的转换，输出信号的一致性较差，比CCD的信号噪声更多。CMOS每个电荷元件都有独立的装换控制器，读出速度很快，FPS在500以上的高速相机大部分使用的都是CMOS。4.13 小目标检测在深度学习目标检测中，特别是人脸检测中，小目标、小人脸的检测由于分辨率低，图片模糊，信息少，噪音多，所以一直是一个实际且常见的困难问题。FPN特征金字塔网络：参考文章：https://zhuanlan.zhihu.com/p/920059274.14 目标检测中的mAP具体参考文章：https://www.cnblogs.com/itmorn/p/14193729.html五. 深度学习5.1 梯度消失的原因和解决办法有哪些？ 梯度消失：每一层非线性层都可以视为是一个非线性函数 f(x)f(x)(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数。那么根据“链式求导”法则，比如rnn来说，其激活函数为tanh，那么tanh的导数的最大值是1，那么如果连乘0.8的100次方，无线接近于0，导致梯度消失。 梯度爆炸：tanh导数 * W权重，这里如果W的值太大了，随着序列长度的增加，连乘无限大，导致梯度爆炸。 解决方案：一个是激活函数比如relu系列，一个初始化权重 ，一个是梯度裁剪5.2 RNN 和LSTM的差别在哪？RNN的前向推导公式：LSTM的三种门控制如下：如上图所示，它们的名字、表示的计算过程及输出分别是： 遗忘门： \\(f_i=\\sigma\\left(W_f\\left[x_i, h_{i-1}\\right]+b_f\\right)\\) 输入们： \\(i_i=\\sigma\\left(W_i\\left[x_i, h_{i-1}\\right]+b_i\\right)\\) 输出们： \\(o_i=\\sigma\\left(W_o\\left[x_i, h_{i-1}\\right]+b_o\\right)\\) 可以看到，除了参数不同，它们计算公式是一样的。啰嗦一句，上图中 [公式] 表示sigmoid函数， [公式] 表示tanh函数： RNN来说，它能够处理一定的短期依赖，但无法处理长期依赖问题。原因：当序列较长时，序列后部的梯度很难反向传播到前面的序列，比如10个元素以前，这就产生了梯度消失问题 当然，RNN也存在梯度爆炸问题，但这个问题一般可以通过梯度裁剪（gradient clipping）来解决 RNN没有细胞状态；LSTM通过细胞状态记忆信息。 RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添加求和操作，减少梯度消失和梯度爆炸的可能性。 RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。5.3 注意力机制是为了解决什么问题？为什么选用了双向循环神经网络？ 人脑在工作时具有一定注意力，当欣赏艺术品时，既可以看到全貌，也可以关注 细节，眼睛聚焦在局部，忽略其他位置信息。说明人脑在处理信息的时候有一定权重划分。而注意力机制的提出正是模仿了人脑的这种核心特性。 实际使用中，随着输入序列长度的增加，模型性能显著下降。因为编码时输入序列的全部信息被压缩到一个向量表示中去。序列越长，句子越前面的词的信息丢失就越严重。以100词的句子为例，编码时将整个句子的信息压缩到一个向量中去，而在解码时(比如翻译)，目标语言第一个单词大概率与源语言第一个单词对应，这就意味着第一步的解码需要考虑到100步之前的信息。一个小技巧是可以将源语言句子逆向输入，或者重复输入两遍，得到一定的提升，也可以使用LSTM缓解这个问题。但对于过长序列仍难以有很好表现。5.4 Batch Normalization和Dropout差别 BN训练和测试时的参数是一样的嘛？BN是对每一批训练数据进行归一化，使用每一批数据的均值和方差；测试的时候，每一批数据中仅有一个样本，没有batch概念了，这里的均值和方差就是全量数据均值和方差。 BN训练时为什么不用全量训练集的均值和方差呢？对于BN，是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。BN操作把分布压缩在[-1,1],服从均值为0,方差为1的正太分布，相当于把大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。 Dropout的作用是什么？ 在训练的过程中以一定概率使得神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。 Dropout 在训练和测试时都需要嘛？dropout仅在训练的时候采用，为了减少神经元对部分上层神经元的依赖，类似于将多个不同的网络结构的模型集成起来，减少过拟合和增强其鲁棒性。测试的时候用到的是整个训练完成的模型，不需要dropout。 Dropout 如何平衡训练和测试时的差异呢？假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。 BN和Dropout共同使用时会出现的问题BN和Dropout单独使用都能减少过拟合并加速训练速度，但如果一起使用的话并不会产生1+1&gt;2的效果，相反可能会得到比单独使用更差的效果。5.5 Batch Normalization和Layer Normalization的差别 LN和BN都是一种归一化方式，差别是：BN是取的是不同样本的同一个特征进行归一化；LN取得是同一个样本的不同特征。 应用场景不同：LN适用于RNN或者batchsize较小；BN适用于CNN。 对于RNN来说，每个样本的长度都是不同的，那么当BN需要统计靠后的时间片段的时候，可能都没有这方面的信息，那么只基于某些长时间片段的样本的统计信息无法反应出全局分布，所以就不合适了。 5.6 bert的具体网络结构，以及训练过程，及其优势在哪 bert处理句子是整体处理的，不是逐字处理的，解决了不受长期依赖问题困扰的主要原因（不存在过去信息丢失的风险），同时提高了训练效率。 多头注意力和位置嵌入：提供了有关不同单词之间的关系信息。 总结：完全避免了递归操作，通过整体处理句子以及学习单词之间的关系来感谢多头注意机制和位置嵌入。5.7 albert和bert的差别在哪 albert的核心：训练出更小但效果更好的模型! 想让模型更轻，训练更快，效果更好！（期望的是用更少量的数据，得到更好的结果）。ALBERT提出了三种优化策略，做到了比BERT模型小很多的模型，但效果反而超越了BERT， XLNet。 Factorized Embedding Parameterization. 他们做的第一个改进是针对于Vocabulary Embedding。在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。为了解决这些问题他们提出了一个基于factorization的方法。他们没有直接把one-hot映射到hidden layer, 而是先把one-hot映射到低维空间之后，再映射到hidden layer。这其实类似于做了矩阵的分解。 Cross-layer parameter sharing. 每一层的layer可以共享参数，这样一来参数的个数不会以层数的增加而增加。所以最后得出来的模型相比BERT-large小18倍以上。 Inter-sentence coherence loss. 在BERT的训练中提出了next sentence prediction loss, 也就是给定两个sentence segments, 然后让BERT去预测它俩之间的先后顺序，但在ALBERT文章里提出这种是有问题的，其实也说明这种训练方式用处不是很大。 所以他们做出了改进，他们使用的是setence-order prediction loss (SOP)，其实是基于主题的关联去预测是否两个句子调换了顺序。 5.8 CNN和RNN的差别 训练速度上：CNN快很多。RNN慢的原因是每个timestep的计算，都要依赖前一个时刻的输出。而cnn的卷积的时候，和空间上其他的点没有任何联系，适合并行计算。 数据约束：CNN对于数据的约束就很强了，图像识别，input的纬度是48*48的，必须定死了，而RNN其实对于数据的长度（句子的长度）没有要求（TF里面有动态rnn来在输入rnn之前去掉pad为0的地方） 卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。共享参数是深度学习一个重要的思想，其在减少网络参数的同时仍然能保持很高的网络容量(capacity)。卷积层在空间方向共享参数，而循环神经网络(recurrent neural networks)在时间方向共享参数。5.9 优化器和超参调节SGD(随机梯度下降)​ 在随机梯度下降算法（SGD）中，优化器基于小批量估计梯度下降最快的方向，并朝该方向迈出一步。由于步长固定，因此 SGD 可能很快停滞在平稳区（plateaus）或者局部最小值上。\\(w_{t+1}=w_{t}-\\alpha \\cdot g_{t}\\)​ 基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲。5.9 深度学习平台 阿里NASA计划的机器学习平台PAI（17年） 全面兼容TF，Caffe，MXNet深度学习框架 提供云端的计算资源 集成很多机器学习算法（分类，回归，聚类） 支持大规模的分布式数据训练 百度paddlepaddle飞桨(18年) 支持大规模的分布式数据训练 多平台部署 产业级的开源模型库（语义理解，图像分类，目标检测，图像分割等多种场景） 微软Microsoft Custom Vision Services（17年） 针对的是图像分类器 提供迁移学习的模型 谷歌的Cloud AutoML 针对的是图像分类器 提供迁移学习的模型"
    } ,
  
    {
      "title"       : "Postman和Jmeter进行上传文件及压力测试",
      "category"    : "",
      "tags"        : "压力测试",
      "url"         : "./Postman%E5%92%8CJmeter%E8%BF%9B%E8%A1%8C%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%8F%8A%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95.html",
      "date"        : "2020-03-23 23:21:00 +0800",
      "description" : "讲述如何利用Postman和Jmeter对网络接口进行压力测试",
      "content"     : "一. 准备工作 postman下载链接：https://www.postman.com/downloads/ Jmeter下载链接：http://jmeter.apache.org/download_jmeter.cgi flask代码地址：https://github.com/yy2lyx/FlaskTutorial/tree/master/Flask-7-upload windows下scoop下载jdk(这里是由于Jmeter需要)：scoop install ojdkbuild二. 构建接口的flask服务其中包含前端表单index.html文件如下和flask的后端&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;upload&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action = \"/success\" method = \"post\" enctype=\"multipart/form-data\"&gt; &lt;input type=\"file\" name=\"file\"&gt; &lt;input type = \"submit\" value=\"Upload\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;三. Postman的http接口测试postman分别在header和body中填入下图： headers中需要填入value:multipart/form-data body 中需要填入key：file(这里参考index.html文件中name=”file”)，value:eml文件地址 然后将写好的保存在collections当中，并构建tests选项（如果不填入，后面的串行压力测试无法开始，报错） 通过collections中选中保存好的请求，run即可四. Jmeter的http接口测试在下载解压好的jmeter二进制文件中打开：apache-jmeter-5.3\\bin\\jmeter.bat 新建一个线程组，如下图，包括http请求及监听 线程中填入线程总数，和全部线程开启总的时间（这里由于需要测试并发1小时2万次访问） 在http请求页面填入请求的参数 http页面下面不用填Parameters和BodyData，在Files Upload中填入下图，其中file和上面一致，而MIME Type需要访问https://www.freeformatter.com/mime-types-list.html，找到其中.eml格式前面的 运行，即可看到并行的接口请求情况 五. 总结 一般的网络接口测试，功能性测试postman较为好用。 需要测试高并发的情况下，只能用Jmeter来进行测试，因为postman是串行，而Jmeter是多线程并行测试。"
    } ,
  
    {
      "title"       : "Neo4j数据库与图数据挖掘算法结合",
      "category"    : "",
      "tags"        : "机器学习, 图算法",
      "url"         : "./Neo4j%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E5%9B%BE%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E7%BB%93%E5%90%88.html",
      "date"        : "2020-02-27 23:21:00 +0800",
      "description" : "讲述Neo4j图数据库的使用，图挖掘算法（社区算法）的实现",
      "content"     : "一. 环境准备 neo4j python包：pip3 install neo4j 和pip3 install py2neo（这里的py2neo 是python对Neo4j的驱动库,同时这里必须是py2neo版本必须是最新版4，不然会报连接数据库的错误，老版本不兼容的问题） Java8：这里由于neo4j 数据库是依赖于java8的。 Neo4j_3.5.14：这里由于neo4j 在中国地区下载慢，并且neo4j3.X版本才支持java8，到4.0版本就是需要java11了。 Neo4j_Desktop：neo4j的桌面端（可以远程数据库和连接本地数据库，同时包含很多额外的扩展） 二. 连接本地图数据库 py2neo V4 官方文档：https://py2neo.org/v4/index.htmlNeo4j 一共有3种连接方式： Bolt：bolt://localhost:11005 HTTP：http://localhost:11006 HTTPS：https://localhost:11007这里可以通过Neo4j Desktop来查看新建的图数据库（同时设置密码）2.1 Neo4j数据库语法Cypher 创建 create (:Movie {title:\"ABC\",released:2016}) return p; 查询match (p: Person) return p; 查询Person类型的所有数据match (p: Person {name:\"sun\"}) return p; 查询名字等于sun的人match( p1: Person {name:\"sun\"} )-[rel:friend]-&gt;(p2) return p2.name , p2.age 查询sun的朋友的名字和年龄match (old) ... create (new) create (old)-[rel:dr]-&gt;(new) return new 对已经存在的节点和新建的节点建立关系 更新 MERGE (m:Movie { title:\"Cloud Atlas\" })ON CREATE SET m.released = 2012RETURN m 筛选过滤 match (p1: Person)-[r:friend]-&gt;(p2: Person) where p1.name=~\"K.+\" or p2.age=24 or \"neo\" in r.rels return p1,r,p2 聚合函数（支持count,sum,avg,min,max） MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations 排序和分页MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a,count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10;2.2 图数据库的基本操作py 这里是通过导入py2neo这个neo4j的第三方库来连接from py2neo import Graph,Nodegraph = Graph( \"http://localhost:11006\", username=\"neo4j\", password=\"yy\") 清空数据库 graph.delete_all() 定义节点关系a = Node('Person', name='Alice')b = Node('Person', name='Bob')r = Relationship(a, 'KNOWNS', b)s = a | b | rgraph.create(s) Node查询# 用CQL进行查询，返回的结果是listdata1 = graph.data('MATCH(p:PersonTest) return p')print(\"data1 = \", data1, type(data1))# 用find_one()方法进行node查找，返回的是查找node的第一个nodedata2 = graph.find_one(label='PersonTest', property_key='name', property_value=\"李四\")print (\"data2 = \", data2, type(data2))# 用find()方法进行node查找,需要遍历输出，类似于mongodbdata3 = graph.find(label='PersonTest')for data in data3: print (\"data3 = \", data) 关系查询relationship = graph.match_one(rel_type='KNOWNS')print (relationship, type(relationship)) 更新pushnode1 = graph.find_one(label='PersonTest', property_key='name', property_value=\"张三\")node1['age'] = 21graph.push(node1)data4 = graph.find(label='PersonTest')for data in data4: print (\"data4 = \", data) #基于上面的操作，再次定义node1[‘age’] = 99,并执行graph.push(node1)，发现已经更新node1['age'] = 99graph.push(node1)data5 = graph.find(label='PersonTest')for data in data5: print (\"data5 = \", data) 删除Node和Relationshipnode = graph.find_one(label='PersonTest', property_key='name', property_value=\"李四\")relationship = graph.match_one(rel_type='KNOWNS')graph.delete(relationship)graph.delete(node)data6 = graph.find(label='PersonTest')for data in data6: print (\"data6 = \", data) 多条件查询a = Node('PersonTest', name='张三', age=21, location='广州')b = Node('PersonTest', name='李四', age=22, location='上海')c = Node('PersonTest', name='王五', age=21, location='北京')r1 = Relationship(a, 'KNOWS', b)r2 = Relationship(b, 'KNOWS', c)s = a | b | c | r1 | r2graph.create(s)data7 = graph.find(label='PersonTest')for data in data7: print (\"data7 = \", data) 单条件查询# 单条件查询，返回的是多个结果selector = NodeSelector(graph)persons = selector.select('PersonTest', age=21)print(\"data8 = \", list(persons)) 多条件查询selector = NodeSelector(graph)persons = selector.select('PersonTest', age=21, location='广州')print(\"data9 = \", list(persons)) 复杂查询orderby# orderby进行更复杂的查询selector = NodeSelector(graph)persons = selector.select('PersonTest').order_by('_.age')for data in persons: print (\"data10 = \", data)三. 中心性算法实验（社区算法）3.1 中心性算法 度中心性：度中心性是最简单度量，即为某个节点在网络中的联结数。 MATCH (c:Character)-[:INTERACTS]-()RETURN c.name AS character, count(*) AS degree ORDER BY degree DESC 加权度中心性：指的是每个节点的权重后的中心性 MATCH (c:Character)-[r:INTERACTS]-()RETURN c.name AS character, sum(r.weight) AS weightedDegree ORDER BY weightedDegree DESC 介数中心性:在网络中，一个节点的介数中心性是指其它两个节点的所有最短路径都经过这个节点，则这些所有最短路径数即为此节点的介数中心性。MATCH (c:Character)WITH collect(c) AS charactersCALL apoc.algo.betweenness(['INTERACTS'], characters, 'BOTH') YIELD node, scoreSET node.betweenness = scoreRETURN node.name AS name, score ORDER BY score DESC 紧密度中心性：指到网络中所有其他角色的平均距离的倒数。 MATCH (c:Character)WITH collect(c) AS charactersCALL apoc.algo.closeness(['INTERACTS'], characters, 'BOTH') YIELD node, scoreRETURN node.name AS name, score ORDER BY score DESC 3.2 PageRank 算法PageRank算法源自Google的网页排名。它是一种特征向量中心性(eigenvector centrality)算法。UNWIND {nodes} AS nMATCH (c:Character) WHERE c.name = n.nameSET c.pagerank = n.pg可以在Neo4j的图中查询最高PageRank值的节点：MATCH (n:Character)RETURN n.name AS name, n.pagerank AS pagerank ORDER BY pagerank DESC LIMIT 10"
    } ,
  
    {
      "title"       : "无监督异常检测模型",
      "category"    : "",
      "tags"        : "机器学习",
      "url"         : "./%E6%97%A0%E7%9B%91%E7%9D%A3%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B.html",
      "date"        : "2019-12-05 03:20:00 +0800",
      "description" : "主要讲述无监督的异常检测模型",
      "content"     : "异常检测模型一般分为五大类：统计和概率模型、线性模型、非线性模型、基于相似度衡量的模型、基于聚类的异常检测模型。一. 统计和概率模型主要是假设和检验。假设数据的分布，检验异常。比如对一维的数据假设高斯分布，然后将3sigma以外的数据划分为异常，上升到高维，假设特征之间是独立的，可以计算每个特征维度的异常值并相加，如果特征之间是相关的，也就变成了多元高斯分布，可以用马氏距离衡量数据的异常度。这类方法要求对问题和数据分布有较强的先验知识。1.1 3σ原则如果特征服从正态分布，那么，在\\(\\pm 3 \\sigma\\)的范围内包含了99.73%的“几乎所有”的内容（所有正常的值都在平均值正负三个标准差的范围内），而可能存在的异常值都在其之外。1.2 3σ原则的适用条件 数据分布满足正态分布（在业务逻辑上母样本满足）。 特征之间相互独立。二. 线性模型2.1 PCAPCA是最常见的线性降维方法，它们按照某种准则为数据集 \\(\\left\\{x_{i}\\right\\}_{i=1}^{n}\\)找到一个最优投影方向 W 和截距和截距 b ，然后做变换 \\(z_{i}=W x_{i}+b\\)得到降维后的数据集 \\(\\left\\{z_{i}\\right\\}_{i=1}^{n}\\)。因为\\(z_{i}=W x_{i}+b\\)是一个线性变换（严格来说叫仿射变换，因为有截距项），因此PCA属于线性模型（处理线性问题）。假设数据在低维空间上有嵌入，那么无法、或者在低维空间投射后表现不好的数据可以认为是异常点。PCA有两种检测异常的方法，一种是将数据映射到低维空间，然后在特征空间不同维度上查看每个数据点和其他数据点的偏差，另一种是看重构误差，先映射到低维再映射回高维，异常点的重构误差较大。这两种方法的本质一样，都是关注较小特征值对应的特征向量方向上的信息。可以利用PCA将二维的特征数据映射到一维上（尽量保持原始信息），然后再映射到二维空间，对比重构的二维数据和初始的二维数据的误差值（MSE），设定阈值，大于阈值的为异常，小于等于阈值的为正常。 模型构建过程： 原始数据： \\(x_{i} \\in R^{d}\\) 编码后的数据： \\(z_{i}=W^{T}\\left(x_{i}+b\\right) \\in R^{c}\\) 解码后的数据： \\(\\hat{x}_{i}=W z_{i}-b \\in R^{d}\\) 重构的误差： \\(\\sum_{i=1}^{n}\\left\\|x_{i}-\\hat{x}_{i}\\right\\|_{p}^{p}\\) 2.2 OneClass-SVM （1）与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。 （2）与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。与传统SVM不同的是，one class SVM是一种非监督的算法。它是指在训练集中只有一类positive（或者negative）的数据，而没有另外的一类。而这时，需要学习（learn）的就是边界（boundary），而不是最大间隔（maximum margin）。 三. 非线性模型3.1 AutoEncoder非线性降维的代表方法是AutoEncoders，AutoEncoders的非线性和神经网络的非线性是一回事，都是利用堆叠非线性激活函数来近似任意函数。事实上，AutoEncoders就是一种神经网络，只不过它的输入和输出相同，真正有意义的地方不在于网络的输出，而是在于网络的权重。 模型构建过程： 原始数据： \\(x_{i} \\in R^{d}\\) 编码后的数据：\\(z_{i}=\\sigma\\left(W^{T} x_{i}+b\\right) \\in R^{c}\\) 解码后的数据： \\(\\hat{x}_{i}=\\hat{\\sigma}\\left(\\hat{W} z_{i}+\\hat{b}\\right) \\in R^{d}\\) 重构的误差： \\(\\sum_{i=1}^{n}\\left\\|x_{i}-\\hat{x}_{i}\\right\\|_{p}^{p}\\) 这里sigma是非线性激活函数。AutoEncoder一般都会堆叠多层（多层神经层同样也能增加模型的非线性），方便起见我们只写了一层。Autoencoder可以参与构建2种不同的异常检测模型。 （1）非线性降维：利用训练好的模型参数，通过输入到Encoder中，直接输出中间的CODE，而这个CODE就是数据经过非线性降维后的数据，然后利用降维后的数据放入到常用的聚类算法中（比如KMeans），搭建无监督异常检测模型。 （2）本身作为异常值的判别器：在训练好的模型之后，数据通过Encoder映射到低维空间后，利用Decoder重构回高维空间，而当输入数据是异常数据的时候，重构的高维数据会和原始数据的loss（MSE）很高，在这里设定一个阈值，大于阈值的为异常，小于等于阈值的为正常。3.2 VAE（Variational AutoEncoder）Variational AutoEncoder（VAE）是由 Kingma 和 Welling 在“Auto-Encoding Variational Bayes, 2014”中提出的一种生成模型。VAE其实可以看作AutoEncoder的一个变种，它在AutoEncoder区别在于在Encoder映射到低维空间中映射成了2个Vector，一个属于原始的CODE，而另一个作为噪声增加到新的CODE中。上图中，M_1，M_2，M_3作为Original Code，sigma_1，sigma_2，sigma_3这种变分的噪点则是通过模型自动训练学习得到的，e_1,e_2,e_3定义为服从一个标准的正太分布的向量，那么在AutoEncoder中的低维空间code在VAE中就是上图中的c_i了。code的定义公式如下：\\(c_{i}=\\exp \\left(\\sigma_{i}\\right) \\times e_{i}+m_{i}\\)当然，对于VAE来说，定义loss的function也和AutoEncoder不一样：\\(loss=\\sum_{i=1}^{3}\\left(\\exp \\left(\\sigma_{i}\\right)-\\left(1+\\sigma_{i}\\right)+\\left(m_{i}\\right)^{2}\\right)\\)VAE相较于AutoEncoder的优势在于：映射到低维空间的code增加了噪声，导致重构的高维空间中的值可以和原始数据不太一样，比如AutoEncoder模型原始输入是满月和斜月，那么输出一定是满月和斜月，而VAE则是可以生成满月和斜月的结合体。四. 基于划分超平面的模型——Isolation Forest4.1 孤立森林的思想是假设我们用一个随机超平面来切割（split）数据空间（data space）, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。4.2 孤立森林的优势孤立森林算法具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。4.3 孤立森林的劣势孤立森林不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。五. 基于聚类的异常检测模型5.1 常用的聚类算法 （1）KMeans：利用找到的簇的中心点和每一个样本的距离值，找到最偏离簇中心的点作为异常点。 （2）DBSCAN–基于密度的聚类：由于需要涉及到算法本身两个参数（min_samples和eps），这里模型会直接输出超过半径eps和确定好最小数的min_samples的样本点作为异常值（label = -1）。 （3）Birch–基于层次的聚类：BIRCH算法利用了一个树结构来帮助我们快速的聚类，这个数结构类似于平衡B+树，一般将它称之为聚类特征树(Clustering Feature Tree，简称CF Tree)。建立好CF Tree后把那些包含数据点少的MinCluster当作outlier。5.2 聚类算法的适应性每一种算法对于不同的数据分布可能存在不同优势。K-Means算法对于凸性数据具有良好的效果，能够根据距离来讲数据分为球状类的簇，但对于非凸形状的数据点，就无能为力了，比如环形数据等等，此时基于密度的算法DBSCAN就更令人满意了。5.2 评价聚类效果指标聚类算法的目标是：簇内相似度高，簇间相似度低。因此通过轮廓系数（Silhouette Coefficient）来评价聚类效果的好坏，适用于上述三种算法。1、将样本x与簇内的其他点之间的平均距离作为簇内的内聚度a2、将样本x与最近簇中所有点之间的平均距离看作是与最近簇的分离度b3、将簇的分离度与簇内聚度之差除以二者中比较大的数得到轮廓系数，计算公式如下\\(s^{(i)}=\\frac{b^{(i)}-a^{(i)}}{\\max \\left\\{b^{(i)}, a^{(i)}\\right\\}}\\)轮廓系数的取值在-1到1之间。当簇内聚度与分度离相等时，轮廓系数为0。当b»a时，轮廓系数近似取到1，此时模型的性能最佳。"
    } ,
  
    {
      "title"       : "Windows下安装C++ IDE（clion）和opencv环境",
      "category"    : "",
      "tags"        : "机器视觉",
      "url"         : "./Windows%E4%B8%8B%E5%AE%89%E8%A3%85C++-IDE-clion-%E5%92%8Copencv%E7%8E%AF%E5%A2%83.html",
      "date"        : "2019-10-01 18:18:00 +0800",
      "description" : "讲述如何在Windows环境中安装clion和配置opencv环境",
      "content"     : "1. 下载软件 clion：C++的IDE cmake : 这里需要添加到环境变量中 D:\\Profile\\mingw64\\bin MinGW ：添加到环境变量 D:\\Profile\\mingw64\\bin opencv3.4.10：开源的计算机视觉库2. MinGW和OpenCV主要是如何用你的编译器来编译OpenCV。我们需要有include文件夹，这个在写代码时就用的到，还有lib和dll，这俩货我也不是很懂，dll的话没有是可以编译成功的，但运行是要失败的，所以我们是肯定要把dll加入到系统环境变量Path里的。lib是编译时就需要的，所以我们得把lib放在CLion的CMakeLists里面。下载完Windows的OpenCV，其实我们只有给Visual Studio用的dll和lib，可是我们想要g++来编译和运行，所以就得自己根据OpenCV的sources文件夹来自己编译OpenCV。 这里需要在cmake中加入OPENCV_ALLOCATOR_STATS_COUNTER_TYPE=int64_t，add Entry ==&gt; string，这里参考报错信息1 这里还需要再cmake中加入OPENCV_ENABLE_ALLOCATOR_STATS=OFF，参考报错信息2 需要2次Configure和1次Genrate即可编译完成。 cd opencv\\mingw-build目录下输入mingw32-make 等待完成，mingw32-make install 打开你的mingw-build文件夹，里面有个install目录就是你要的，可以复制一下这个文件夹，以后就不用重新编译了。我在C盘建立了OpenCV目录，并且把install文件夹下的文件复制进去了,C:\\OpenCV\\x64\\mingw\\bin加入系统环境变量Path中。3. 写CMakeList其实就是加入lib目录和include目录cmake_minimum_required(VERSION 3.16)project(opencv_test)set(CMAKE_CXX_STANDARD 14)add_executable(opencv_test main.cpp)## 添加的OpenCVConfig.cmake的路径set(OpenCV_DIR \"D:/Profile/opencv_builded\")## 搜索OpenCV目录find_package(OpenCV REQUIRED)## 添加OpenCV头文件目录include_directories(\"D:/Profile/opencv_builded/include\")## 链接OpenCV库文件target_link_libraries(opencv_test ${OpenCV_LIBS})4. 编译成可执行文件main.cpp文件中写完后，cd 项目目录，cmake .，即可看到项目中新加了文件夹cmake-build-debug中里面存在.exe可执行文件。"
    } ,
  
    {
      "title"       : "Linux下python安装和包管理",
      "category"    : "",
      "tags"        : "coding",
      "url"         : "./Linux%E4%B8%8Bpython%E5%AE%89%E8%A3%85%E5%92%8C%E5%8C%85%E7%AE%A1%E7%90%86.html",
      "date"        : "2019-06-20 04:20:00 +0800",
      "description" : "讲述在Linux环境下python包编译及安装过程，以及包管理工具virtualenv",
      "content"     : "1. 上传python文件并打包编译 下载python版本：https://www.python.org/ftp/python/ 解压：tar -xf Python-3..1.tgz 编译：sudo ./configure --prefix=/path/you/want/to/install/ --with-ssl &amp;&amp; make &amp;&amp; make install(这里需要加–prefix是因为可以直接在指定文件夹下删除软件即可，加入with ssl是由于pip需要ssl),在编译结束后，正常程序会装在 /usr/local/bin 下（注意这里如果不加–with-ssl默认安装的软件涉及到ssl的功能不可用） 创建软连接：ln -sf /usr/local/bin/python3.8 /usr/bin/python和ln -sf /usr/local/bin/python3.8-config /usr/bin/python-config2. venv管理和包安装 安装virtualenvs：pip3 install virtualenv 创建环境：sudo virtualenv --python=python3.6 环境名字 安装第三方包：进入环境下的bin目录，sudo ./pip3 install -r requirements.txt -i 指定的pip安装源 这里指定安装源较快。 3. 创建软连接ln -sf /usr/local/bin/python3.8 /usr/bin/pythonln -sf /usr/local/bin/python3.8-config /usr/bin/python-config4. 设置pip镜像源，下载提速之前利用pip进行安装的时候，要不是直接在pip下载的中途断掉，要不就是网速特别慢。这里推荐设置下国内的源进行pip下载。 临时使用的方式：pip install tensorflow -i 国内源国内源： 清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/这里最好不要一味的相信某一个源（比如清华源），吐槽下：下其他的包速度都很快，某些包的时候不仅慢，它还中途断掉！所以推荐最好每个都试试！ 永久配置某个源：这里就不需要再加-i 国内源linux：修改 ~/.pip/pip.confwindows：直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.inilinux和windows的具体内容都一致，如下：[global]index-url = 国内源[install]trusted-host=mirrors.aliyun.com"
    } ,
  
    {
      "title"       : "深度学习调参经验总结",
      "category"    : "",
      "tags"        : "深度学习",
      "url"         : "./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93.html",
      "date"        : "2019-03-09 18:20:00 +0800",
      "description" : "讲述在深度学习建模过程中遇到的问题及其解决思路",
      "content"     : "一. 网络中loss表现过于震荡1.1 模型拟合能力不够导致模型震荡model（层数：input(30,300,3) ==&gt; ful_collected_layer(30,300,64) ==&gt; lstm ==&gt; ful_collected_layer ==&gt; output）此时模型的loss由1到192震动太大，acc也是在一个epoch中时好时坏，由此考虑到是模型的分类能力的问题（可能处理不了非线性或者是异或的问题）\"\"\"增加了一层全连接层，之后效果显著，模型虽然也存在loss和acc会有极小幅度的震荡，但是趋向于收敛\"\"\"input(30,300,3) ==&gt; ful_collected_layer(30,300,64) ==&gt; lstm ==&gt; ful_collected_layer ==&gt; ful_collected_layer ==&gt; output1.2 batch size 设置过小导致模型震荡之前模型用的是batch_size = 30，经过增大batch_size之后，模型的震荡程度也减小。这里如果GPU显存小的情况下，只能将batch设置小。1.3 输入模型的数据没有shuffle导致模型震荡之前数据没有进行shuffle，导致在某一个batch_size中学习到的全是正样本，某一个batch_size里面又全是负样本，shuffle之后，振荡减小。np.random.seed(110) # 设定种子数，不然下面shuffle之后的y无法与X对应上np.random.shuffle(X)np.random.seed(110)np.random.shuffle(y)二. 网络经过多轮迭代依然无法上升了，acc始终在79%2.1 增大学习率开始学习率设置的是learning_rate = 0.001,之后增大10倍，设置为learning_rate = 0.01之后，acc在70多轮的时候就能提升到90% ，300轮之后能到97%。理由：当我们把学习率设置较小的时候，那么梯度下降的时候迈的步子就小，可能在遇到大的坑的时候就出去，然后就一致在坑里徘徊，最终只能达到局部最优，无法达到全局最优，调参的过程中应该首先实验大的学习率，然后再依次减小实验。learning_rate = 0.1 ==&gt; learning_rate = 0.01 ==&gt; learning_rate = 0.0012.2 优化器的选择实验下其他的梯度下降的优化器（optimizer），比如Adam，SGD，Adadelta，RMSProp，Momentum等，一般来说Adam较快，SGD最慢，但是却是最准确和稳定的，因此可以先用Adam进行实验，最后用SGD进行调参。tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=’Adadelta’)tf.train.MomentumOptimizer(learning_rate, momentum, use_locking=False, name=’Momentum’, use_nesterov=False)tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name=’Adam’)tf.train.GradientDescentOptimizer(learning_rate, use_locking=False,name=’GradientDescent’)三. 遇到loss和weights在训练中全是nan值的情况3.1 查看输入数据中是否存在nan值检查自己做完预处理的数据，看下是否存在nan值（比如需要计算0/0和log0的情况）\"\"\"检验下input_data中是否存在nan值\"\"\"input_data = np.array(input_data).reshape([-1,n_input])# 这里的input_data 是三维数组必须转成2dinput_data_pd = pd.DataFrame(input_data)if np.any(input_data_pd.isnull()) == True:print(\"input data has nan value!\")list_nan = list(map(tuple, np.argwhere(np.isnan(input_data_pd.values))))print(list_nan)3.2 梯度爆炸或者是梯度消失可能是梯度爆炸，有以下解决方式 （1）预训练+微调 （2）梯度剪切 + 权重正则 （3）使用不同的激活函数，比如之前用relu，可以换成tanh或者是elu （4）使用batchnorm （5）使用LSTM网络（如果之前用的是RNN结构） （6）使用残差结构以下是几种在不改变模型层数和结构的情况下解决梯度爆炸和梯度消失的方案。\"\"\"权重L2正则化\"\"\"cross_entropy = -tf.reduce_sum(ys * tf.log(tf.clip_by_value(tf.nn.softmax(prediction), 1e-10, 1.0)))weights_lossL2 = tf.add(tf.nn.l2_loss(weights_in),tf.nn.l2_loss(weights_out)) * 0.01regularzation_loss = cross_entropy + weights_lossL2cost = tf.reduce_mean(regularzation_loss)\"\"\"梯度剪裁\"\"\"opt = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.5)# Compute the gradients for a list of variables.grads_and_vars = opt.compute_gradients(cross_entropy, tf.trainable_variables())# grads_and_vars is a list of tuples (gradient, variable). Do whatever you# need to the 'gradient' part, for example cap them, etc.capped_grads_and_vars = [(tf.clip_by_value(gv[0], 0.1, 5.), gv[1]) for gv in grads_and_vars]# Ask the optimizer to apply the capped gradients.optimizer = opt.apply_gradients(capped_grads_and_vars)3.3 使用了梯度参见和L2正则之后出现Loss增大的情况在使用了梯度裁剪之后，其实只是人为的控制梯度的变化（将weights控制在小范围内(0.1,5)之间），此时权重依旧可以通过BP算法向负梯度的方向前进，但是由于人为的控制，导致weight的梯度极有可能朝着正梯度方向进行，这就会导致可以更新权重，但是loss反而增大的原因。3.4 这里必须要修改模型结构举一个例子：利用4层全连接层作为一个分类器，来训练。经历过以上所有的方式（包括调整激活函数等），依旧无法使得模型的loss减少，当我将层数降低为3层的时候，模型loss开始收敛，那么这就说明当无法使得模型收敛的时候，其实极有可能是模型的结构问题，需要重新设计模型的结构层数。四.训练中模型loss不收敛的几种情况总结： train loss 不断下降，val loss不断下降 ==&gt; 说明网络仍在学习 train loss 不断下降，val loss趋于不变 ==&gt; 说明网络过拟合 train loss 趋于不变，val loss不断下降 ==&gt; 说明数据集100%有问题 train loss 趋于不变，val loss趋于不变 ==&gt; 说明学习遇到瓶颈，需要减小学习率或批量数目 train loss 不断上升，val loss不断上升 ==&gt; 说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题 train loss 到稳定的时候反而比val loss还要高 ==&gt; 测试集数据量太小了，误差计算算法有问题 train loss 和 val loss趋于不变，但是val loss趋于0，而train loss却还很高 ==&gt; 说明使用dropout层后模型拟合能力变差，去掉dropout层。 train loss 和 val loss同时极缓的形式增大，这里可以考虑降低学习率或者是从这里进行截断，以loss最低点作为模型最优点。"
    } ,
  
    {
      "title"       : "机器学习常用函数",
      "category"    : "",
      "tags"        : "机器学习",
      "url"         : "./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0.html",
      "date"        : "2018-11-21 18:18:00 +0800",
      "description" : "介绍python的数据分析常用工具pandas、numpy和机器学习工具sklearn的总结",
      "content"     : "介绍python的数据分析常用工具pandas、numpy和机器学习工具sklearn的总结:一.数据处理工具pandas和numpy1.1 pandas读取数据data = pd.read_csv(\"data.csv\",index_col=False,encoding = \"utf-8\")data = pd.read_tabel(\"data.txt\",sep = \",\")1.2 numpy.random 生成数据的用法numpy.random.rand(4,2,3) ==&gt;生成4*2*3的矩阵，其中元素在[0,1)，floatnp.random.randn(4,2,3) ==&gt;生成4*2*3的矩阵，元素是标准正态分布（以0为均值、以1为标准差的正态分布，记为N（0，1）），floatnp.random.randint(-5,5,size=(2,2)) ==&gt;生成一个2*2的矩阵，元素值在[-5,5)随机整数，intnp.random.random_sample(size=(2,2)) ==&gt;生成一个2*2的矩阵，元素是[0,1),floatnp.random.seed(1676) ==&gt;设置种子数，每次生成的随机数相同1.3 pandas_profiling用于简单快速查看数据分布和得到数据报告。import pandas_profilingpfr = pandas_profiling.ProfileReport(data)pfr.to_file(\"./example.html\")1.4 pandas去掉重复项 df.drop_duplicates()df[df.duplicated()].shapedf = df.drop_duplicates()df.shape1.5 pandas中loc和iloc区别 loc利用index的名称（这里可以是index和行号不一致），来获取想要的行（或列）。（名称导向的这个特点，使得df[df.loc[‘col_name’] == ‘condition’, ‘col_name’] = value_1成立。具体的实际应用，可参考 代码案例 一步实现EXCEL的合并数据、数据筛选、数据透视功能。 iloc利用index的具体位置（所以它只能是整数型参数，行号），来获取想要的行（或列）。 # 这里loc就可以直接用自己写的索引来构成df.loc['C':'6', '3':, -1] # 利用iloc抽取指定位置（所在的行整数值）的索引所构成的新的dataframenew_dataframe = df.iloc[index_list,:]1.6 找到Nan值——np.isnan()nan_np_list = np.argwhere(np.isnan(np_data))1.7 哑变量生成——pd.get_dummies()dummy_device_type = pd.get_dummies(data_org['platform'],prefix='device_type')1.8 计算特征之间的相关系数自变量之间相关系数较大的话，需要考虑共线性的问题，共线性会导致模型出现开式解，降低模型的稳定性。常见方法有皮尔森相关系数和斯皮尔曼相关系数。两个系数都是介于-1和1之间，-1表示完全负相关，1表示完全正相关，0表示完全不相关。使用皮尔森相关系数有局限：要求数据是成对的从正态分布中取得的。而斯皮尔曼相关系数是一种秩相关系数，不受数据分布的影响，它通过对变量排序，以排序后两个变量的秩次差来计算相关系数。pearson = data.corr() # 适用于都是连续性变量spearman = data.corr('spearman') # 适用于离散和连续变量1.9 dataframe的拼接df_all_row = concat([df1,df2]) #等价于 df1.append(df2)，纵着拼接#等价于 merge(df1,df2,left_index=True,right_index=True,how='outer')df_all_col = concat([df1,df2],axis=1) # 横着拼接1.10 groupby的使用df = pd.DataFrame({'A': ['a', 'b', 'a', 'c', 'a', 'c', 'b', 'c'], 'B': [2, 8, 1, 4, 3, 2, 5, 9], 'C': [102, 98, 107, 104, 115, 87, 92, 123]})df.groupby('A').mean()==&gt; A B C a 2.0 108.000000b 6.5 95.000000c 5.0 104.6666671.11 Series.apply该函数用于对该series的所有元素进行处理生成一个新的series。new_series = pd.Series([i for i in range(10)]).apply(lambda x:x**2)\"\"\"这里是函数中带2个参数的\"\"\"def subtract_custom_value(x, custom_value): return x - custom_valuenew_series_2 = df.apply(subtract_custom_value, args=(5,))1.12 Series可以直接用于2个列相加减a = pd.Series([i for i in range(10)])b = pd.Series([i+1 for i in range(10)])diff_series = a - b1.13 sort_values将dataframe根据某一列顺序重新生成# 比如这里需要将整个dataframe根据时间戳的顺序（True）进行重新调整data = data.sort_values(by = ['timestamp']，ascending=True)1.14 concat 将Dataframe或者是Series进行合并data = pd.concat([x1,x2,x3],axis=1) # 这里是三个Series根据每个的index进行合并（按列）1.15 Series.reset_index 直接将Series转成Dataframedf = series.reset_index(name = 'index_name') # 这里给series的index设置名字为'index_name'，并变成一列1.16 df.groupby()的apply方式使用# 初始的dataframedf = pd.DataFrame({'A': 'a a b'.split(), 'B': [1,2,3], 'C': [4,6, 5]})g = df.groupby('A')print(g.apply(lambda x: x / x.sum())) B C0 0.333333 0.41 0.666667 0.62 1.000000 1.0print(g.apply(list)) B C0 [1,2] [3]1 [4,6] [5]1.17 df.groupby() 直接分成2个组的dataframedf = pd.DataFrame({'A': 'a a b'.split(), 'B': [1,2,3], 'C': [4,6, 5]})g = df.groupyby('A')for name,group in g: name_i = name df_i = group # 这里就是A只有a的dataframe二.绘图查看数据分布——seaborn和matplotlib2.1 频数分布直方图def plot_bar(x,y,color,title,width = 0.5): plt.figure() idx = np.arange(len(x)) plt.bar(idx,y,width,color = color) for xx, yy in zip(x, y): plt.text(xx, yy + 0.1, str('%.2f%%' % ((yy/np.array(y).sum())*100)), ha='center') plt.xticks(idx,x) plt.title(title) plt.xlabel('Hour') plt.ylabel('Trade-Frequence') plt.show()mu, sigma = 100, 15x = mu + sigma * np.random.randn(10000)# the histogram of the datan, bins, patches = plt.hist(x, 50, density=1, facecolor='g', alpha=0.75)plt.xlabel('Smarts')plt.ylabel('Probability')plt.title('Histogram of IQ')plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')plt.axis([40, 160, 0, 0.03])plt.grid(True)plt.show()2.2 散点图plt.plot(x,y)2.3 直线图plt.scatter(x,y)2.4 双变量分布图def two_dims_draw_relationship(df,xlabel,ylabel): # 双变量分布 x,ylabel是字符串，df必须是双维度的dataframe sns.set(color_codes=True) g = sns.jointplot(x=xlabel, y=ylabel, data=df, kind=\"kde\", color=\"y\") g.plot_joint(plt.scatter, c=\"m\", s=30, linewidth=1, marker=\"+\") g.ax_joint.collections[0].set_alpha(0) # 画背景网格线 g.set_axis_labels(\"${}$\".format(xlabel), \"${}$\".format(ylabel)) plt.show()2.5 多变量两两之间的分布图def all_two_feature_distribution(df = sns.load_dataset('iris')): sns.set(style=\"ticks\") sns.pairplot(df, hue=\"species\") plt.show()2.6 热度图scores_h = pd.DataFrame(np.array(scores_h).reshape(18, 3))scores_h.index = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]scores_h.columns = [1,2,3]plt.figure()ax = sns.heatmap(scores_h, annot=True,xticklabels=cols_all_features, yticklabels=cols_all_features,fmt='.2f')plt.ylabel(\"N_cluster\")plt.xlabel(\"linkages\")# 必须先savefig，之后再show，就不会出现保存时白色图片了plt.savefig(\"h.jpg\")plt.show()三.机器学习包sklearn3.1 划分数据集from sklearn.model_selection import train_test_split(trainX,testX,trainY,testY) = train_test_split(X,y,test_size=0.2,random_state=42)3.2 特征标准化 标准化：使用scale模块直接计算标准化，将标准化的array放在x_scale中，同时可以查看均值和标准差，但是该方式的一个不足是当存在新的样本到来时，无法利用已有的模块直接复用，需要利用mean和std自己计算。 x_scale = preprocessing.scale(DatMat) x_scale.mean(axis=0)x_scale.std(axis=0) 归一化：使用StandardScaler模块计算标准化，可以利用训练集数据建立一个转化的类，类似于实现将mean和std存储在该类中，将数据输入，就可以直接求出结果。 scaler = preprocessing.StandardScaler().fit(datingDatMat)datingDatMat = scaler.transform(datingDatMat)new_date = numpy.array([1, 2, 3])new_date_std = scaler.transform(new_date.reshape(1, -1)) 这里的scaler更象是扮演一个计算器的角色，本身并不存储数据。 3.3 交叉验证——查验模型稳定性from sklearn.model_selection import cross_val_score model_stability = cross_val_score(model, trainX,trainY,cv=10,scoring=\"accuracy\")mean_score_model = model_stability.mean()3.4 模型评估方式——混淆矩阵from sklearn.metrics import confusion_matrixprint(confusion_matrix(testY,y_pred))3.5 分类结果展示（准确性，召回率，f1-score）from sklearn.metrics import classification_reportprint(confusion_matrix(testY,y_pred))3.6 模型的保存和加载import pickle# 保存模型pickle.dump(rf_clf, open('model/model.model', 'wb'))# 导入模型model = pickle.load(open('model_save/model.model','wb'))3.7 数据样本不均衡——SMOTEENNfrom imblearn.combine import SMOTEENNsmote_enn = SMOTEENN(random_state=42)X_resampled,y_resampled = smote_enn.fit_sample(X,y)3.8 自动超参调节 自动调参——GridSearchCVfrom sklearn.model_selection import GridSearchCVrcl=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features='sqrt',random_state=10)# 现在不确定RandomForest其中一个参数n_estimators的个数param_test1 = {'n_estimators':range(10,71,10)}gsearch1= GridSearchCV(estimator =rcl,param_grid= param_test1,scoring='roc_auc',cv=5)gsearch1.fit(X,y)print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_) 自动调参神器——Hyperoptfrom hyperopt import fmin, tpe, hp, randimport numpy as npfrom sklearn.metrics import accuracy_scorefrom sklearn import svmfrom sklearn import datasets# SVM的三个超参数：C为惩罚因子，kernel为核函数类型，gamma为核函数的额外参数（对于不同类型的核函数有不同的含义）# 有别于传统的网格搜索（GridSearch），这里只需要给出最优参数的概率分布即可，而不需要按照步长把具体的值给一个个枚举出来parameter_space_svc ={ # loguniform表示该参数取对数后符合均匀分布 'C':hp.loguniform(\"C\", np.log(1), np.log(100)), 'kernel':hp.choice('kernel',['rbf','poly']), 'gamma': hp.loguniform(\"gamma\", np.log(0.001), np.log(0.1)),}# 鸢尾花卉数据集，是一类多重变量分析的数据集# 通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类iris = datasets.load_digits()#--------------------划分训练集和测试集--------------------train_data = iris.data[0:1300]train_target = iris.target[0:1300]test_data = iris.data[1300:-1]test_target = iris.target[1300:-1]#-----------------------------------------------------------# 计数器，每一次参数组合的枚举都会使它加1count = 0def function(params): c = params[\"C\"] kernel = params[\"kernel\"] gamma = params[\"gamma\"] # **可以把dict转换为关键字参数，可以大大简化复杂的函数调用 clf = svm.SVC(C=c,kernel = kernel,gamma = gamma) # 训练模型 clf.fit(train_data,train_target) # 预测测试集 prediction = clf.predict(test_data) global count count = count + 1 score = accuracy_score(test_target,prediction) print(\"第%s次，测试集正确率为：\" % str(count),score) # 由于hyperopt仅提供fmin接口，因此如果要求最大值，则需要取相反数 return -score# algo指定搜索算法，目前支持以下算法：# ①随机搜索(hyperopt.rand.suggest)# ②模拟退火(hyperopt.anneal.suggest)# ③TPE算法（hyperopt.tpe.suggest，算法全称为Tree-structured Parzen Estimator Approach）# max_evals指定枚举次数上限，即使第max_evals次枚举仍未能确定全局最优解，也要结束搜索，返回目前搜索到的最优解best = fmin(function, parameter_space_svc, algo=tpe.suggest, max_evals=100)# best[\"kernel\"]返回的是数组下标，因此需要把它还原回来kernel_list = ['rbf','poly']best[\"kernel\"] = kernel_list[best[\"kernel\"]]print(\"最佳参数为：\",best)clf = svm.SVC(**best)print(clf)3.9 to_categorical功能：将label转为one_hot形式，源于keras.utils包from keras.utils import to_categoricaly_onehot = to_categorical(y,num_classes(总类别数))3.10 绘制ROC曲线from sklearn.metrics import roc_curve, aucdef draw_ROC_curve(y_test, y_predict, savepath): '''画ROC曲线''' false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict) roc_auc = auc(false_positive_rate, true_positive_rate) plt.title('ROC') plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f' % roc_auc) plt.legend(loc='lower right') plt.plot([0, 1], [0, 1], 'r--') plt.ylabel('TPR') plt.xlabel('FPR') plt.savefig(savepath) plt.show() plt.close(0)四.分类模型4.1 随机森林from sklearn.ensemble import RandomForestClassifierrf_clf = RandomForestClassifier(n_estimators=700,n_jobs=-1,max_leaf_nodes=30)rf_clf.fit(trainX,trainY)y_pred = rf_clf.predict(testX)# 特征重要性feature_importance = rf_clf.feature_importances_4.2.Xgboost优势：表现快，训练时可以用所有的 CPU 内核来并行化建树；用分布式计算来训练非常大的模型；对于非常大的数据集还可以进行 Out-of-Core Computing)。参数：learning_rate ＝ 0.1 或更小，越小就需要多加入弱学习器；tree_depth ＝ 2～8；subsample ＝ 训练集的 30%～80%。from xgboost import XGBClassifiermodel = XGBClassifier()model.fit(X_train, y_train)y_pred = model.predict(X_test)# 可以在每加入一颗树后打印出 loglosseval_set = [(X_test, y_test)]model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)# 输出特征重要性from xgboost import plot_importancefrom matplotlib import pyplotplot_importance(model)pyplot.show()4.3 SVMfrom sklearn.svm import SVCclf = SVC(gamma='auto',C,kernel = \"RBF\")clf.fit(X, y)y_pred = clf.predict(testX)五.聚类模型5.1.轮廓系数评估聚类效果好坏——轮廓系数（Silhouette Coefficient）结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。该值处于-1~1之间，值越大，表示聚类效果越好。#聚类评估：轮廓系数from sklearn.metrics import silhouette_score# Kmeans的聚类结果来进行测试labels = KMeans(n_clusters=k).fit(data).labels_score = silhouette_score(data, labels)"
    } 
  
]
